,index,id,text,source,fake,source_org,label_org_pred,score,text_attacked,word_replacements,label,score
0,116.0,1477,"Measuring the effect of a nonlinear factor on the relative change in mean fitness of two groups can be difficult. In this study, we investigate the relative fitness effects of an environmental forcing and a nonlinear factor, such as a disease. We obtain a more realistic model, which allows us to examine the effect of a nonlinear factor by modifying the population size. We calculate the relative fitness effects for both the linear and nonlinear models. We show that the relative fitness effects of environmental forcing and nonlinear factor are dependent on the population size.",chatgpt,1,generated_gpt3,chatgpt,0.9998179078102112,"Measuring the effect of a nonlinear factor on the relative deepen in mean fitness of two groups can be difficult. In this study, we look into the congenator fitness effects of an environmental forcing and a nonlinear factor, such as a disease. We obtain a more realistic model, which allows us to examine the effect of a nonlinear factor by modifying the population size. We calculate the relative fitness effects for both the linear and nonlinear models. We show that the relative fitness effects of environmental forcing and nonlinear factor are dependent on the population size. ",+deepen+look into+congenator,human,0.9993602633476257
1,117.0,2031,"In this paper, we examine the impact of the amount of money a firm spends on advertising on its overall profit. The model assumes a monopolistic industry with a single seller. The profit is the difference between the revenue and the cost of the product, and the revenue is a function of the number of customers in the market. In addition, the revenue is a function of the advertising expenditure. The firm earns the revenue by selling the product to the customers. The advertising expenditure includes the cost of advertising and the profit earned from selling the product. We also assume that the advertising expenditure affects the revenue. We test for the presence of a price-advertising elasticity in the industry. We find that the advertising expenditure affects the profit. Moreover, the advertising expenditure has an impact on the profit even though the advertising is done on a very low budget. This is because the impact on the profit is due to sales.",chatgpt,1,generated_gpt3,chatgpt,0.9997984766960144,"In this paper, we examine the impact of the amount of money a firm spends on advertising on its boilers suit profit. The model assumes a monopolistic industriousness with a single seller. The profit is the difference between the revenue and the cost of the product, and the revenue is a function of the identification number of customers in the market. In addition, the revenue is a function of the advertising expenditure. The firm earns the revenue by selling the product to the customers. The advertising expenditure includes the cost of advertising and the profit earned from selling the product. We also get into that the advertising expenditure affects the revenue. We test for the presence of a monetary value - advertising elasticity in the industry. We find that the advertising expenditure affects the profit. Moreover, the advertising expenditure has an impact on the profit even though the advertising is done on a very low budget. This is because the impact on the profit is due to sales. ",+boilers suit+industriousness+identification number+get into+monetary value,chatgpt,0.9741992950439453
2,118.0,2564,"In this paper, we present a model to analyze the effects of the diffusion of new technologies on the market structure of an industry. We use the framework of an ecological network, where firms are linked by market structure. We show how the degree of interconnectivity of firms determines its ability to participate in innovation and to capture economic rents. We then show that the diffusion of new technologies, such as the availability of new technologies in the network, affects the market structure. The tools used to model the network are based on the concept of a gamma Pareto front, an extension of the Pareto front that is based on the degree of connections between nodes.",chatgpt,1,generated_gpt3,chatgpt,0.9998093247413636,"In this paper, we present a model to analyze the effects of the diffusion of new technologies on the market structure of an industry. We use the theoretical account of an bionomic communications network, where firms are linked by securities industry bodily structure. We show how the degree of interconnectivity of firms determines its ability to take part in innovation and to capture economic rents. We then show that the diffusion of new technologies, such as the availability of new technologies in the network, affects the market structure. The tools used to model the network are based on the concept of a gamma Pareto front, an extension of the Pareto front that is based on the degree of connections between nodes. ",+theoretical account+bionomic+communications network+securities industry+bodily structure+take part,chatgpt,0.9996949434280396
3,119.0,2643,"In this paper, we propose some numerical algorithms to solve one-dimensional partial differential equations in the space of functions with integrable kernels. In particular, we consider the problem of solving a second order ordinary differential equation for a given function and a given kernel in the space of continuous functions with integrable kernels, where the kernel is defined as the kernel of the corresponding ordinary differential equation. We show that the solution of the problem can be achieved by the Galerkin method, and this is our main result. We also present some numerical examples in which the proposed algorithm fails to converge. In the case of the Galerkin method, we show that the numerical stability of the algorithm is determined by the stability of the solution. We also show that the convergence can be achieved using the Galerkin method, however, the numerical stability of the algorithm is not a problem. ",chatgpt,1,generated_gpt3,chatgpt,0.999772608280182,"In this paper, we propose some numerical algorithms to solve one - dimensional partial differential equations in the space of functions with integrable kernels. In particular, we consider the problem of solving a second order ordinary differential equation for a given function and a given inwardness in the space of continuous functions with integrable kernels, where the kernel is defined as the kernel of the corresponding ordinary differential equation. We show that the solution of the problem can be achieved by the Galerkin method, and this is our main result. We also present some numerical examples in which the proposed algorithm fails to converge. In the case of the Galerkin method, we show that the numerical stability of the algorithm is determined by the stability of the solution. We also show that the convergence can be achieved using the Galerkin method, however, the numerical stability of the algorithm is not a problem. ",+inwardness,chatgpt,0.9998077750205994
4,120.0,3120,"The effect of uncertainty on risk-aversion is often seen as opposing the central role of risk-aversion. This paper challenges this view by demonstrating that risk-aversion depends on the source of uncertainty. Specifically, risk aversion increases when uncertainty is generated by a random draw or the course of random events. This is a consequence of the risk-averse agent being more in control of the uncertainty than of the random events. The paper shows that this effect follows from the conditioned utility model.",chatgpt,1,generated_gpt3,chatgpt,0.9904375672340392,"The effect of uncertainty on risk - aversion is often seen as opposing the central role of risk - aversion. This newspaper publisher challenges this view by demonstrating that risk - aversion depends on the source of uncertainty. Specifically, risk aversion increases when uncertainty is generated by a random draw or the course of hit-or-miss events. This is a consequence of the risk - averse agent being more in control of the uncertainty than of the random events. The paper shows that this effect follows from the conditioned utility model. ",+newspaper publisher+hit-or-miss,human,0.9871416091918945
5,121.0,6544,"In this paper, we introduce a new concept of ‘stochastic functional forms’ to model the dependence of the correlation coefficient matrix on the covariance matrix. The concept of functional forms in statistics has been extended to analyze the dependence of dependent variables on the independent variables in the stochastic analysis of ecological networks. The new concept of functional form enables us to analyze the dependence of the correlation coefficient matrix on the covariance matrix in a network. The analysis of the functional forms in stochastic analysis of ecological networks is based on the stochastic block model for the distribution of the correlation coefficient matrix. The analysis is extended to investigate the dependence of the correlation coefficient matrix on the covariance matrix. ",chatgpt,1,generated_gpt3,chatgpt,0.9998119473457336,"In this newspaper publisher, we introduce a newly arisen concept of ‘ stochastic functional forms ’ to model the dependence of the correlation coefficient matrix on the covariance matrix. The concept of functional forms in statistics has been extended to analyze the dependence of dependent variables on the independent variables in the stochastic analysis of ecological networks. The new concept of functional form enables us to analyze the dependence of the correlation coefficient matrix on the covariance matrix in a network. The analysis of the functional forms in stochastic analysis of ecological networks is based on the stochastic block model for the distribution of the correlation coefficient matrix. The analysis is extended to investigate the dependence of the correlation coefficient matrix on the covariance matrix. ",+newspaper publisher+newly arisen,chatgpt,0.9996761083602905
6,122.0,6648,"We study the dynamics of the temporal structure of the Internet in order to test the hypothesis that the Internet is becoming more mesoscopic. This hypothesis is supported by a number of historical and archival data. We find evidence for mesoscale structure in the Internet by analyzing the size distribution of webpages, the number of domains, the average number of hops from a domain to another domain, and the average number of links between pairs of domains. We also find that the number of domains is decreasing and the average number of links is increasing, consistent with the hypothesis of an increase in mesoscale structure. ",chatgpt,1,generated_gpt3,chatgpt,0.9945263266563416,"We study the kinetics of the temporal structure of the Internet in order to test the hypothesis that the Internet is becoming more mesoscopic. This hypothesis is supported by a number of historical and archival data. We find evidence for mesoscale structure in the Internet by analyzing the size distribution of webpages, the number of domains, the average number of hops from a domain to another demesne, and the average number of links between pairs of domains. We also find that the number of domains is decreasing and the average number of links is increasing, consistent with the hypothesis of an increase in mesoscale structure. ",+kinetics+demesne,human,0.8013163805007935
7,123.0,6918,"In general, the interval size distribution function of a random variable, that is, its distribution function over the interval [a, b], is characterized by a non-uniform tail at a. In this paper, we provide a simple and practical method to estimate the tail likelihood function of a random variable, that is, its distribution function over [a, b], by using a simple counting argument. Specifically, assuming that the variable has the continuous distribution function f, we can extend the tail likelihood function of f to a tail likelihood function of the square of f, where the tail probability function of f is defined as L(f) = -a/2f. By using this method, we are able to estimate both the tail likelihood function and the tail probability function of a random variable. To illustrate our method, we apply it to the tail likelihood function of the distribution function of the sex ratio in a population of birds and show that the tail likelihood function of the sex ratio is significantly non-",chatgpt,1,generated_gpt3,chatgpt,0.9997820258140564,"In general, the interval size distribution function of a random variable, that is, its distribution function over the interval [a, b], is characterized by a non - uniform tail at a. In this paper, we provide a simple and practical method to estimate the tail likelihood function of a random variable, that is, its distribution function over [a, b], by using a simple numeration argument. Specifically, assuming that the variable has the continuous distribution function f, we can extend the tail assembly likelihood function of f to a tail likelihood function of the square of f, where the tail probability function of f is defined as L(f) = -a/2f. By using this method, we are able to estimate both the tail likelihood function and the tail probability function of a random variable. To illustrate our method, we apply it to the tail likelihood function of the distribution function of the sex ratio in a population of birds and show that the tail likelihood function of the sex ratio is significantly non-",+numeration+tail assembly,chatgpt,0.9998010993003845
8,124.0,7168,"We introduce a new approach to (non-linear) optimization that can be used in the context of a statistical estimation of the distribution of a random variable. In order to avoid the need for the normalization of the data, we use the principle of ‘consistency’ between the data and the distribution to calculate the expectation of the data and the distribution. We show that the proposed method can be applied to a wide range of metrics, including the entropy, mutual information and the KL divergence. The general strategy is to use the principle of consistency between the data and the distribution to calculate the expectation of the data and the distribution. It is shown that the proposed method can be applied to a wide range of metrics, including the entropy, mutual information and the KL divergence. The general strategy is to calculate the expectation of the data and the distribution by applying a standard estimator to the data, and then use the result to calculate the expectation of the data and the distribution. To illustrate the method",chatgpt,1,generated_gpt3,chatgpt,0.9997802376747132,"We introduce a new approach to (non - linear) optimization that can be used in the context of a statistical estimation of the distribution of a hit-or-miss variable quantity. In order to avoid the have got for the normalization of the data, we use the principle of ‘ consistency ’ between the data and the distribution to calculate the expectation of the data and the distribution. We show that the proposed method can be applied to a wide range of metrics, including the entropy, mutual information and the KL divergence. The general strategy is to use the principle of consistency between the data and the distribution to calculate the expectation of the data and the distribution. It is shown that the proposed method can be applied to a wide range of metrics, including the entropy, mutual information and the KL divergence. The general strategy is to calculate the expectation of the data and the distribution by applying a standard estimator to the data, and then use the result to calculate the expectation of the data and the distribution. To illustrate the method",+hit-or-miss+variable quantity+have got,chatgpt,0.9997912049293518
9,125.0,7187,"In this paper, we consider the problem of minimizing a linear program, which has a convex quadratic objective function, subject to a linear constraint, in the Euclidean space. We propose a reformulation of the problem that appears to be more natural, and we prove that the optimal solution to this problem is obtained in polynomial time.",chatgpt,1,generated_gpt3,chatgpt,0.9840667247772216,"In this newspaper publisher, we consider the problem of minimizing a linear program, which has a convex quadratic objective function, subject to a linear constraint, in the Euclidean space. We propose a reformulation of the problem that appears to be more natural, and we prove that the optimal solution to this problem is obtained in polynomial time. ",+newspaper publisher,chatgpt,0.5097002983093262
10,126.0,7220,"A model of the provision of an educational service is developed which takes into account the effects of age, experience, and work status on the willingness of individuals to participate, and on the demand for the service of providing the service. This model is extended to allow for the introduction of a co-payment. The paper considers three different situations: (1) a situation with no co-payment; (2) a situation with a co-payment of £10 per annum; and (3) a situation with a co-payment of £20 per annum. The results of the paper suggest that when a co-payment is introduced, the number of individuals who are willing to participate in a service falls, and the number of individuals who are willing to pay rises. These results suggest that, generally, the introduction of a co-payment reduces the number of individuals who are willing to participate in a service.",chatgpt,1,generated_gpt3,chatgpt,0.999805986881256,"A model of the provision of an educational service is developed which takes into account the effects of age, experience, and work status on the willingness of individuals to participate, and on the call for for the service of providing the service. This theoretical account is stretched out to allow for the introduction of a co - payment. The paper considers three different situations: (1) a situation with no co - payment; (2) a situation with a co - defrayment of £ 10 per annum; and (3) a situation with a co - payment of £ 20 per annum. The results of the newspaper publisher suggest that when a co - payment is introduced, the number of individuals who are willing to participate in a service falls, and the number of individuals who are willing to pay rises. These results suggest that, generally, the first appearance of a co - payment reduces the number of individuals who are willing to participate in a service. ",+call for+theoretical account+stretched out+defrayment+newspaper publisher+first appearance,chatgpt,0.9997794032096863
11,127.0,7247,"We have recently proposed a probabilistic approach for estimating the time of death of a patient when all other information is either unavailable or incomplete. We have shown that this approach is consistent and accurate when applied to patients who have died in intensive care units. In this paper, we use the same approach to estimate the time of death of a patient who died in an emergency department and whose cause of death is unknown. We show that the probabilistic approach is consistent and accurate under various assumptions. We also demonstrate that the approach is useful for inferring the time of death of a patient whose cause of death is unknown. This journal is on-line at: http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1522-4959/issue/13/ ",chatgpt,1,generated_gpt3,chatgpt,0.999826967716217,"We have of late proposed a probabilistic approach for estimating the time of death of a patient when all other information is either unavailable or incomplete. We have shown that this approach is consistent and accurate when applied to patients who have died in intensive care units. In this paper, we use the same approach to estimate the time of death of a patient who died in an emergency department and whose cause of death is unknown. We show that the probabilistic approach is consistent and accurate under various assumptions. We also demonstrate that the approach is useful for inferring the time of death of a patient whose cause of death is unknown. This journal is on - line at: http://onlinelibrary. wiley. com/journal/10. 1111/(ISSN)1522-4959/issue/13/",+of late,chatgpt,0.9995075464248657
12,128.0,8249,"We investigate the effects of individual differences on the performance of teams in a multi-stage repeated-measures experiment. Teams are randomly assigned to three treatment groups, with each team performing a different task. We ask which characteristics of team members affect the team's performance in each of the tasks. We also test whether these effects are consistent across tasks. Our results suggest that personality traits are important for team performance in a multi-stage experiment: teams with high-performing members who differ in personality traits achieve higher performance on all tasks. We argue that our results provide insights into the effect of personality on team performance. ",chatgpt,1,generated_gpt3,chatgpt,0.9996620416641236,"We investigate the effects of individual differences on the performance of teams in a multi - microscope stage repeated - measures experimentation. Teams are at random assigned to deuce-ace treatment groups, with each team performing a different task. We ask which characteristics of team members affect the team up's performance in each of the tasks. We also test whether these effects are consistent across tasks. Our results suggest that personality traits are important for team performance in a multi - stage experiment: teams with high - performing members who differ in personality traits achieve higher performance on all tasks. We debate that our results provide insights into the effect of personality on team performance. ",+microscope stage+experimentation+at random+deuce-ace+team up+debate,human,0.9761333465576172
13,129.0,8290,We study the impact of the number of agents on the efficiency of an allocation mechanism. We do so using a simple model with two-sided markets. The allocation mechanism is a first-come-first-served algorithm. We show that the allocation mechanism becomes more inefficient as the number of agents increases. Our results suggest that agents should be randomly distributed in the market. We also show that the efficiency of the allocation mechanism is dependent on the distribution of agents.,chatgpt,1,generated_gpt3,chatgpt,0.9998255372047424,We study the impact of the number of agents on the efficiency of an allocation mechanism. We do so using a simple theoretical account with 2 - sided markets. The allocation mechanism is a first - issue forth - first - served algorithmic program. We show that the allocation mechanism becomes more uneconomical as the number of agents increases. Our results paint a picture that agents should be at random dealt out in the market. We also show that the efficiency of the allocation mechanism is dependent on the distribution of agents. ,+theoretical account+2+issue forth+algorithmic program+uneconomical+paint a picture+at random+dealt out,chatgpt,0.7046782374382019
14,130.0,8621,"The issue of cost-sharing in medical care has been a topic of interest for many years. This paper applies a weighted-avg-rate scheme for cost-sharing in the context of a publicly-funded health care system. The scheme is designed to be robust to the variation in costs of medical care services and to the variations in the cost-sharing parameters, such as the maximum cost-sharing amount and the percentage of cost-sharing for the insured group. The scheme is also designed to be robust to the transition from a single-payer system to a public health insurance system. The scheme advocates a specified maximum cost-sharing level, which is the maximum cost-sharing that the insured population would have if they were the only individuals who received medical care services. The scheme also advocates a specified percentage of cost-sharing, which is the maximum cost-sharing that the insured population would have if they were the only individuals who received medical care services and the maximum cost-sharing level were set to the",chatgpt,1,generated_gpt3,chatgpt,0.9997934699058532,"The issue of cost - sharing in medical care has been a topic of interest for many old age. This paper applies a adjusted - avg - rate scheme for cost - sharing in the context of a publicly - funded health care system. The dodging is designed to be robust to the fluctuation in costs of medical care services and to the variations in the cost - sharing parameters, such as the maximum cost - sharing amount and the percentage of cost - sharing for the insured group. The scheme is also designed to be robust to the changeover from a single - payer system to a public health insurance system. The scheme advocates a such that maximum cost - sharing level, which is the maximum cost - sharing that the insured population would have if they were the only individuals who received medical care services. The scheme also advocates a specified per centum of monetary value - sharing, which is the maximum cost - sharing that the insured population would have if they were the only individuals who received medical care services and the upper limit cost - sharing level were set to the",+old age+adjusted+dodging+fluctuation+changeover+such that+per centum+monetary value+upper limit,human,0.654964029788971
15,131.0,10971,"In any large population, it is not possible to meet all of the people and their needs. Therefore, we must make choices between different types of interventions in order to meet the needs of the people in a population. In this paper, we investigate the trade-offs between economic and social efficiency when deciding how many of each type of intervention to provide. We develop a model that captures the trade-off between the economic and social inefficiency of a program. We find that, when society is relatively uninterested in an intervention, the social inefficiency of a program can be larger than the economic inefficiency. This is the case even when the program has a high social value. Moreover, when there is no clear trade-off between the economic and social inefficiency of a program, the social inefficiency of a program can be larger than the economic inefficiency.",chatgpt,1,generated_gpt3,chatgpt,0.9998130202293396,"In any too large population, it is not researchable to meet all of the people and their of necessity. Therefore, we have got make choices between different types of interventions in order to meet the needs of the people in a population. In this paper, we investigate the trade - offs between economic and social efficiency when deciding how many of each type of intervention to provide. We develop a model that captures the business deal - off between the economic and social inefficiency of a program. We find that, when society is relatively uninterested in an intervention, the social inefficiency of a program can be larger than the economic inefficiency. This is the case even when the program has a high social value. Moreover, when there is no clear trade - off between the economic and social inefficiency of a program, the social inefficiency of a program can be larger than the economic inefficiency. ",+too large+researchable+of necessity+have got+business deal,human,0.9997112154960632
16,132.0,11634,"The majority of the population of a country says that the population should rise to the next level in a given year. However, that does not mean that the country will reach the next level in the next year. This paper provides a model that predicts the probability of the country reaching the next level in the next year by using a population projection model with a logistic equation. The model is validated using a data set of more than 80 countries in the world. The findings are consistent with the literature and the model explains the majority of the variation in the data set. The predictions of the model are very accurate and they are in agreement with those of the World Population Prospects, the United Nations. The model can be used to project the next year’s population of the country, for example, for a country that says it is concerned about its population. ",chatgpt,1,generated_gpt3,chatgpt,0.999789535999298,"The majority of the population of a country says that the population should rise to the next level in a given year. However, that does not mean that the country will reach the next level in the next year. This paper provides a model that predicts the probability of the country arrival the next level in the next year by using a population forcing out model with a logistic equation. The model is validated victimisation a data set of more than fourscore countries in the world. The findings are consistent with the literature and the model explains the majority of the variation in the data set. The predictions of the model are very accurate and they are in agreement with those of the worldly concern Population Prospects, the United Nations. The model can be used to project the succeeding year ’s population of the country, for example, for a country that says it is concerned about its population. ",+arrival+forcing out+victimisation+fourscore+worldly concern+succeeding,chatgpt,0.9973112344741821
17,133.0,13867,"Objectively assessing the quality of the educational materials is a crucial stage in the process of producing new and improved learning materials. This is an important part of a robust and successful system that aims to cater to the different demands and individual preferences of every individual. In this study we use the concept of educational goals, which are a set of learning objectives that influence the learning and development of the individual. Educational goals are a result of the needs of learners and the capabilities of the educational materials. It is also an indication of the extent of the variation to which a learning objective can be applied to a wide range of learners and learning materials. This is particularly useful in the context of construction of effective learning materials that cater to the individual needs and learning styles. Educational goals are subjective and intangible, therefore their assessment requires the use of authentic and reliable measures that can be applied across a wide range of learners and materials. In order to assess the educational goals of the readers, the current study applies a set of measures that have",chatgpt,1,generated_gpt3,chatgpt,0.9997920393943788,"Objectively assessing the quality of the educational materials is a crucial stage in the process of producing new and improved learning materials. This is an important part of a robust and successful system that aims to cater to the different demands and individual preferences of every individual. In this study we use the concept of educational goals, which are a set of learning objectives that influence the learning and development of the individual. instructive goals are a result of the needs of learners and the capabilities of the educational materials. It is also an indication of the extent of the variation to which a learning objective can be applied to a wide range of learners and learning materials. This is particularly useful in the context of mental synthesis of effective learning materials that cater to the individual needs and learning styles. Educational goals are subjective and intangible, therefore their assessment requires the use of authentic and reliable measures that can be applied across a wide range of learners and materials. In order to assess the educational goals of the readers, the current study applies a set of measures that have",+instructive+mental synthesis,chatgpt,0.9998137354850769
18,134.0,14866,"The recent explosion of interest in the study of the biology of aging has created a need for a general theory that can provide a unified explanation of the biology of aging. This paper develops one such general theory, the theory of species-specific aging. The theory is based on the concept of developmental plasticity, which implies that a species' life history is adaptive to its environment. A species' life history determines the rate at which it ages, the rate at which its constituent components age, and the rate at which its constituent components age at different rates. The theory is based on the fundamental assumption that aging is a process that begins at one end of life history and proceeds to the other. The theory is rooted in the basic observation that aging is universal, extending to all species. The theory is developed by first outlining the life history of a species and then considering the age-related changes in the life history. The theory is used to explain the biology of aging in terms of the life history of a species",chatgpt,1,generated_gpt3,chatgpt,0.99979430437088,"The recent plosion of interest in the study of the biology of aging has created a need for a general theory that can provide a unified explanation of the biology of aging. This paper develops one such general theory, the theory of species - particularised aging. The theory is based on the concept of developmental plasticity, which implies that a species' life history is adaptive to its environment. A species' life history determines the rate at which it ages, the rate at which its constituent components age, and the rate at which its constituent components age at different rates. The theory is based on the fundamental assumption that aging is a process that begins at one end of life history and takings to the other. The theory is rooted in the alkalic watching that aging is universal, extending to all species. The theory is developed by first outlining the life history history of a species and then considering the old age - related changes in the life history. The theory is used to explain the biology of aging in terms of the life history of a species",+plosion+particularised+takings+alkalic+watching+life history+old age,chatgpt,0.999779999256134
19,135.0,15032,"This paper proposes a method for determining the optimal duration for a clinical trial for pharmaceutical companies. This method is based on the theoretical approach of the theory of optimal stopping rules (OSTR). The OSTR is not commonly applied because it is computationally expensive. In this paper, we propose a method based on a fast OSTR algorithm using the linear programming technique. We demonstrate that the proposed method is more accurate than the OSTR. We also provide some theoretical results for when the OSTR algorithm is not feasible. We also demonstrate that the proposed method is more accurate than the OSTR for the stopping rules of continuous and discrete trials. ",chatgpt,1,generated_gpt3,chatgpt,0.9998106360435486,"This paper proposes a method for determining the optimal duration for a clinical trial for pharmaceutical companies. This method is based on the theoretical approach of the possibility of optimum stopping rules (OSTR). The OSTR is not commonly applied because it is computationally expensive. In this paper, we propose a method based on a fast OSTR algorithm using the linear programming technique. We demonstrate that the proposed method is more accurate than the OSTR. We also provide some theoretical results for when the OSTR algorithm is not feasible. We also demonstrate that the proposed method is more accurate than the OSTR for the stopping rules of continuous and discrete trials. ",+possibility+optimum,chatgpt,0.9998169541358948
20,136.0,15748,"Simulations of financial markets have been a fundamental tool in the study of asset prices, economic agents’ decisions and market efficiency. We present a model of financial markets where agents can cooperate or compete and where one agent makes a decision based on the action of his competitors. We show that the degree of cooperation or competition depends on the level of information available to the agents and the degree of the agents’ commitment to the rule of cooperation. We show that the degree of competition, as a function of the degree of information, is much less sensitive to the level of the agents’ commitment. We show that agents’ commitment to the rule of cooperation is a function of the agents’ trust in each other. We show that the degree of trust is a function of information. And we show that cooperation and trust are both sensitive to the level of the agents’ information.",chatgpt,1,generated_gpt3,chatgpt,0.9997913241386414,"Simulations of financial markets have been a fundamental tool in the study of asset prices, economic agents ’ decisions and securities industry efficiency. We present a theoretical account of fiscal markets where agents can cooperate or compete and where one agent makes a decision based on the action of his competitors. We show that the degree of cooperation or competition depends on the level of information available to the agents and the degree of the agents ’ commitment to the rule of cooperation. We show that the degree of competition, as a function of the degree of information, is much less sensitive to the level of the agents ’ commitment. We show that agents ’ commitment to the rule of cooperation is a function of the agents ’ trust in each other. We show that the degree of trust is a function of information. And we show that cooperation and trust are both sensitive to the level of the agents ’ information. ",+securities industry+theoretical account+fiscal,chatgpt,0.9998262524604797
21,137.0,15885,"The effect of the population structure on the species richness of plant communities is a topic of great interest. The diversity of species that are present in a community is influenced by the diversity of its components. Some components may be more important than others, and the relative importance of different species on the overall community diversity may vary among species. In this paper, we study the effect of the relative importance of different species on the composition of plant communities in terms of their species richness. We use a simple framework to show that the species richness is not only strongly affected by the species diversity, but also by the relative importance of different species. Our results show that the relative importance of different species affects the species richness in plant communities.",chatgpt,1,generated_gpt3,chatgpt,0.9997972846031188,"The effect of the population structure on the species richness of plant communities is a theme of of import interest. The diversity of species that are present in a community is influenced by the diversity of its components. Some components may be more important than others, and the relative importance of different species on the overall community diversity may vary among species. In this paper, we study the effect of the relative importance of different species on the composition of plant communities in terms of their species richness. We use a simple framework to show that the species richness is not only strongly affected by the species diversity, but also by the relative importance of different species. Our results show that the relative importance of different species affects the species richness in industrial plant communities. ",+theme+of import+industrial plant,chatgpt,0.9998204112052917
22,138.0,18863,"We investigate the effects of competition on the dynamics of consumer-brand relationships. Our model is motivated by the high level of competition in the food industry and the substantial evidence that consumer-brand relationships are important for brand-consumer relationships. We use a standard two-period symmetric-difference-type model of consumer-brand relationships. We show that brand competition tends to lead to a reduced duration of brand-consumer relationships on the one hand and to an increased number of impulsive brand switches on the other hand. In the long run, increased competition leads to a higher proportion of brand-consumer relationships that are stable. The effect of competition on brand switching, however, depends on the extent of brand differentiation. The higher is the degree of differentiation, the more important is the effect of competition on switching.",chatgpt,1,generated_gpt3,chatgpt,0.9995691180229188,"We investigate the personal effects of competition on the kinetics of consumer - trade name relationships. Our model is motivated by the high level of competition in the food industry and the substantial evidence that consumer - brand relationships are of import for brand - consumer relationships. We use a standard two - period symmetric - difference - typewrite model of consumer - brand relationships. We show that brand competition tends to atomic number 82 to a reduced duration of brand - consumer relationships on the one hand and to an increased number of impulsive brand switches on the other hand. In the long run, increased competition leads to a higher proportion of brand - consumer relationships that are stable. The effect of competition on brand switching, however, depends on the extent of brand differentiation. The higher is the degree of differentiation, the more important is the effect of competition on switching. ",+personal effects+kinetics+trade name+of import+typewrite+atomic number 82,human,0.9984458088874817
23,139.0,19767,"Most of us are familiar with the ‘curse of dimensionality’, i.e. the phenomenon whereby simple models, which are easy to understand, often lead to complicated results. This is also true for mathematical models for biological systems. This paper shows that this is because we often treat the system as a point process. In mathematical terms, the system is not discrete, but continuous, that is, we need to take into account the statistics of the continuous probability distribution. We show that the classical results of point process theory generalise to the continuous setting. The main contribution is the introduction of a new measure of diversity, the ratio of the number of points in the system to the maximum number of points in the system. This measure is a generalisation of the Kronecker delta, which is a measure of the diversity in point processes. We also introduce an alternative measure of diversity, which is based on the sum of the differences between the mean and the median of the system. We",chatgpt,1,generated_gpt3,chatgpt,0.9997958540916444,"Most of united states of america are familiar with the ‘ curse of dimensionality ’, i. e. the phenomenon whereby simple models, which are easy to understand, often lead to complicated results. This is also true for mathematical models for biological systems. This paper shows that this is because we often treat the system as a point process. In mathematical terms, the system is not discrete, but continuous, that is, we need to take into account the statistics of the continuous probability distribution. We show that the classical results of point process theory generalise to the continuous setting. The main contribution is the introduction of a new measure of diversity, the ratio of the number of points in the system to the maximum number of points in the system. This measure is a generalisation of the Kronecker delta, which is a measure of the diversity in point processes. We also introduce an alternative measure of diversity, which is based on the sum of the differences between the mean and the median of the system. We",+united states of america,chatgpt,0.9995720982551575
24,140.0,20996,"The problem of transportation of goods between two locations is one of the basic problems of modern economy. It is important to consider the problem of two-dimensional transportation in which the transportation can take place on two different modes. In this paper, we consider a new two-dimensional transportation problem. It considers a transportation path which has to be made between two locations, on a given road network. The problem is called the Transportation Problem on Two-Dimensional Roads (TP2DR). ",chatgpt,1,generated_gpt3,chatgpt,0.9998190999031068,"The problem of transportation of goods between two locations is one of the basic problems of modern economy. It is important to consider the problem of two - multidimensional transportation in which the transportation can take place on two different modes. In this newspaper publisher, we consider a new 2 - dimensional transportation trouble. It considers a transportation system way of life which has to be made between two locations, on a given road network. The problem is called the transportation system Problem on Two - Dimensional roadstead (TP2DR). ",+multidimensional+newspaper publisher+2+trouble+transportation system+way of life+transportation system+roadstead,chatgpt,0.664513885974884
25,141.0,21989,"Barriers, such as discrimination and low belongingness to the majority, are major obstacles for racial integration in the labor market. We study the impact of different types of barriers on the racial integration of the labor market in the United States, using the 1980 Census. We focus on the number of people of a specific race employed in different industries, since it is the logarithm to the number employed who are black and the number employed who are white that are the most relevant to the analysis. We find that the effect of each barrier depends on its type. The effect of discrimination is greater in the current than in the past decade. Our results indicate that the black-white unemployment gap has been stable in the past decade. Additionally, the black-white unemployment gap varies considerably according to the type of barrier. The black-white unemployment gap is larger for race-specific barriers than for general barriers. In addition, the black-white unemployment gap varies with the type of barrier according to whether it is",chatgpt,1,generated_gpt3,chatgpt,0.9998188614845276,"Barriers, such as discrimination and low-toned belongingness to the majority, are major obstacles for racial integration in the labor securities industry. We study the impact of different types of barriers on the racial integration of the labor market in the United States, using the 1980 nosecount. We focus on the number of people of a specific race employed in different industries, since it is the logarithm to the number employed who are black and the number made use of who are white that are the most relevant to the analysis. We find that the effect of each barrier depends on its type. The effect of discrimination is greater in the current than in the past decade. Our results indicate that the black - white unemployment gap has been stable in the past decade. Additionally, the black - white unemployment gap varies considerably according to the type of barrier. The black - white unemployment gap is larger for race - specific barriers than for general barriers. In addition, the black - white unemployment gap varies with the type of barrier according to whether it is",+low-toned+securities industry+nosecount+made use of,chatgpt,0.8632539510726929
26,142.0,22889,"In this paper, we investigate the optimal number of edges in an Adjacency Network, which is a generalization of the Adjacency Matrix. The main focus is on the case of modularity optimization and we present a formulation of the problem of finding the optimal number of edges in a modular network. We show that the optimal number of edges is not necessarily the same as the optimal size of the modules. Moreover, we study the case of modular networks which are not necessarily isomorphic. Our results can be used in practice to design efficient algorithms for finding the optimal number of edges in a modular network.",chatgpt,1,generated_gpt3,chatgpt,0.9983664155006408,"In this paper, we investigate the optimal number of edges in an Adjacency Network, which is a generalization of the Adjacency Matrix. The main focus is on the case of modularity optimization and we present a formulation of the problem of finding the optimal number of edges in a modular network. We show that the optimal number of edges is not necessarily the same as the optimal size of the modules. Moreover, we study the case of modular networks which are not necessarily isomorphic. Our results can be used in practice to design efficient algorithms for finding the optimal number of edges in a modular network. ",,chatgpt,0.9838951230049133
27,143.0,25574,"The belief propagation algorithm (BPA) is a Bayesian inference algorithm with the goal of solving the maximum a posteriori (MAP) problem under the Gaussian Dirichlet prior for latent variables. In the BPA, the taxonomy of the classes is fixed but the architecture of the posterior distribution depends on the problem at hand. In this paper, we present a comparison of the BPA with a variant of this algorithm, the Belief Propagation Algorithm (BPA). We analyze the MAP problem for Gaussian distributions and for Bayesian networks, and we show that the BPA is more stable in the presence of Markovian processes. To our knowledge, this is the first comparison of the BPA with a different algorithm in the context of a Bayesian Maximum Entropy framework.",chatgpt,1,generated_gpt3,chatgpt,0.9993641972541808,"The belief propagation algorithm (BPA) is a Bayesian inference algorithm with the goal of solving the maximum a posteriori (MAP) problem under the Gaussian Dirichlet prior for latent variables. In the BPA, the taxonomy of the classes is fixed but the architecture of the posterior distribution depends on the problem at hand. In this paper, we present a comparison of the BPA with a variant of this algorithm, the Belief Propagation Algorithm (BPA). We analyze the MAP problem for Gaussian distributions and for Bayesian networks, and we show that the BPA is more stable in the presence of Markovian processes. To our knowledge, this is the first comparison of the BPA with a different algorithm in the context of a Bayesian Maximum Entropy framework. ",,chatgpt,0.998233437538147
28,144.0,109,"The effect of artificial selection on the fitness of individuals is known to be a key component of the evolutionary process. So far, the effect of artificial selection has been studied in an individual level context. This study shows that the magnitude of the effect of artificial selection on the fitness of individuals can be maximized by allowing the effects of both selection and genetic drift to act simultaneously on the individuals. We present the results of such a combined selection-drift experiment on the ‘basal’ population of Drosophila melanogaster. We find that the fitness of the population is maximized only if the selection and drift act together. This result suggests that the combined effect of selection and drift can be maximized by acting on individuals. ",chatgpt,1,generated_gpt3,chatgpt,0.999806582927704,"The effect of artificial selection on the fitness of individuals is known to be a key component of the evolutionary process. So far, the effect of artificial selection has been studied in an individual horizontal surface context. This study shows that the magnitude of the effect of artificial selection on the fitness of individuals can be maximized by allowing the effects of both selection and genetic be adrift to act simultaneously on the individuals. We present the results of such a combined selection - drift experiment on the ‘ basal ’ population of Drosophila melanogaster. We find that the fitness of the population is maximized only if the selection and drift act together. This result suggests that the combined effect of selection and drift can be maximized by acting on individuals. ",+horizontal surface+be adrift,chatgpt,0.8868616819381714
29,145.0,382,"The concept of ‘joint distribution’, introduced by Sargent and Sejnowski, has been exploited in many areas of neuroscience, including electrophysiology, image analysis, and computational neuroscience. In this paper, we present a general framework for joint probability density functions that has been used to analyze a number of tasks in neuroscience. The framework is expressed in terms of a measure on a standard density function space and is based on the notion of the affine structure of the joint distribution. We show how the framework can be used to analyze a variety of problems in neuroscience, including binomial and Poisson regression, Fourier analysis, signal analysis, and clustering. The basic intuition behind the framework is that the joint distribution has a form that allows the joint probability of a set of measurements to be expressed in terms of a single probability measure. We illustrate the application of the framework to the analysis of Gaussian random field images and to the estimation of the affine structure of the",chatgpt,1,generated_gpt3,chatgpt,0.999784767627716,"The concept of ‘ joint distribution ’, introduced by Sargent and Sejnowski, has been exploited in many areas of neuroscience, including electrophysiology, image analysis, and computational neuroscience. In this paper, we present a general framework for joint probability density functions that has been used to analyze a number of tasks in neuroscience. The framework is expressed in terms of a measure on a standard density function space and is based on the notion of the affine structure of the joint distribution. We show how the framework can be used to analyze a variety of problems in neuroscience, including binomial and Poisson regression, Fourier analysis, signal analysis, and clustering. The basic intuition behind the framework is that the joint distribution has a form that allows the joint probability of a set of measurements to be expressed in terms of a single probability measure. We illustrate the application of the framework to the analysis of Gaussian random field images and to the estimation of the affine structure of the",,chatgpt,0.9997918009757996
30,146.0,681,"We consider a closed interval graph G = (V,E) with edges and e = (x,y) in V representing integers, and with B(G) the set of all breakpoints of G. Given a cardinality function poly and a function φ : B(G) → 0, we show that if poly(e) < φ(e), then there is a deterministic non-bipartite graph on B(G) whose automorphism group is generated by the elements x and y of e. If poly(e) = φ(e) , then the automorphism group is generated by the element i of e. We discuss the conditions on poly(e) < φ(e) and the invariant that we can associate to the resulting graph. This invariant is an upper bound on the number of 'peaks' in the graph and on the values of the breakpoint positions. ",chatgpt,1,generated_gpt3,chatgpt,0.943420708179474,"We look at a closed time interval graph G = (V, E) with edges and e = (x, y) in V representing integers, and with B(G) the set of all breakpoints of G. Given a cardinality function poly and a function φ: B(G) → 0, we show that if poly(e) <φ(e), then there is a deterministic non - bipartite graph on B(G) whose automorphism group is generated by the elements x and y of e. If poly(e) = φ(e), then the automorphism group is generated by the element i of e. We discuss the conditions on poly(e) <φ(e) and the invariant that we can associate to the resulting graph. This invariant is an upper bound on the number of' peaks' in the graph and on the values of the breakpoint positions. ",+look at+time interval,human,0.542871356010437
31,147.0,1579,"Inference of causality is a fundamental problem in many areas of research, such as epidemiology, biostatistics, and psychology. The problem of causality inference in networks is particularly important, because it relates to a crucial issue in network science and the analysis of complex networks. In this paper, we present a new method for the inference of causality in network data, which we call the causal filter method. The filter consists of three steps. The first step is to determine a causality criterion for each variable. The second step is to compute a conditional probability mass function (CPMF) of all the variables in the dataset. The third step is to form a Bayesian network using the CPMF. Our method yields a causal Bayesian network, which provides a Bayesian explanation for the observed variables. We apply our method to a dataset consisting of gene expression data obtained from human colorectal tumor samples, and we find that the Bayesian network is able to explain the",chatgpt,1,generated_gpt3,chatgpt,0.9998061060905457,"Inference of causality is a fundamental problem in many areas of research, such as epidemiology, biostatistics, and psychology. The problem of causality inference in networks is particularly important, because it relates to a crucial issue in network science and the analysis of complex networks. In this paper, we present a new method for the inference of causality in network data, which we call the causal filter method. The filter consists of three steps. The first step is to determine a causality criterion for each variable. The second step is to compute a conditional probability mass function (CPMF) of all the variables in the dataset. The third step is to form a Bayesian network using the CPMF. Our method yields a causal Bayesian network, which provides a Bayesian explanation for the observed variables. We apply our method to a dataset consisting of gene expression data obtained from human colorectal tumor samples, and we find that the Bayesian network is able to explain the",,chatgpt,0.9998061060905457
32,148.0,1606,"The well-known Principal Component Analysis (PCA) is an unsupervised tool for data representation with a restricted dimensionality. It is used in a large number of applications and is even considered a robust tool for statistical data analysis. However, the application of PCA to a high dimensional data set is still challenging. In such cases, PCA often fails to capture the information from the original data set. We propose the novel PCA-based method, PCA-D, which is an unsupervised dimensionality reduction technique that can handle a high dimensional data set. PCA-D is an extension of PCA that consists of two steps: PCA-D first computes the coefficients of the PCA projection matrix and then uses PCA to generate the new data set. The PCA coefficients of PCA-D are calculated by a PCA-D-PCA method. This paper presents a thorough discussion on PCA-D and its applications.",chatgpt,1,generated_gpt3,chatgpt,0.9997852444648744,"The well - known Principal Component Analysis (PCA) is an unsupervised tool for data representation with a restricted dimensionality. It is used in a large number of applications and is even considered a robust tool for statistical data analysis. However, the application of PCA to a high dimensional data set is still challenging. In such cases, PCA often fails to capture the information from the original data set. We propose the novel PCA - based method, PCA - D, which is an unsupervised dimensionality reduction technique that can handle a high dimensional data set. PCA - D is an extension of PCA that consists of two steps: PCA - D first computes the coefficients of the PCA projection matrix and then uses PCA to generate the new data set. The PCA coefficients of PCA - D are calculated by a PCA - D - PCA method. This paper presents a thorough discussion on PCA - D and its applications. ",,chatgpt,0.9998006224632263
33,149.0,1619,"The ability of a species or gene to be co-segregated with a disease is a key feature of an evolutionary response. It is widely believed that such co-segregation is the result of an association between the genetic variant and the disease, and that the association is causal. In this paper, we show that the co-segregation of a variant with a disease can often be explained by chance. Our results further show that the explanation of this association by chance is often a rare event. Moreover, we find that the selective advantage of a mutation at the locus associated with disease is significantly lower than the overall selective advantage of the mutation. ",chatgpt,1,generated_gpt3,chatgpt,0.999819815158844,"The ability of a species or gene to be co - segregated with a disease is a key feature of an evolutionary reception. It is wide believed that such co - segregation is the result of an association between the genetic variant and the disease, and that the association is causal. In this paper, we show that the co - segregation of a variant with a disease can often be explained by chance. Our results further show that the explanation of this association by chance is often a rare event. Moreover, we find that the selective advantage of a mutation at the locus associated with disease is significantly lower than the overall selective advantage of the mutation. ",+reception+wide,chatgpt,0.8716222047805786
34,150.0,1751,"In this paper, we study the problem of distinguishing a single positive integer from a set of positive integers. We present a new algorithm and prove that the result holds in general. The algorithm is based on the mean-square error (MSE) of the logarithm of the probability of the input. The proof is based on the result of Cheeger.",chatgpt,1,generated_gpt3,chatgpt,0.9993019104003906,"In this paper, we study the problem of distinguishing a unwedded positive whole number from a set of affirmatory integers. We present a new algorithm and prove that the result holds in full general. The algorithmic program is based on the mean - square error (MSE) of the logarithm of the probability of the input. The proof is based on the result of Cheeger. ",+unwedded+whole number+affirmatory+full general+algorithmic program,human,0.7604161500930786
35,151.0,2267,"In this paper, we study the problem of assessing the risk of a patient with a normal EEG and only a single seizure event in the past decade. We first analytically establish the relationship between seizure incidence and patient’s age, gender, and seizure duration. Then, we introduce a new method to quantify the probability of a patient having a seizure from a given EEG without any prior seizure history to judge the risk of a patient. The method uses the geometric mean of seizure onset and seizure duration to quantify the probability of a seizure occurring in any given time period. The method is applied to the population of patients with normal EEG and single seizure event, and the results show that the seizure risk is not an insurmountable barrier for some individuals, with the risk ranging from 0.25 to 0.9. The method may also be used to estimate the risk of a patient with a high quality of life who has never had a seizure in their life.",chatgpt,1,generated_gpt3,chatgpt,0.999786913394928,"In this paper, we study the problem of assessing the risk of a patient with a normal encephalogram and only a single seizure event in the past decennary. We first analytically establish the relationship between seizure incidence and patient ’s age, gender, and seizure duration. Then, we introduce a new method to quantify the probability of a patient having a seizure from a given EEG without any prior seizure history to judge the risk of a patient. The method uses the geometric mean of seizure onset and seizure duration to quantify the probability of a seizure occurring in any given time period. The method is applied to the population of patients with normal EEG and single seizure event, and the results show that the seizure risk is not an insurmountable barrier for some individuals, with the risk ranging from 0. 25 to 0. 9. The method may also be used to estimate the risk of a patient with a high quality of life who has never had a seizure in their life. ",+encephalogram+decennary,chatgpt,0.9998220801353455
36,152.0,2433,"The use of data mining techniques for clustering and classification of the data can be challenging, especially when the documents are sparse (e.g., less than 10% of the records have a value in the target field). The authors propose a simple yet robust method based on K-means clustering that handles sparse data and uses heuristics to identify clusters. This approach shows promising results and can be used in practice.",chatgpt,1,generated_gpt3,chatgpt,0.5399900674819946,"The use of goods and services of data mining techniques for clustering and categorisation of the information can be challenging, especially when the documents are sparse (e. g. , less than 10% of the records have a value in the target field). The authors propose a simple yet robust method based on K - means clustering that handles sparse data and uses heuristics to identify clusters. This approach shows promising results and can be used in practice. ",+use of goods and services+categorisation+information,human,0.9202739596366882
37,153.0,3018,"A firm is a collective entity that produces an output and can trade a surplus with other entities. The firm is owned by a group of agents, which are individuals that are associated with the firm. The firm and its agents transact with each other in a market for the sale and purchase of the firm´s output. The firm´s output is a function of the inputs supplied by its agents, which are individuals that provide inputs to the firm. We have studied the behavior of the firm, its agents and their interactions in a simplified model of the firm-agent, a possible microeconomic model of the firm. We consider the structure of the firm as a model of the microeconomy. Having established that the firm can trade a surplus with its agents, we investigate its behavior in a market for the sale and purchase of the firm’s output. The firm’s output is a function of its inputs. The model captures the main role played by the firm in the economy, which is",chatgpt,1,generated_gpt3,chatgpt,0.99952232837677,"A firm is a collective entity that produces an output and can trade a surplus with other entities. The firm is owned by a group of agents, which are individuals that are associated with the firm. The firm and its agents transact with each other in a market for the sale and buy of the firm´s output. The firm´s output is a function of the inputs supplied by its agents, which are individuals that provide inputs to the firm. We have studied the behavior of the firm, its agents and their interactions in a simplified model of the firm - agent, a possible microeconomic model of the firm. We consider the structure of the firm as a model of the microeconomy. Having established that the firm can trade a surplus with its agents, we investigate its behavior in a market for the sale and purchase of the firm ’s output. The firm ’s output is a function of its inputs. The model captures the main role played by the firm in the economy, which is",+buy,human,0.5929205417633057
38,154.0,3208,"In this paper we present the results of a new approach to the analysis of linear systems. We have used a four-parameter group-theoretic metric. The metric is constructed by connecting each point of the system with a path of the system. As a consequence, the metric can represent a wide variety of systems. We present a series of examples to illustrate the power of this approach. In particular, we have applied the metric to a number of different systems and found that the metric is well-suited to handle a wide range of examples. We also present a number of new features of the metric, some of which we have used in order to handle more complicated systems. Finally, we discuss the computational issues involved in the use of the metric.",chatgpt,1,generated_gpt3,chatgpt,0.9997791647911072,"In this newspaper publisher we present the results of a new approach to the analysis of unsubdivided systems. We have used a four - parameter mathematical group - theoretic metric. The rhythmical is constructed by connecting each point of the system with a path of the system. As a consequence, the metric can represent a wide variety of systems. We present a series of examples to illustrate the power of this approach. In particular, we have applied the metric to a number of different systems and found that the metric is well - suited to handle a wide range of examples. We also present a number of new features of the metric, some of which we have used in order to handle more complicated systems. Finally, we discuss the computational issues involved in the use of the metric. ",+newspaper publisher+unsubdivided+mathematical group+rhythmical,chatgpt,0.961275577545166
39,155.0,3211,"This paper is concerned with the problem of selecting and appraising schools to be included in different school ranking exercises. It is based on a study of the problem of ranking educational institutions, which has been undertaken by the British Government in their evaluation of the effectiveness of schools in England and Wales.",chatgpt,1,generated_gpt3,chatgpt,0.999122679233551,"This paper is concerned with the problem of selecting and appraising schools to be enclosed in different schooltime ranking exercises. It is based on a study of the problem of higher-ranking instructive institutions, which has been undertaken by the British Government in their valuation of the effectiveness of schools in England and cymru. ",+enclosed+schooltime+higher-ranking+instructive+valuation+cymru,chatgpt,0.9419578909873962
40,156.0,3255,"The core of the fast neutron reactor (FNR) is the sodium-cooled molten-salt reactor (MSR), which has been designed and constructed in the Soviet Union and in the United States. The MSR is characterized by a high power density, small size and high reactivity. The MSR has been used for the generation of electricity since the 1960s. The design of the MSR is based on a material balance approach, which considers the heat generation and the cooling system as the two main components of the core. The design of the MSR has been developed through a large set of experiments with test cells in order to verify the validity of this approach. The publications on the MSR core design are relatively few and these are not always of high quality. This paper analyzes the core design of the MSR by using the model of a heat generation and a cooling system. By using the model, which can be obtained by a linearized Blasius solution, we show",chatgpt,1,generated_gpt3,chatgpt,0.999799907207489,"The core of the fast neutron reactor (FNR) is the sodium - cooled molten - salt reactor (MSR), which has been designed and constructed in the Soviet Union and in the United States. The MSR is characterized by a high power density, small size and high reactivity. The MSR has been used for the generation of electricity since the 1960s. The design of the MSR is based on a material balance approach, which considers the heat generation and the cooling system as the two main components of the core. The design of the MSR has been developed through a large set of experiments with test cells in order to verify the validity of this approach. The publications on the MSR core design are relatively few and these are not always of high quality. This paper analyzes the core design of the MSR by using the model of a heat generation and a cooling system. By using the model, which can be obtained by a linearized Blasius solution, we show",,chatgpt,0.9998134970664978
41,157.0,3397,"The high-performance computing (HPC) sector has been experiencing unprecedented growth and development. Some of the HPC applications in the sector have been based on problems that have been successfully solved in scientific calculations. These applications have been widely used in the field of scientific research and industrial applications, but there are few studies on HPC applications in applied physics. In this paper, we introduce the concept of an ‘applied physics calculation’. An HPC application of an applied physics calculation is defined as a solution method used to solve a problem in the field of physics that is well-known in the HPC field. We introduce the following two new classes of HPC applications: (1) a problem whose solution method was well-known in the HPC field, and (2) a problem whose solution method was not well-known in the HPC field. The performance of the first class of applications was investigated by using a new benchmark that incorporates the calculation method of the problem and the",chatgpt,1,generated_gpt3,chatgpt,0.9997586607933044,"The high - performance computing (HPC) sector has been experiencing unprecedented growth and development. Some of the HPC applications in the sector have been based on problems that have been successfully resolved in scientific calculations. These applications have been widely used in the field of scientific research and industrial applications, but there are few studies on HPC applications in applied physics. In this paper, we introduce the concept of an ‘ applied physics calculation ’. An HPC application of an applied physics calculation is defined as a solution method used to solve a problem in the field of physics that is well - known in the HPC field. We introduce the following two new classes of HPC applications: (1) a problem whose solution method was well - known in the HPC field, and (2) a problem whose solution method was not well - known in the HPC field. The performance of the first class of applications was investigated by using a new benchmark that incorporates the calculation method of the problem and the",+resolved,chatgpt,0.9998180270195007
42,158.0,3664,"In the context of the empirical estimation of the cost of capital, we know the optimal cost of capital, the cost of capital that maximizes net present value of firm's cash flows. However, the cost of capital is affected by the joint distribution of the parameters of the cost of capital and the parameter of the cost of capital for the industry, etc. In this paper, we consider the joint distribution of these parameters, and we show that under certain assumptions, the optimal cost of capital is equivalent to the cost of capital that maximizes the net present value of the firm's cash flows. This is a key result which may be useful for the empirical estimation of the cost of capital.",chatgpt,1,generated_gpt3,chatgpt,0.9998266100883484,"In the context of the empirical estimation of the cost of capital, we know the optimum cost of capital, the cost of capital that maximizes net present value of firm's cash flows. However, the cost of capital is affected by the articulatio distribution of the parameters of the cost of capital and the parameter of the cost of capital for the industry, etc. In this paper, we consider the joint distribution of these parameters, and we show that under certain assumptions, the optimal cost of capital is equivalent to the cost of capital that maximizes the net present value of the unfluctuating's immediate payment flows. This is a key result which may be useful for the empirical estimation of the cost of capital. ",+optimum+articulatio+unfluctuating+immediate payment,chatgpt,0.8970947265625
43,159.0,4131,"In this paper, we study the different applications of the mathematical theory of random walks in bioinformatics. First, we present a simple and intuitive proof of the theorem that says that if the mean and variance of a function are constant for all walks on the same graph, then the function is a walk on the graph. Second, we show that for a given graph, the number of walks is the same for all walks on the same graph. Third, we show that the mean and variance of a function on the graph are constant as the graph is ‘folded’. The results are based on the theory of random walks on graphs, particularly the theory of random walks on graphs with a given mean and fixed variance. We assume that the graph is ‘relatively’ small and that the mean and variance are constant. In particular, we exclude the case where the graph is very small or the variance is very large.",chatgpt,1,generated_gpt3,chatgpt,0.999819815158844,"In this paper, we study the different applications of the mathematical theory of random walks in bioinformatics. First, we present a simple and intuitive proof of the theorem that says that if the mean and variance of a function are constant for all walks on the same graph, then the function is a walk on the graph. Second, we show that for a given graph, the number of walks is the same for all walks on the same graph. Third, we show that the mean and variance of a function on the graph are constant as the graph is ‘ folded ’. The results are based on the theory of random walks on graphs, particularly the theory of random walks on graphs with a given mean and fixed variance. We assume that the graph is ‘ relatively ’ small and that the mean and variance are constant. In particular, we exclude the case where the graph is very small or the variance is very large. ",,chatgpt,0.9998037219047546
44,160.0,4141,"This paper presents a novel algorithm called ‘minimal communication’ (MTC) that utilizes several existing metrics in the natural language processing (NLP) community to account for the inter-annotator disagreement in multiple-document corpora, a common problem in many applications. The resulting metric, which has been shown to be as good as other commonly used metrics, is used to derive a decision-theoretic model of evaluation and metric choice. The proposed metric is also shown to improve on existing metrics in the NLP community by accounting for the limited amount of information provided by the corpus. Once a decision-theoretic model of metric choice is derived, a set of rules can be used to construct a metric evaluation algorithm that is tailored to the specific application.",chatgpt,1,generated_gpt3,chatgpt,0.999822437763214,"This newspaper publisher presents a novel algorithm called ‘ minimal communication ’ (MTC) that utilizes several existing metrics in the natural language processing (NLP) community to account for the inter - annotator disagreement in multiple - document corpora, a common problem in many applications. The resulting metric, which has been shown to be as good as other commonly used metrics, is used to derive a decision - theoretic model of evaluation and metric choice. The proposed metric is also shown to improve on existing metrics in the NLP community by accounting for the limited amount of information provided by the corpus. Once a decision - theoretic model of metric choice is derived, a set of rules can be used to construct a metric evaluation algorithm that is tailored to the specific application. ",+newspaper publisher,chatgpt,0.9996795654296875
45,161.0,4864,We investigate the resolving power of a linear circuit based on the array of nonlinear differential equations that includes a nonlinear feedback. We consider a resonant circuit where the input to the circuit is a function of the output of the circuit. The output is determined by the nonlinear differential system. The circuit can be solved analytically. Our study suggests that some nonlinearities of the circuit such as nonlinearity around resonances and nonlinearity around the zero input point (ZIP) are important in determining the resolving power of the circuit.,chatgpt,1,generated_gpt3,chatgpt,0.9993763566017152,We investigate the resolving power of a unsubdivided circuit based on the array of nonlinear differential equations that includes a nonlinear feedback. We consider a resonant circuit where the input to the circuit is a function of the output of the circuit. The output is determined by the nonlinear differential system. The circuit can be solved analytically. Our study suggests that some nonlinearities of the circuit such as nonlinearity around resonances and nonlinearity around the zero input point (ZIP) are important in determining the resolving power of the circuit. ,+unsubdivided,human,0.9894116520881653
46,162.0,5200,"In this paper, we develop a new method to extract the most informative features and to design the appropriate classifier to predict the outcome of an experiment. We start with a set of features that are selected by a random forest classifier. Then we use a voting algorithm to refine the feature set. A classifier is designed to predict the outcome of an experiment. This paper introduces an asymmetric voting algorithm for any classifier that runs in an exponential time. The algorithm takes advantage of the fact that for any classifier, the logarithm of the number of votes cast is bounded by a constant. We show that there is always a classifier that runs in an exponential time and that the voting algorithm can be used to design the most informative features.",chatgpt,1,generated_gpt3,chatgpt,0.9998124241828918,"In this paper, we develop a new method to extract the most informative features and to design the called for classifier to predict the outcome of an experiment. We start with a set of features that are selected by a random timberland classifier. Then we use a voting algorithmic program to refine the feature set. A classifier is designed to predict the outcome of an experiment. This paper introduces an asymmetric voting algorithm for any classifier that runs in an exponential time. The algorithm takes advantage of the fact that for any classifier, the logarithm of the number of votes cast is bounded by a constant. We show that there is always a classifier that runs in an exponential time and that the voting algorithm can be used to design the most informative features. ",+called for+timberland+algorithmic program,chatgpt,0.885669469833374
47,163.0,5205,"Bacteria are a major cause of infectious diseases in general, and antibiotic resistance is one of the major challenges in human medicine. In this paper, we study the effects of antibiotic treatment on the population dynamics of bacterial cells given in a two-dimensional (2D) environment. We consider the case where treatment of bacteria with antibiotics is carried out within a closed system and the associated dynamics are determined by the rate of infection. The treatment of bacteria causes a negative feedback on the rate of cells being treated. Using a 2D model, we demonstrate that the dynamics of cells treated with antibiotics have a strong dependence on the rate of bacterial treatment. The results show that the dynamics of cells treated with antibiotics are nonlinear in the rate of treatment.",chatgpt,1,generated_gpt3,chatgpt,0.9998028874397278,"Bacteria are a major cause of infectious diseases in general, and antibiotic electrical resistance is one of the major challenges in human medicine. In this newspaper publisher, we study the effects of antibiotic drug treatment on the population kinetics of bacterial cells donated in a two - dimensional (2D) environment. We consider the case where treatment of bacteria with antibiotics is carried out within a closed system and the associated dynamics are determined by the rate of infection. The treatment of bacteria causes a negative feedback on the rate of cells being treated. Using a 2D model, we demonstrate that the dynamics of cells treated with antibiotics have a strong dependence on the rate of bacterial treatment. The results show that the dynamics of cells treated with antibiotics are nonlinear in the rate of treatment. ",+electrical resistance+newspaper publisher+antibiotic drug+kinetics+donated,human,0.5773718953132629
48,164.0,6201,"The paper presents a model of the functioning of the human information processing system and a model of its operation. The model is based on the theory of mechanism of the human information processing system, which is based on the theory of the response to information. The first part of the model is the mechanism of the human information processing system, which is the working mechanism of the human information processing system. The second part is the model based on the theory of the response to information, which is the model of the human information processing system in accordance with the response to information. The model of the human information processing system and the model of the human information processing system in accordance with the response to information are solved simultaneously. Keywords: human information processing system, human information processing system in accordance with the response to information, human information processing system, information processing system.",chatgpt,1,generated_gpt3,chatgpt,0.9997928738594056,"The newspaper publisher presents a model of the functional of the human information processing system and a model of its operation. The model is based on the theory of mechanism of the human information processing system, which is based on the theory of the response to information. The first part of the model is the mechanism of the human information processing system, which is the on the job mechanism of the human information processing system. The second part is the model based on the theory of the reception to information, which is the model of the human information processing system in accordance with the response to information. The model of the human information processing system and the model of the human information processing system in accordance with the response to information are solved simultaneously. Keywords: hominine information processing system, human information processing system in accordance with the response to information, human information processing system, information processing system. ",+newspaper publisher+functional+on the job+reception+hominine,chatgpt,0.9998164772987366
49,165.0,6438,"One of the biggest challenges of the design of a new pharmaceutical agent is the choice of optimal dosage. The choice of dosing regimen is a subjective choice that is based on the expected benefits and risks of using the drug, as well as on the preferences of the patient. The optimal dosing regimen is a complex decision that is sensitive to a variety of factors. This paper introduces a new method for finding the optimal dose in a simple and unbiased way. The method is based on the multiple-dose response (MDR) model and the single-dose response (SDR) model. The general idea of the MDR model is to use a fixed dose of the drug for a fixed number of days and multiple doses of the drug for each day. The MDR model captures the so-called ‘fixed-dose effect’, which is observed in many pharmaceutical trials. Then, the SDR model is introduced in which the dose of drug is determined by the amount of drug required to achieve",chatgpt,1,generated_gpt3,chatgpt,0.9997861981391908,"One of the biggest challenges of the design of a new pharmaceutical agent is the choice of optimal dosage. The choice of dosing regimen is a subjective choice that is based on the expected benefits and risks of using the drug, as well as on the preferences of the patient. The optimum dosing regimen is a complex decision that is sensitive to a variety of factors. This paper introduces a new method for finding the optimal dose in a simple and unbiased way. The method is based on the multiple - dose response (MDR) model and the single - dose response (SDR) model. The general idea of the MDR model is to use a fixed dose of the drug for a fixed number of days and multiple doses of the drug for each day. The MDR model captures the so - called ‘ fixed - dose effect ’, which is observed in many pharmaceutical trials. Then, the SDR model is introduced in which the dose of drug is determined by the amount of drug required to achieve",+optimum,chatgpt,0.9995421171188354
50,166.0,6503,We present a new method for stochastic analysis of the performance of insurance markets. We use a method for the calculation of the price of an option to buy a fixed amount of insurance for a given period of time. The method is based on the application of a simplified version of the Black–Scholes option pricing model. We demonstrate the method's effectiveness by applying it to analyze the performance of different European insurance market schemes.,chatgpt,1,generated_gpt3,chatgpt,0.9994373917579652,We present a new method for stochastic analysis of the performance of insurance markets. We use a method for the calculation of the price of an choice to buy a fixed amount of insurance for a given period of clock time. The method is based on the application of a simplified interlingual rendition of the black person – Scholes option pricing model. We demonstrate the method's effectiveness by applying it to analyze the performance of different European insurance market schemes. ,+choice+clock time+interlingual rendition+black person,human,0.7952148914337158
51,167.0,6730,"The J-function is an important tool to study the long memory of the ordinary differential equations. It is a function which transforms the space of functions into a finite-dimensional vector space. Although it can be used as a tool to study the stability of a set of slowly-varying linear differential equations, it also has a number of other useful properties. In this paper, we present a rigorous proof of the following results:",chatgpt,1,generated_gpt3,chatgpt,0.999657392501831,"The J - function is an important tool to study the long memory board of the ordinary differential equations. It is a mathematical function which transforms the topological space of functions into a tensed - dimensional vector space. Although it can be used as a tool to study the stability of a set of tardily - varying unsubdivided differential coefficient equations, it also has a number of other useful properties. In this paper, we present a rigorous proof of the undermentioned results:",+memory board+mathematical function+topological space+tensed+tardily+unsubdivided+differential coefficient+undermentioned,chatgpt,0.9478119611740112
52,168.0,7113,"A two-dimensional general purpose computer system, comprising a processor and a memory, is disclosed. The processor includes logic for receiving, at the processor and at a remote computer, data relating to a product, and for causing the processor to generate, based on the data, a list of the product's attributes that are applicable to the product, the attributes being arranged in a hierarchy. The list is transmitted to the remote computer and the remote computer is caused to transmit back, to the processor, a list of the product's attributes that are applicable to the product. The list of the product's attributes that are applicable to the product is then used to generate, at the processor, a list of product attributes that are applicable to the product that matches the list of attributes that are applicable to the product provided by the remote computer.",chatgpt,1,generated_gpt3,chatgpt,0.9997590184211732,"A two - dimensional general purpose computer system, comprising a processor and a memory, is disclosed. The processor includes system of logic for receiving, at the processor and at a remote control computer, data relating to a product, and for causation the processor to generate, based on the data, a list of the product's attributes that are applicable to the product, the attributes being staged in a hierarchy. The list is transmitted to the remote computer and the remote computer is caused to transmit back, to the processor, a list of the product's attributes that are applicable to the product. The list of the product's attributes that are applicable to the product is then used to generate, at the processor, a list of product attributes that are applicable to the product that matches the list of attributes that are applicable to the product provided by the remote computer. ",+system of logic+remote control+causation+staged,human,0.8415691256523132
53,169.0,7171,"In this paper, we first discuss a basic idea of a multi-item social choice rule: to have a utility function that is invariant under permutations, a voter must prefer items with equal utilities when they are switched. We then motivate an alternative rule: to have a utility function that is invariant under swaps of any two items, a voter should prefer items with equal utilities when they are switched. We show that these two rules lead to different rule families for social choice. In particular, we show that the difference between our two rule families is greater than their similarities, and we provide a simple proof for this result. In Section 3, we illustrate our results by considering two simple applications: decision-making under incomplete information, and the allocation of limited resources. We show that the two rule families are not only different but also lead to different answers for these applications. Finally, Section 4 shows that our results generalize to the case of several item sets.",chatgpt,1,generated_gpt3,human,0.9778169989585876,"In this paper, we first discuss a basic idea of a multiple - token swarming choice rule: to have a utility function that is invariant under permutations, a voter must prefer items with equal utilities when they are switched. We then motivate an alternative rule: to have a utility function that is invariant under swaps of any two items, a voter should prefer items with equal utilities when they are switched. We show that these two rules lead to different rule families for social choice. In particular, we show that the difference between our two rule families is greater than their similarities, and we provide a simple proof for this result. In Section 3, we illustrate our results by considering two simple applications: decision - making under incomplete information, and the allocation of limited resources. We show that the two rule families are not only different but also lead to different answers for these applications. Finally, Section 4 shows that our results generalize to the case of several item sets. ",+multiple+token+swarming,human,0.9691499471664429
54,170.0,7395,"A central tenet of public health is that the best way to reduce disease is by directly and indirectly influencing the behavior of individuals. In the case of the influenza virus, this means manipulating the behavior of the individual based on their level of immunization and susceptibility to infection. This paper evaluates the impact of the vaccination of individuals on the level of influenza virus circulation in a community, as well as the impact of the vaccination of a subset on the level of influenza virus circulation in a community. In the model, individuals have the propensity to have a certain level of vaccination and, if they have the propensity, they are vaccinated. The vaccine is distributed among individuals in the community in such a way that the individuals who are most immunized have the least number of unvaccinated individuals in their community. The model is simulated by considering a community of individuals who are immunized or not immunized, with the individuals who are not immunized having the propensity to be vaccinated. The community is divided into subsets that",chatgpt,1,generated_gpt3,chatgpt,0.9997538924217224,"A central tenet of public health is that the best way to reduce disease is by directly and indirectly influencing the behavior of individuals. In the case of the influenza virus, this means manipulating the behavior of the individual based on their level of immunization and susceptibility to infection. This newspaper publisher evaluates the impact of the vaccination of individuals on the horizontal surface of influenza virus circulation in a community, as well as the impact of the vaccination of a subset on the level of influenza virus circulation in a community. In the theoretical account, individuals have the propensity to have a certain level of vaccination and, if they have the aptness, they are vaccinated. The vaccine is distributed among individuals in the community in such a way that the individuals who are most immunized have the least number of unvaccinated individuals in their community. The model is simulated by considering a community of individuals who are immunized or not immunized, with the individuals who are not immunized having the propensity to be vaccinated. The community is divided into subsets that",+newspaper publisher+horizontal surface+theoretical account+aptness,chatgpt,0.999591052532196
55,171.0,7430,"A system of linear equations having a single solution is called homogeneous. There exist index sets for which a homogeneous system may not be solved completely. This raises a question on the nature of solutions in homogeneous systems. In this paper, we consider the infinitary problem of finding a complete solution to a homogeneous system of equations. The framework is based on Hahn’s construction. We apply this method to the problem of finding a solution to a large class of systems of equations and show that there are homogeneous solutions that cannot be described by any small finite set. We then show that larger solutions can be found by using iterative methods. We finally show that repeated application of Hahn’s method eventually leads to a solution to the whole class of systems of equations.",chatgpt,1,generated_gpt3,chatgpt,0.9029027819633484,"A system of linear equations having a single solution is called homogeneous. in that location exist index sets for which a homogeneous system may not be solved completely. This raises a question on the natural phenomenon of solutions in homogeneous systems. In this paper, we consider the infinitary problem of finding a complete solution to a homogeneous system of equations. The framework is based on Hahn ’s construction. We apply this method to the problem of finding a solution to a large class of systems of equations and show that there are homogeneous solutions that cannot be described by any small finite set. We then show that larger solutions can be found by using iterative methods. We finally show that repeated application of Hahn ’s method eventually leads to a solution to the whole class of systems of equations. ",+in that location+natural phenomenon,human,0.9757238030433655
56,172.0,7453,"The main aim of this study was to investigate the relationship between the weight-loss behavior of multiple sclerosis patients and the severity of the disease. The primary objective was to study the correlation among the three weight-loss behaviors, namely, diet and exercise, food intake restriction and exercise, and the severity of the disease. We completed a PubMed search to identify relevant studies conducted in the English language. Fifteen studies were selected for study. The studies were analyzed, and the results were summarized as follows: We found that the correlation between the severity of the disease and the weight-loss behaviors were not significant in most of the studies. On the other hand, the correlation between the weight-loss behaviors and the severity of the disease were significant. The correlation between the three weight-loss behaviors and the severity of the disease were significant in many of the studies. These results suggest that more emphasis should be placed on the severity of the disease in the treatment of multiple sclerosis patients.",chatgpt,1,generated_gpt3,chatgpt,0.9998182654380798,"The main aim of this study was to investigate the relationship between the weight - loss behavior of multiple sclerosis patients and the severity of the disease. The primary objective was to study the correlation among the deuce-ace weight - loss behaviors, that is to say, low-cal and exercise, food intake limitation and exercise, and the severity of the disease. We completed a PubMed search to identify relevant studies conducted in the English language. Fifteen studies were selected for study. The studies were analyzed, and the results were summarized as follows: We found that the correlation between the severity of the disease and the weight - loss behaviors were not significant in most of the studies. On the other hand, the correlation between the weight - loss behaviors and the severity of the disease were significant. The correlation between the three weight - loss behaviors and the severity of the disease were significant in many of the studies. These results suggest that more emphasis should be placed on the severity of the disease in the treatment of multiple sclerosis patients. ",+deuce-ace+that is to say+low-cal+limitation,chatgpt,0.8840566277503967
57,173.0,8416,"In this paper, we provide a detailed study of a family of continuous stochastic differential equations named the “Stochastic Reproduction-Transmission Kinetics” (SRTK) with a discrete-time Markovian dynamics. We derive the closed form of the SRTK equation, which allows us to derive the discrete-time SRTK equation to solve for the dynamics of the specific cases of interest. Furthermore, we show that the SRTK equation is a special case of another general stochastic differential equation, the “Stochastic Evolution-Transmission Kinetics” (SETK), with a continuous-time Markovian dynamics. This journal is on-line at: http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1522-4959/issue/14/ ",chatgpt,1,generated_gpt3,chatgpt,0.9998193383216858,"In this paper, we provide a detailed study of a family of continuous stochastic differential equations onymous the “ Stochastic Reproduction - Transmission Kinetics ” (SRTK) with a discrete - time Markovian dynamics. We derive the closed form of the SRTK equation, which allows us to derive the discrete - time SRTK equation to solve for the dynamics of the specific cases of interest. Furthermore, we show that the SRTK equation is a special case of another general stochastic differential equation, the “ Stochastic Evolution - Transmission Kinetics ” (SETK), with a continuous - time Markovian dynamics. This journal is on - line at: http://onlinelibrary. wiley. com/journal/10. 1111/(ISSN)1522-4959/issue/14/",+onymous,chatgpt,0.993238091468811
58,174.0,9259,"Sparse linear regression is an effective method of analyzing the spatial dimension of a data set. Nevertheless, the sparsity structure of linear regression can be misleading as it ignores the nonlinearity of the data. To overcome this limitation, we propose a novel nonlinear regression model and analyze its performance empirically. We consider a data set of the size of a square of side length (6,4) and calculate the predictive and error averages (mean square error and mean square prediction error). We find that our model outperforms both the LASSO and ridge regression models. The quality of the predictions based on this model is better than the error-only predictions of the LASSO model or ridge regression model and is comparable to the predictive quality of the Bayesian model. Moreover, we show that the model is robust to the choice of the coordinate transformation. The results suggest that the nonlinearity of the data is not ignored when the data are analyzed using the sparse linear regression model.",chatgpt,1,generated_gpt3,chatgpt,0.8624612092971802,"Sparse linear regression is an effective method of analyzing the spatial dimension of a data set. even so, the sparsity structure of linear regression can be misleading as it ignores the nonlinearity of the data. To overcome this limitation, we propose a novel nonlinear regression model and analyze its performance empirically. We consider a data set of the size of a square of side length (6, 4) and calculate the predictive and error averages (mean square error and mean square prediction erroneous belief). We find that our model outperforms both the LASSO and ridge regression models. The quality of the predictions based on this model is better than the error - only predictions of the LASSO model or ridge regression model and is comparable to the predictive quality of the Bayesian model. Moreover, we show that the model is robust to the choice of the coordinate transformation. The results suggest that the nonlinearity of the data is not ignored when the data are analyzed using the sparse linear regression model. ",+even so+erroneous belief,human,0.9995323419570923
59,175.0,9538,"In this paper, we analyze the effect of interaction mechanisms of interest on the efficiency of an economic auction. We derive a new invariant measure of performance to measure the efficiency of auction rules in terms of an auctioneer’s profit, in the case where the interaction mechanism is a repeated game. We show that the dependence of the auctioneer’s profit on the interaction mechanism is A-shaped, and that the efficiency of an auction is significantly higher than that of a pure two-sided auction.",chatgpt,1,generated_gpt3,chatgpt,0.9998071789741516,"In this paper, we analyze the effect of interaction mechanisms of interest on the efficiency of an economic auction. We derive a new invariant measuring rod of carrying into action to measuring rod the efficiency of auction rules in terms of an auctioneer ’s profit, in the case where the interaction mechanism is a repeated game. We show that the dependence of the auctioneer ’s profit on the interaction mechanism is A - shaped, and that the efficiency of an auction is significantly higher than that of a pure two - sided auction. ",+measuring rod+carrying into action+measuring rod,chatgpt,0.8333967328071594
60,176.0,9559,"In this paper, we study the effect of advertising on the efficiency of a competitive industry. We consider a market consisting of two firms with identical products, which compete in the absence of advertising. In our numerical simulations, we find that advertising is not only efficient at improving welfare, but it also enhances welfare. Our results are in general agreement with the economic theory of advertising, although we show that the change in welfare is not due to the price effect of advertising but to the product effect of advertising.",chatgpt,1,generated_gpt3,chatgpt,0.9998143315315248,"In this paper, we study the effect of advertising on the efficiency of a competitive industry. We consider a securities industry consisting of two firms with superposable products, which compete in the absence of publicizing. In our denotive simulations, we find that advertising is not only efficient at rising welfare, but it also enhances welfare. Our results are in full general concord with the economic theory of advertising, although we show that the deepen in welfare-statist is not due to the price effect of advertising but to the product effect of advertising. ",+securities industry+superposable+publicizing+denotive+rising+full general+concord+deepen+welfare-statist,human,0.9464643001556396
61,177.0,10341,"In this paper, we use a probabilistic model to explore the relationship between the location of a plant’s leaves and other plant traits. The model is based on the concept of internal correlation, which is a model that takes into account the correlation between a plant’s traits (X and Y) but does not depend on any prior distribution of the traits. Internal correlations are commonly used to predict the locations of leaves. We calculate the internal correlation for five traits, and then use the model to predict the locations of leaves for various combinations of traits. The predictions show significant variation depending on the trait combinations. We suggest that internal correlations are useful for predicting the locations of leaves, but that they must be combined with other models, such as the random walk or the distance decay model, to make predictions that are more accurate. In addition, we show that the locations of leaves are correlated with the traits, but that this correlation depends on the trait combinations. Moreover, we show that the location",chatgpt,1,generated_gpt3,chatgpt,0.9998026490211488,"In this paper, we use a probabilistic model to explore the relationship between the location of a plant ’s leaves and other plant traits. The model is based on the concept of internal correlation, which is a model that takes into account the correlation between a plant ’s traits (X and Y) but does not depend on any prior distribution of the traits. Internal correlations are commonly used to predict the locations of leaves. We calculate the internal correlation for five traits, and then use the model to predict the locations of leaves for various combinations of traits. The predictions show significant variation depending on the trait combinations. We suggest that internal correlations are useful for predicting the locations of leaves, but that they must be combined with other models, such as the random walk or the distance decay model, to make predictions that are more accurate. In addition, we show that the locations of leaves are correlated with the traits, but that this correlation depends on the trait combinations. Moreover, we show that the location",,chatgpt,0.9998266100883484
62,178.0,10899,"A project has been initiated to collect data on the performance of eight different types of public housing in the City of Edmonton, including two projects that were designed by the City of Edmonton. A database of the performance is being assembled to facilitate comparisons of the relative performance of different types of public housing. The database will include information on building and site characteristics, tenant characteristics, characteristics of the tenant's household and the type of building. The database will also include information on the physical characteristics of the buildings, such as the type of roof, insulation and cladding. It is expected that the database will be useful to other housing management agencies and researchers.",chatgpt,1,generated_gpt3,chatgpt,0.9998096823692322,"A project has been initiated to collect data on the performance of eight different types of public housing in the City of Edmonton, including two projects that were designed by the City of Edmonton. A database of the performance is being assembled to facilitate comparisons of the congenator performance of different types of public housings. The database will include information on building and site characteristics, renter characteristics, characteristics of the tenant's household and the type of building. The database will also include information on the physical characteristics of the buildings, such as the type of roof, insulation and cladding. It is expected that the database will be useful to other housing managing agencies and researchers. ",+congenator+housings+renter+managing,human,0.991689145565033
63,179.0,11009,"In this paper, we study the determinants of the quality of the national health insurance (NHI) services at the scale of the districts in Vietnam. We use a complementary logit model to analyze the effects of the NHI coverage and the NHI access to health professionals on the quality of the NHI services at the scale of the districts in Vietnam. We test for the under- or overestimation of the effects of NHI coverage and NHI access to health professionals on the quality of the NHI services at the scale of the districts in Vietnam. We find that both the NHI coverage and the NHI access to health professionals have significant effects on the quality of the NHI services at the scale of the districts in Vietnam. The results of the study could be used to improve the design and delivery of the NHI in Vietnam.",chatgpt,1,generated_gpt3,chatgpt,0.99982351064682,"In this paper, we study the determinants of the quality of the national health insurance (NHI) services at the scale of the districts in viet nam. We use a complemental logit model to analyze the effects of the NHI coverage and the NHI access to health professionals on the quality of the NHI services at the scale of the districts in Vietnam. We test for the under- or overestimation of the effects of NHI coverage and NHI access to health professionals on the quality of the NHI services at the scale of the districts in Vietnam. We find that both the NHI coverage and the NHI access to health professionals have significant effects on the quality of the NHI services at the scale of the districts in Vietnam. The results of the study could be used to improve the design and delivery of the NHI in Vietnam. ",+viet nam+complemental,chatgpt,0.997603714466095
64,180.0,11095,"In this article we introduce a new method to deal with the general case of the probit model with continuous, i.e. discrete, response variables. As a result, the probit model with continuous response variables can be considered as a special case of the generalized linear model (GLM). In our approach, the probit model with continuous response variables is divided into two parts and these parts are connected by a linear constraint. For the first part, we introduce a new method to deal with the probit model with continuous, i.e. discrete, response variables. The method is based on the difference between the cumulative distribution function of the logit model and that of the probit model. For this purpose, we introduce a new distribution function F(x), which is a difference of the logit and probit distributions. The method is computationally efficient and yields unbiased estimates for the parameters in the probit model with continuous response variables. Consistent estimates for the parameters of the probit",chatgpt,1,generated_gpt3,chatgpt,0.9997997879981996,"In this article we introduce a new method to deal with the general case of the probit model with continuous, i. e. discrete, response variables. As a result, the probit model with continuous response variables can be considered as a special case of the generalized linear model (GLM). In our approach, the probit model with continuous response variables is divided into two parts and these parts are connected by a linear constraint. For the first part, we introduce a new method to deal with the probit model with continuous, i. e. discrete, response variables. The method is based on the difference between the cumulative distribution function of the logit model and that of the probit model. For this purpose, we introduce a new distribution function F(x), which is a difference of the logit and probit distributions. The method is computationally efficient and yields unbiased estimates for the parameters in the probit model with continuous response variables. Consistent estimates for the parameters of the probit",,chatgpt,0.9998027682304382
65,181.0,11167,"Revenue from the sale of real estate is one of the most important sources of income for the real estate sector. The sale or lease of a property can be a complex process. In this paper, we propose a system GMM model based on pseudo-panel data for the analysis of the sale process for residential properties. To mitigate possible endogeneity, we control for cohort effects while also attenuating dynamic bias in the estimation from a dynamic perspective. We provide robust estimates based on multi-model regression. The results show that (1) the revenue from the sale of residential properties depends on the sale price, (2) the revenue per square meter varies with the number of bedrooms, and (3) the revenue per square meter varies with the number of bathrooms. The results are also consistent with the real estate market data. We conclude that the revenue from the sale of residential properties depends on the sale price and the number of bedrooms and bathrooms.",chatgpt,1,generated_gpt3,chatgpt,0.8143534660339355,"Revenue from the sale of real estate is one of the most of import sources of income for the real estate of the realm sector. The sale or lease of a material possession can be a building complex process. In this paper, we propose a system GMM model based on pseudo - panel data for the analysis of the sale process for residential properties. To mitigate possible endogeneity, we control for cohort effects while also attenuating dynamic bias in the estimation from a dynamic perspective. We provide robust estimates based on multi - model regression. The results show that (1) the revenue from the sale of residential properties depends on the sale price, (2) the revenue per square meter varies with the number of bedrooms, and (3) the revenue per square meter varies with the number of bathrooms. The results are also consistent with the real estate market data. We conclude that the revenue from the sale of residential properties depends on the sale price and the number of bedrooms and bathrooms. ",+of import+estate of the realm+material possession+building complex,human,0.9684928059577942
66,182.0,11344,"The mechanism of sleep-wake transitions is the topic of intense research. In this paper we consider the possibility of sleep initiation at the intermediate level of a hierarchical state-space model. In this model, sleep initiation is governed by a bifurcation in a sequence of non-linear, non-Gaussian states. We propose a dynamical system-based method for determining the parameter values that produce sleep initiation. The method imposes a minimal number of restrictions on the state of the model, based on the known properties of the sleep-wake mechanism. We use the method to determine the values of the parameters for a hierarchical model of sleep initiation. The results are consistent with existing experimental evidence, and the technique can be used to produce more realistic parameter values.",chatgpt,1,generated_gpt3,chatgpt,0.999817669391632,"The mechanism of sleep - wake island transitions is the theme of blood-and-guts research. In this paper we consider the possibility of sleep initiation at the intermediate level of a hierarchical state - space model. In this model, eternal rest initiation is governed by a bifurcation in a sequence of non - linear, non - Gaussian states. We propose a dynamical system - based method for determining the parameter values that produce sleep initiation. The method imposes a minimal number of restrictions on the state of the model, based on the known properties of the sleep - wake mechanism. We use the method to determine the values of the parameters for a hierarchical model of sleep initiation. The results are consistent with existing experimental evidence, and the technique can be used to produce more realistic parameter values. ",+wake island+theme+blood-and-guts+eternal rest,human,0.9949719905853271
67,183.0,11418,"The authors examine the relationship between the relative price of goods and their demand in the real world, and analyze the effect of the relative price of goods on the distribution of income. They consider a model where goods are produced in a continuum and are consumed in a continuum. The model predicts that the relative price of goods can have an impact on the distribution of income. The paper is based on the log-linear model of a beverage distribution system.",chatgpt,1,generated_gpt3,chatgpt,0.9998220801353456,"The authors examine the relationship between the relative price of goods and their demand in the real worldly concern, and analyze the effect of the congenator monetary value of commodity on the distribution of income. They consider a model where goods are produced in a continuum and are consumed in a continuum. The theoretical account predicts that the relative price of goods can have an impact on the distribution of income. The paper is based on the log - linear model of a drinkable distribution system. ",+worldly concern+congenator+monetary value+commodity+theoretical account+drinkable,human,0.8609434962272644
68,184.0,11469,"Consensus identification of the best sequence for DNA synthesis is a vital step for the successful synthesis of DNA molecules. The consensus sequence is that of a longest homopolymer in a series of synthesized molecules. Sequence identification requires knowledge of the sequence of at least one primer. In this paper, we study the problem of consensus identification of the best DNA synthesis sequence in the presence of multiple primers. We address the problem by studying the evolutionary dynamics of DNA synthesis. We assume that the primers are produced at random from a finite population. We assume that the primers are more similar to each other than to the consensus sequence. We show that the dynamics of the synthesis process are governed by two well-known results in population genetics, the birth-death process and the Pareto distribution. The evolution of the consensus sequence is driven by a birth-death process. The population size is limited by the number of primers available. We show that the process is influenced by the distribution of primers,",chatgpt,1,generated_gpt3,chatgpt,0.9997931122779846,"Consensus identification of the best sequence for DNA synthetic thinking is a vital step for the successful synthesis of DNA molecules. The general agreement sequence is that of a longer homopolymer in a series of synthesized molecules. Sequence identification requires knowledge of the sequence of at least one primer. In this paper, we study the problem of consensus identification of the best DNA synthesis sequence in the presence of multiple primers. We address the problem by studying the evolutionary dynamics of DNA synthesis. We assume that the primers are produced at random from a finite population. We assume that the primers are more similar to each other than to the consensus sequence. We show that the dynamics of the synthesis process are governed by two well - known results in population genetics, the birth - death process and the Pareto distribution. The evolution of the consensus sequence is driven by a birth - death process. The population size is limited by the number of primers available. We show that the process is influenced by the distribution of primers, ",+synthetic thinking+general agreement+longer,chatgpt,0.9997630715370178
69,185.0,11787,"We study the effects of the presence of a heterogeneous population on the pattern of species diversity in a spatial context. We outline a model in which the species are independent and it is possible to obtain the equilibrium species diversity. We show that the equilibrium species diversity is determined by the spatial structure of the population. We show that the presence of a heterogeneous population can lead to an increase in equilibrium diversity. We also show that when the species are independent, the presence of a heterogeneous population does not have any effect on the equilibrium species diversity.",chatgpt,1,generated_gpt3,chatgpt,0.9998144507408142,"We study the effects of the presence of a heterogeneous population on the pattern of species diversity in a spacial context. We chalk out a theoretical account in which the species are nonsymbiotic and it is possible to obtain the equilibrium species diversity. We show that the sense of equilibrium species diversity is determined by the spatial structure of the population. We show that the presence of a heterogeneous population can lead to an increase in equilibrium diversity. We also show that when the species are independent, the presence of a heterogeneous population does not have any effect on the equilibrium species diversity. ",+spacial+chalk out+theoretical account+nonsymbiotic+sense of equilibrium,chatgpt,0.7835613489151001
70,186.0,11934,"The decision making process of an agent is not deterministic. In this paper we consider the influence of different factors that can influence the decision making process of an agent at the same time. The factors can be divided into two types depending upon their influence on the decision making process. First type of factors are those that affect the decision making process through the decision utility function. The second type of factors are those that affect the decision making process through the probability distribution. The influence of both types of factors is considered in the setting of linear probability models and the impact is studied by performing a laddering analysis of the decision utility function. The results show that the influence of the decision utility function on the decision making process is much more important than the influence of the probability distribution on the decision making process. The basic ability of an agent to take a decision can be shown to be dependent on its ability to process information about the environment. The dependency is more pronounced for higher levels of the decision utility function. In addition,",chatgpt,1,generated_gpt3,chatgpt,0.99980491399765,"The decision making process of an agent is not deterministic. In this paper we consider the influence of different factors that can influence the decision fashioning process of an agent at the same time. The factors can be divided into two types depending upon their act upon on the decision making process. First type of factors are those that affect the decision making process through the decision utility function. The second type of factors are those that affect the decision making process through the probability distribution. The influence of both types of factors is considered in the setting of linear probability models and the impact is designed by performing a laddering analysis of the decision utility function. The results show that the influence of the decision utility function on the decision making process is much more important than the influence of the probability distribution on the decision making process. The basic ability of an agent to take a decision can be shown to be dependent on its ability to process information about the environment. The dependance is more pronounced for higher levels of the decision service program function. In addition, ",+fashioning+act upon+designed+dependance+service program,human,0.6496883630752563
71,187.0,12094,"In this paper, we show that the local optimality of the market allocation depends on the number of firms in the market. Our analysis applies to both the continuous and the discrete models of the market with a homogeneous product. In the continuous model, we show that the market efficiency of the continuous model is obtained by using the local optimality. In the discrete model, we show that the market efficiency of the discrete model is obtained by using the local optimality and the market efficiencies of the continuous model and the discrete model are equal. ",chatgpt,1,generated_gpt3,chatgpt,0.9998263716697692,"In this paper, we show that the local optimality of the market parceling depends on the number of firms in the market. Our analysis applies to both the continuous and the discrete models of the market with a homogeneous product. In the free burning model, we show that the market efficiency of the continuous theoretical account is obtained by using the local optimality. In the discrete model, we show that the market efficiency of the discrete model is obtained by using the local optimality and the market efficiencies of the continuous model and the discrete model are equal. ",+parceling+free burning+theoretical account,chatgpt,0.9954929351806641
72,188.0,12176,"The present paper introduces a new concept for studying the phenomenon of ‘gene flow’; the concept of a gene flow network. In this paper, we also introduce the concept of a gene flow network tree. With the aid of these two concepts, we propose a new method of measuring gene flow that is applied to the analysis of social networks. By comparing the analysis of a gene flow network tree with the traditional network analysis, we show that there is a much stronger relationship between the two social networks than previously recognized. This is particularly significant to the study of gene flow networks. The new method is also applied to the analysis of social networks in the social sciences.",chatgpt,1,generated_gpt3,chatgpt,0.9998077750205994,"The present newspaper publisher introduces a new concept for studying the phenomenon of ‘ gene flow ’; the concept of a gene flow network. In this paper, we also introduce the concept of a gene flow network tree. With the aid of these two concepts, we propose a new method of measuring gene flow that is applied to the analysis of social networks. By comparing the analysis of a gene flow network tree with the traditional network analysis, we show that there is a much stronger relationship between the two social networks than previously recognized. This is particularly significant to the study of gene flow networks. The new method is also applied to the analysis of social networks in the social sciences. ",+newspaper publisher,chatgpt,0.9998146891593933
73,189.0,12259,"In this paper, we consider the role of prospect theory (PT) in the decision of a consumer to purchase a certain product. We present a theoretical analysis of the utility function of a consumer in the context of a PT utility function and a choice model. We then make a number of simulation results and show that, when a consumer acts in the context of a PT utility function, the utility of the choice is non-additive. We also show that the average utility of the choice is non-additive, and that the average utility of the choice is lower for a consumer that is risk averse. In the case of a consumer that is risk neutral, the utility of the choice is additive. In the case of a consumer that is risk seeking, the utility of the choice is non-additive. We also study the pricing of this product. We find that the consumer is willing to purchase the product at a price that is less than the expected utility of the consumer. We also find",chatgpt,1,generated_gpt3,chatgpt,0.9998024106025696,"In this paper, we consider the role of prospect theory (PT) in the decision of a consumer to purchase a certain product. We present a theory-based analysis of the utility function of a consumer in the context of a PT utility function and a tasteful model. We then make a identification number of pretending results and show that, when a consumer acts in the context of a atomic number 78 utility function, the utility of the choice is non - additive. We also show that the average utility of the choice is non - additive, and that the average utility of the choice is lower for a consumer that is risk averse. In the case of a consumer that is risk neutral, the utility of the choice is additive. In the case of a consumer that is risk seeking, the utility of the choice is non - additive. We also study the pricing of this product. We find that the consumer is willing to purchase the product at a price that is less than the expected utility of the consumer. We also find",+theory-based+tasteful+identification number+pretending+atomic number 78,human,0.7421929240226746
74,190.0,12955,"In this paper, we study the structure of a family of random polytope whose faces are all of the same general type. We show that in the polytope, the density of the faces is given by the squared Bessel function of zero order or higher. We also show that the polytope is self-similar and that the polytope parameters are closely related to the volume of the polytope. Furthermore, we show that the polytope is non-triangulated and that the polytope is not all of the same type. We also show that the number of edges in the polytope is the same as the number of faces in the polytope.",chatgpt,1,generated_gpt3,chatgpt,0.9998138546943665,"In this newspaper publisher, we study the structure of a family of hit-or-miss polytope whose faces are all of the same full general type. We show that in the polytope, the density of the faces is given by the squared Bessel function of zero order or higher. We also show that the polytope is self - similar and that the polytope parameters are closely related to the volume of the polytope. Furthermore, we show that the polytope is non - triangulated and that the polytope is not all of the same type. We also show that the number of edges in the polytope is the same as the number of faces in the polytope. ",+newspaper publisher+hit-or-miss+full general,chatgpt,0.9997001886367798
75,191.0,13512,"The relation between the number of missing teeth and health care cost has been studied in many countries over the last decades. In this paper, we investigate the relation between missing teeth and health care costs in the Netherlands. We use a panel data model to estimate the effect of missing teeth on the health care cost. The model includes a sample of the Dutch population, its age structure and the relation between the number of missing teeth and the health care cost. The model is estimated for the period 2000 to 2006. The results show that missing teeth are related to the health care cost. The relation between the number of missing teeth and the health care cost is not significant when the model is estimated for the whole Dutch population. However, the relation between the number of missing teeth and the health care cost is significant when the model is estimated for the age group of 50-64, who are most likely to be suffering from the dental disease. Our paper shows that the relation between the number of missing teeth and health care costs",chatgpt,1,generated_gpt3,chatgpt,0.999794900417328,"The relation between the number of missing teeth and health care cost has been studied in many countries over the last-place decades. In this paper, we investigate the relation between missing teeth and health care costs in the the netherlands. We use a control board data model to estimate the effect of missing teeth on the health care cost. The model includes a sample of the Dutch population, its age structure and the relation between the number of missing teeth and the health care cost. The model is estimated for the period 2000 to 2006. The results show that missing teeth are related to the health care cost. The relation between the number of wanting teeth and the health care cost is not significant when the model is estimated for the whole Dutch population. However, the relation between the number of missing teeth and the health care cost is significant when the model is estimated for the age group of 50 - 64, who are most likely to be suffering from the dental disease. Our paper shows that the relation between the number of missing teeth and health care costs",+last-place+the netherlands+control board+wanting,chatgpt,0.9514662623405457
76,192.0,13588,"A wide variety of modern technologies, such as wireless networks and sensor networks, are widely deployed in urban environments and the associated data, securely collected and transmitted, are beneficial to many applications. However, due to the increasing number of wireless devices, the wireless networks (WSNs) and sensor networks (SNs) have to be monitored for network integrity and data traffic analysis as well as security. In this paper, we propose a novel approach for analyzing and monitoring the wireless networks and sensor networks which are composed of a large number of wireless devices. To the best of our knowledge, there is currently no open source solution that can be used for this purpose. This makes the monitoring solution costly and time-consuming. We use a novel approach based on the clustering algorithm for clustering and data mining in wireless networks and sensor networks. The proposed approach is fully scalable and can be easily extended to other network sizes. We have conducted extensive experiments on wireless networks and sensor networks of different sizes and interest. We",chatgpt,1,generated_gpt3,chatgpt,0.99978905916214,"A wide variety of modern technologies, such as wireless networks and sensor networks, are widely deployed in urban environments and the associated data, firmly collected and transmitted, are beneficial to many applications. However, due to the increasing number of wireless devices, the wireless networks (WSNs) and sensor networks (SNs) have to be monitored for network integrity and data traffic analysis as well as security. In this paper, we propose a novel approach for analyzing and monitoring the wireless networks and sensor networks which are composed of a large number of wireless devices. To the best of our knowledge, there is currently no open source solution that can be used for this purpose. This makes the monitoring solution costly and time - consuming. We use a novel approach based on the clustering algorithm for clustering and data mining in wireless networks and sensor networks. The proposed approach is fully scalable and can be easily extended to other network sizes. We have conducted extensive experiments on wireless networks and sensor networks of different sizes and interest. We",+firmly,human,0.7520456910133362
77,193.0,13767,"The statistical inference in economics is usually based on generalized likelihood techniques for which the variational estimates of the parameters are needed. Recently, Bayesian techniques in statistics have gained considerable interest. In this paper, we present a new multivariate linear regression model with a Bayesian prior. This model deals with the problem of variable selection. We examine the problem of variable selection using a comparison between the Bayesian and the frequentist results. We show the Bayesian variable selection posterior can be used as the prior distribution of the sample mean and covariance. As a result, the Bayesian posterior distribution of the sample mean and covariance can be computed. We then discuss the distribution of the Bayesian posterior mean and variance. In addition, we show that the Bayesian model can be used to deal with the problem of model misspecification. We conclude that the Bayesian approach has a significant advantage over the frequentist approach in the sense that the Bayesian model can be viewed as a generalization of the classical",chatgpt,1,generated_gpt3,chatgpt,0.9998201727867126,"The statistical inference in economics is usually based on generalized likelihood techniques for which the variational estimates of the parameters are needful. Recently, Bayesian techniques in statistics have gained considerable interest. In this paper, we present a new multivariate linear regression model with a Bayesian prior. This model deals with the problem of variable selection. We examine the problem of variable selection using a comparison between the Bayesian and the frequentist results. We show the Bayesian variable selection posterior can be used as the prior distribution of the sample mean and covariance. As a result, the Bayesian posterior distribution of the sample mean and covariance can be computed. We then discuss the distribution of the Bayesian posterior mean and variance. In addition, we show that the Bayesian model can be used to deal with the problem of model misspecification. We conclude that the Bayesian approach has a significant advantage over the frequentist approach in the sense that the Bayesian model can be viewed as a generalization of the classical",+needful,chatgpt,0.9998231530189514
78,194.0,14257,"In this paper we address the question of whether or not the vulnerability of a species to extinction is related to its rarity. We consider the effect that rarity has on a species extinction risk in a broad range of taxa, including vertebrates, invertebrates, and plants. We find that the extinction risk of a species is determined by the probability of extinction, the abundance of the species, and the rarity of the species. The rarity of a species does not necessarily increase its extinction risk, because rare species may be more likely to survive for a long time. ",chatgpt,1,generated_gpt3,chatgpt,0.9998210072517396,"In this paper we computer address the question of whether or not the exposure of a species to extinction is related to its rarity. We look at the bring about that low density has on a species extinction risk in a overspreading range of mountains of taxa, including vertebrates, invertebrates, and plants. We find that the extinction risk of a species is determined by the probability of extinction, the abundance of the species, and the rarity of the species. The rarity of a species does not necessarily increase its extinction risk, because rare species may be more likely to survive for a long time. ",+computer address+exposure+look at+bring about+low density+overspreading+range of mountains,human,0.7294327020645142
79,195.0,15184,"In this paper, we propose a novel approach for the estimation of a Gaussian process model of an area under the receiver operating characteristic curve (ROC) curve. The proposed method is based on a novel semi-parametric estimation of the hyperparameters of the Gaussian process model. The method is more efficient than existing methods and is able to estimate the hyperparameters in the presence of outliers, which is crucial to the estimation of the ROC curve. The method is shown to be able to estimate the area under the ROC curve with a high degree of accuracy and can be used in practice. ",chatgpt,1,generated_gpt3,chatgpt,0.9998226761817932,"In this paper, we propose a novel approach for the estimation of a Gaussian process model of an area under the receiving system operating characteristic curve (ROC) curved shape. The projected method is based on a novel trailer truck - parametric estimation of the hyperparameters of the Gaussian process model. The method is more efficient than existing methods and is able to estimate the hyperparameters in the presence of outliers, which is crucial to the estimation of the ROC curve. The method is shown to be able to estimate the area under the ROC curve with a high degree of accuracy and can be used in practice. ",+receiving system+curved shape+projected+trailer truck,chatgpt,0.999264657497406
80,196.0,15756,"We consider the problem of optimizing a vehicle routing problem for multi-vehicle road networks. We propose an optimization scheme that allows us to control the number of “near-misses” (collisions) in the road network. A near miss occurs when at least one vehicle on the network is near the intersection it is approaching. In this paper, we show that our scheme, termed the “near-miss control scheme”, produces a lower number of near misses than an optimal scheme that does not take these into account. The near-miss control scheme is also easier to implement in practice.",chatgpt,1,generated_gpt3,chatgpt,0.9998034834861756,"We consider the problem of optimizing a vehicle routing trouble for multi - vehicle touring networks. We propose marriage an optimization scheme that allows us to control the identification number of “ come near - misses ” (collisions) in the road network. A near miss occurs when at least one vehicle on the network is near the intersection it is approaching. In this paper, we show that our scheme, termed the “ near - miss control scheme ”, produces a lower number of near misses than an optimal scheme that does not take these into account. The near - miss control scheme is also easier to implement in practice. ",+trouble+touring+propose marriage+identification number+come near,human,0.9943364262580872
81,197.0,16229,"This paper proposes a novel mechanism for the prediction of pre-term birth where the pregnancy is not yet complete, and the prediction is based on the comparison between the current pregnancy and the previous pregnancy. Since the previous pregnancy is not known, the prediction is based on the comparison between the current pregnancy and the previous two pregnancies. It is found that the risk of pre-term birth increases as the difference between current pregnancy and previous pregnancy increases. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The risk of pre-term birth is higher than the one of term pregnancy. The",chatgpt,1,generated_gpt3,chatgpt,0.999823272228241,"This newspaper publisher proposes a novel mechanism for the foretelling of pre - term birth where the pregnancy is not til now complete, and the prediction is based on the comparing between the current pregnancy and the previous pregnancy. Since the previous pregnancy is not known, the prediction is based on the comparison between the current pregnancy and the premature two pregnancies. It is found that the risk of pre - term birth increases as the difference between current pregnancy and previous pregnancy increases. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The risk of pre - term birth is higher than the one of term pregnancy. The",+newspaper publisher+foretelling+til now+comparing+premature,chatgpt,0.7992119193077087
82,198.0,17386,"In this paper, a new statistical technique based on the exact Lévy flight distribution is introduced. It is a new extension of the Lévy flight distribution which has been widely used in probability theory and game theory to model the stochastic behavior of a random process. Using the methodology of the Lévy flight distribution, we introduce an analytical representation of the Lévy flight distribution and show that it is equivalent to the exact Lévy flight distribution. This new Lévy flight distribution is useful for testing the convergence of the expected value of a random variable.",chatgpt,1,generated_gpt3,chatgpt,0.9998217225074768,"In this paper, a new statistical technique based on the exact Lévy flight distribution is introduced. It is a new extension service of the Lévy flight distribution which has been widely used in probability theory and game theory to model the stochastic behavior of a random process. Using the methodology of the Lévy flight distribution, we introduce an analytical representation of the Lévy flight distribution and show that it is equivalent to the exact Lévy flight distribution. This new Lévy flight distribution is useful for testing the convergence of the expected value of a random variable. ",+extension service,chatgpt,0.9270018935203552
83,199.0,17584,"We study the effects of host-parasite coevolution on host-level natural selection. We consider a model in which there are two host species, host 1 and host 2, with both host species competing over their parasite population. How the parasite population evolves depends on host 1 and 2, but is not affected by the parasite population. We show that the evolution of the parasite population is determined by a stochastic process and that the evolution of the host population is determined by a non-stochastic process. The parasite population evolves through a succession of phases, where each phase is characterized by a different parasite population size. We also consider the case when host 2 evolves to become a parasite of host 1 and vice versa. In this case, we show that the parasite population evolves in a manner dependent on both host 1 and 2. Finally, we consider the case where the parasite population is forced to change through the introduction of a new parasite population. In this case, the evolution of the",chatgpt,1,generated_gpt3,chatgpt,0.9997791647911072,"We study the effects of host - parasite coevolution on host - level natural selection. We consider a model in which there are two host species, host 1 and host 2, with both host species competing over their parasite population. How the parasite population evolves depends on host 1 and 2, but is not affected by the parasite population. We show that the evolution of the parasite population is determined by a stochastic process and that the evolution of the host population is determined by a non - stochastic process. The parasite population evolves through a succession of phases, where each phase is characterized by a different parasite population size. We also consider the case when host 2 evolves to become a parasite of host 1 and vice versa. In this case, we show that the parasite population evolves in a manner dependent on both host 1 and 2. Finally, we consider the case where the parasite population is forced to change through the introduction of a new parasite population. In this case, the evolution of the",,chatgpt,0.999808132648468
84,200.0,18058,"In this paper, we investigate the influence of the type of article and journal on the readership and the journals’ impact on the readership of an academic journal. We consider the impact of the article type on the readership of the journal. We also consider the impact of the journal on the readership of the journal. When we consider the impact of the journal, we focus on the impact of the journals’ impact on the readership of the journal. We use the impact of the journal's impact on the readership of the journal as the dependent variable. We use the impact of the journal on the readership of the journal as the independent variable. We consider the impact of the article type on the readership of the journal. We use the impact of the article type on the readership of the journal as the dependent variable. We use the impact of the article type on the readership of the journal as the independent variable. We use variance analysis as our statistical analysis tool.",chatgpt,1,generated_gpt3,chatgpt,0.9997838139533995,"In this paper, we investigate the influence of the type of article and journal on the readership and the journals ’ impact on the readership of an academic journal. We consider the impact of the article type on the readership of the journal. We also consider the impact of the journal on the readership of the journal. When we consider the impact of the journal, we focus on the impact of the journals ’ impact on the readership of the journal. We use the impact of the journal's impact on the readership of the journal as the dependent variable. We use the impact of the journal on the readership of the journal as the independent variable. We consider the impact of the article type on the readership of the journal. We use the impact of the article type on the readership of the journal as the dependent variable. We use the impact of the article type on the readership of the journal as the independent variable. We use variance analysis as our statistical analysis tool. ",,chatgpt,0.9997919201850891
85,201.0,18060,"The survival of organisms is determined by their genes and the environment in which they evolve. We would like to understand how the environment affects gene evolution by comparing the frequency of an amino acid in a strongly linked family of organisms that evolved under different ecological circumstances. To achieve this goal, we use a large dataset of proteins from eleven species of nematodes, and quantify the degree to which the frequency of an amino acid changes when the environment changes due to environmental change. The data show that the frequency of a particular amino acid, ‘serine’, changes differently in the different species, depending on the degree of similarity between the species. We conclude that this result is also applicable to the evolution of protein functional groups.",chatgpt,1,generated_gpt3,chatgpt,0.9997966885566713,"The survival of the fittest of organisms is determined by their genes and the environment in which they evolve. We would suchlike to understand how the environment affects gene evolution by comparing the frequency of an aminic acid in a strongly linked family of organisms that evolved under different ecological circumstances. To achieve this goal, we use a large dataset of proteins from football team species of nematodes, and quantify the degree to which the relative frequency of an amino lysergic acid diethylamide changes when the environment changes due to environmental change. The data show that the frequency of a particular amino acid, ‘ serine ’, changes differently in the different species, depending on the degree of similarity between the species. We conclude that this result is also applicable to the evolution of protein functional groups. ",+survival of the fittest+suchlike+aminic+football team+relative frequency+lysergic acid diethylamide,chatgpt,0.999817430973053
86,202.0,18068,"Gradual temperature change is a major concern for the survival of many species. In this paper, we show that a species’ distribution is influenced by temperature change in a non-linear manner, with the effect of temperature change increasing with temperature change. This has interesting implications for the study of the distribution of species that are at their thermal limits and may be living in areas where climate change will occur.",chatgpt,1,generated_gpt3,chatgpt,0.9998084902763368,"Gradual temperature change is a major concern for the survival of many species. In this paper, we show that a variety ’ dispersion is influenced by temperature change in a non - unsubdivided manner, with the effect of temperature change increasing with temperature change. This has unputdownable implications for the study of the distribution of species that are at their thermal limits and may be living in areas where climate change will occur. ",+variety+dispersion+unsubdivided+unputdownable,human,0.6645860075950623
87,203.0,18086,"In this paper, we consider the problem of designing algorithms to efficiently search a set of inputs in a database. We assume that every input must be compared to every other input and that the results may be sorted into a list. We consider the problem in two forms: first, we consider an input-output database, where the inputs are the possible outputs of a given algorithm and the outputs are the solutions of the given algorithm; second, we consider a set of inputs and a set of solutions to a given problem, where the solutions are sorted in a list. We show that the solution to the first problem is the solution to the second problem if the two problems are similar in that the solutions are sorted and the inputs are sorted. Moreover, we show that the solution to the second problem is the solution to the first problem if the outputs can be sorted. We also show that if the inputs are partitioned into subsets, the solution to the first problem is the solution to the second problem if the",chatgpt,1,generated_gpt3,chatgpt,0.9998063445091248,"In this paper, we consider the problem of scheming algorithms to efficiently search a set of inputs in a database. We assume that every input have got be compared to every other input and that the results may be sorted into a list. We consider the problem in two forms: first, we consider an input - output database, where the inputs are the researchable outputs of a given algorithm and the outputs are the solutions of the given algorithm; second, we consider a set of inputs and a set of solutions to a given problem, where the solutions are sorted in a list. We show that the solution to the first problem is the solution to the second problem if the two problems are confusable in that the solutions are sorted and the inputs are grouped. Moreover, we show that the solution to the second problem is the solution to the first problem if the outputs can be sorted. We also show that if the inputs are partitioned into subsets, the solution to the first problem is the solution to the second problem if the",+scheming+have got+researchable+confusable+grouped,human,0.9747107028961182
88,204.0,18331,"In this paper, we study the dynamic behavior of the similarity matrix of a two-dimensional lattice under the action of the random walk process. The similarity matrix of the lattice is associated with the second-order probability distribution of the jumps of the random walk process. We show that the similarity matrix of the lattice is a skew-symmetric matrix. Then we prove that the skewness of the matrix increases with the size of the lattice. Finally, we show that the second-order correlation of the jumps of the random walk process is a mean-zero process.",chatgpt,1,generated_gpt3,chatgpt,0.9635730981826782,"In this paper, we subject field the dynamic deportment of the similarity matrix of a two - multidimensional lattice under the military action of the random walk unconscious process. The similarity matrix of the lattice is related to with the second - order probability distribution of the jumps of the random walk process. We show that the similarity matrix of the lattice is a skew - symmetric matrix. Then we prove that the skewness of the matrix increases with the size of the lattice. Finally, we show that the second - order correlation of the jumps of the random walk process is a mean - zero process. ",+subject field+deportment+multidimensional+military action+unconscious process+related to,human,0.9806046485900879
89,205.0,18334,"We present a new algorithm for the design of suboptimal randomized complete block designs, in which the probability of a member of a block has to be increased to avoid the block being empty. It has two constructive steps: (1) The procedure starts with the original randomized complete block design and applies the new algorithm for each block. (2) If a block is empty, it is updated to contain the remaining higher-probability blocks. This procedure is illustrated on a number of examples. It works with the classical block designs and the randomized completepack designs which are the most frequently used in practice.",chatgpt,1,generated_gpt3,chatgpt,0.5570193529129028,"We present tense a new algorithm for the design of suboptimal randomized complete auction block designs, in which the probability of a extremity of a block has to be increased to avoid the block being empty. It has two constructive steps: (1) The procedure starts with the original randomized complete block designing and applies the new algorithm for each block. (2) If a block is empty, it is updated to hold in the remaining higher - probability blocks. This procedure is illustrated on a number of examples. It works with the classical block designs and the randomized completepack designs which are the most frequently used in practice. ",+present tense+auction block+extremity+designing+hold in,human,0.9998127818107605
90,206.0,18400,"In this paper, we investigate the performance of a class of nonlinear multi-objective evolutionary algorithms for the optimization of a single-objective optimization algorithm. The core idea is to use a class of evolutionary algorithms that, for a multi-objective problem, have been shown to have high performance in a single-objective optimization algorithm. In the literature, these algorithms have been called the ‘multi-objective evolutionary algorithms’ (MOEAs). We use these algorithms in the optimization of a multi-objective criterion function. We show that using the MOEAs, it is possible to achieve a speed up of at least ten-fold. We also show that the performance of the MOEAs is not dependent on the number of objectives, and that the choice of the MOEA is independent of the choice of the objective function. We discuss the relationship between the MOEA and the multi-objective evolutionary algorithm.",chatgpt,1,generated_gpt3,chatgpt,0.999762237071991,"In this newspaper publisher, we investigate the performance of a class of nonlinear multi - objective evolutionary algorithms for the optimization of a single - objective optimization algorithm. The core idea is to use a class of evolutionary algorithms that, for a multi - objective problem, have been shown to have high performance in a single - objective optimization algorithm. In the literature, these algorithms have been called the ‘ multi - objective evolutionary algorithms ’ (MOEAs). We use these algorithms in the optimization of a multi - objective criterion function. We show that using the MOEAs, it is possible to achieve a speed up of at least ten - fold. We also show that the performance of the MOEAs is not dependent on the number of objectives, and that the choice of the MOEA is independent of the choice of the objective function. We discuss the relationship between the MOEA and the multi - objective evolutionary algorithm. ",+newspaper publisher,chatgpt,0.9998084902763367
91,207.0,18790,"This paper studies the impact of the interaction of exchange rates, the capital account and inflation on the real exchange rate. We first find that the real exchange rate depends on the level of capital flight and the stock of foreign assets in the country. We then suggest that the real exchange rate is also affected by the movements of exchange rates, inflation and the stock of foreign assets in the country. The balance of payments is then calculated as the sum of changes in the stock of foreign assets, foreign assets and the stock of domestic liabilities.",chatgpt,1,generated_gpt3,chatgpt,0.9998227953910828,"This newspaper publisher studies the impact of the interaction of exchange rates, the capital account and inflation on the real exchange rate. We first find that the real number exchange rate depends on the level of upper-case letter flight of stairs and the stock of foreign assets in the country. We then suggest that the real exchange rate is also affected by the movements of telephone exchange rates, inflation and the stock of foreign assets in the country. The balance of payments is then calculated as the sum of changes in the stock of foreign assets, foreign assets and the stock of domestic liabilities. ",+newspaper publisher+real number+upper-case letter+flight of stairs+telephone exchange,chatgpt,0.9918679594993591
92,208.0,18930,"The aim of this study is to develop a new framework for the evaluation of the performance of an optimization algorithm, based on a comparative analysis of the performance of the algorithm and two alternative algorithms. The first algorithm, known as the ‘unique-solution algorithm’, is designed to find a unique solution to a given optimization problem. The second algorithm, known as the ‘local-solution algorithm’, is designed to find a solution that is localized in some neighborhood of the solution found by the ‘unique-solution algorithm’. The ‘unique-solution algorithm’ can find a unique solution to a given optimization problem with an integer-valued objective function and linear constraints on the parameters. The ‘local-solution algorithm’ is characterized by the use of a parameter estimation technique and a monotonic relaxation of the objective function. This paper is based on an algorithm and an associated algorithm-comparison criterion that is described in the paper",chatgpt,1,generated_gpt3,chatgpt,0.9997552037239076,"The aim of this study is to develop a new framework for the evaluation of the performance of an optimization algorithm, based on a comparative analysis of the performance of the algorithm and two alternative algorithms. The first algorithm, known as the ‘ unique - solution algorithm ’, is designed to find a unique solution to a given optimization problem. The second algorithm, known as the ‘ local - solution algorithm ’, is designed to find a solution that is localized in some neighborhood of the solution found by the ‘ unique - solution algorithm ’. The ‘ unique - solution algorithm ’ can find a unique solution to a given optimization problem with an integer - valued objective function and linear constraints on the parameters. The ‘ local - solution algorithm ’ is characterized by the use of a parameter estimation technique and a monotonic relaxation of the objective function. This paper is based on an algorithm and an associated algorithm - comparison criterion that is described in the paper",,chatgpt,0.9997897744178772
93,209.0,19391,"The study of the dynamic behaviors of biological systems is of great interest in the current era of increasing global travel. In this paper, we investigate the effects of commuting and distance between work and home on urban mobility using a model of traffic flow. We use the data from the U.S. 2000 Census to estimate the commuting pattern of the population, and from the data on driving distance between the home and the workplace (in km) to estimate the impact of commuting distance on urban mobility. Our results show that a commuter is more likely to travel shorter distances in the morning and the evening, and at the weekend, and is less likely to travel long distances on any given day. Logistic regression analysis reveals that commuting distance has a significant effect on urban mobility. Furthermore, the effect of commuting distance is stronger for those with shorter commute times. These findings suggest the importance of commuting distance in determining the urban mobility of the population, and the need to improve the efficiency of transportation networks in order to reduce the urban",chatgpt,1,generated_gpt3,chatgpt,0.9997983574867249,"The study of the dynamic behaviors of biological systems is of great interest in the current epoch of increasing global travel. In this paper, we investigate the effects of commuting and distance between work and home on urban mobility using a model of traffic flow. We use the data from the U. S. 2000 Census to estimate the commuting pattern of the population, and from the data on driving distance between the home and the workplace (in km) to estimate the impact of commuting distance on urban mobility. Our results show that a commuter is more likely to travel shorter distances in the morning and the evening, and at the weekend, and is less likely to travel long distances on any given day. Logistic regression analysis reveals that commuting distance has a significant effect on urban mobility. Furthermore, the effect of commuting distance is stronger for those with shorter commute times. These findings suggest the importance of commuting distance in determining the urban mobility of the population, and the need to improve the efficiency of transportation networks in order to reduce the urban",+epoch,chatgpt,0.9998087286949158
94,210.0,19515,"Mining of the Internet involves many stages, including indexing, text retrieval, and data mining. In this paper, we develop a novel approach to efficiently mine the web data by incorporating a hierarchy of nested relationships. The hierarchical structure is encoded in an attribute-value structure, which is a data model suitable for the data mining tasks. A Preliminary Data Model (PDM) for the attribute-value structure is proposed, and the PDM is used to develop a novel algorithm, which is a combination of a dynamic programming and the greedy algorithm, for finding the shortest path from the root-to-leaf and leaf-to-leaf paths, respectively. The performance of the algorithm is evaluated by extensive simulations. The results show that the algorithm is able to efficiently find the shortest path from the root-to-leaf path. The algorithm also achieves a high recall for the leaf-to-leaf path, as compared to the path-searching method. Moreover, the performance of the algorithm is",chatgpt,1,generated_gpt3,chatgpt,0.9997984766960144,"Mining of the Internet involves many stages, including indexing, text retrieval, and data mining. In this paper, we develop a novel plan of attack to efficiently mine the web data by incorporating a hierarchy of nested relationships. The hierarchical structure is encoded in an attribute - value structure, which is a data model suited for the data minelaying tasks. A Preliminary Data Model (PDM) for the attribute - value structure is proposed, and the PDM is used to develop a novel algorithm, which is a combination of a dynamic programming and the greedy algorithm, for finding the shortest path from the root - to - leaf and leaf - to - leaf paths, respectively. The performance of the algorithm is evaluated by extensive simulations. The results show that the algorithm is able to efficiently find the shortest path from the root - to - leaf way of life. The algorithm also achieves a high recall for the leaf - to - leafage path, as compared to the path - searching method. Moreover, the performance of the algorithm is",+plan of attack+suited+minelaying+way of life+leafage,chatgpt,0.9997091889381409
95,211.0,19772,"The analysis of biological networks has been an active field of research for the last five decades. In recent years, the use of real-valued attributes to provide structural information in networks has been investigated. However, the representation of information based on attribute values poses a problem in terms of the representation accuracy and computational complexity. In this study, we investigate how to represent the structural information in the network of species distributions across Spanish sites in a simple and efficient way. We use the distribution of attributes and their values as a key to organize this network, which we define as the attribute network. We create a representation of the attribute network as a re-ordered version of the original network that is equivalent to the original network but reordered according to the attributes and their values. We demonstrate how the attribute network can be used to visualize the structure of a real-valued network. We also show that the attribute network can be used to represent information in both unweighted and weighted networks. The attribute network can be used to",chatgpt,1,generated_gpt3,chatgpt,0.9998273849487304,"The analysis of biological networks has been an active field of research for the last five decades. In recent years, the use of real - valued attributes to provide structural information in networks has been investigated. However, the representation of information based on attribute values poses a problem in terms of the representation truth and computational complexity. In this study, we investigate how to represent the structural information in the network of species distributions across spanish people sites in a simple and efficient way. We use the distribution of attributes and their values as a key to organize this network, which we define as the attribute network. We create a representation of the attribute network as a re - ordered version of the underivative network that is combining weight to the original network but reordered according to the attributes and their values. We demonstrate how the attribute network can be used to visualize the structure of a real - valued network. We also show that the attribute network can be used to represent information in both unweighted and weighted networks. The attribute network can be used to",+truth+spanish people+underivative+combining weight,human,0.9955931305885315
96,212.0,20601,"The first step in the design of a new computer program is usually to create a specification of the task. The specification usually consists of a set of requirements, and is usually a formal document. The requirements may be explicitly stated, or may be implicitly understood by the programmer. If a formal specification is used, it serves as a guide to help the programmer to design the program. However, a specification is often not enough to ensure that the program satisfies the requirements. This paper shows that in some cases the program may not satisfy the requirements even though the specification is complete. We also show that the programmer may not be able to guess from the specification whether the program will satisfy the requirements. We present a solution to this problem based on analyzing the program to determine whether the program fulfills its own requirements.",chatgpt,1,generated_gpt3,chatgpt,0.9998175501823424,"The first step in the design of a new computer program is usually to create a specification of the undertaking. The specification usually consists of a set of requirements, and is usually a formal document. The requirements may be explicitly stated, or may be implicitly apprehanded by the computer programmer. If a white-tie specification is used, it serves as a guide to help the programmer to design the program. However, a specification is often not enough to ensure that the program satisfies the requirements. This paper shows that in some cases the program may not satisfy the requirements even though the specification is complete. We also show that the programmer may not be able to dead reckoning from the specification whether the program will satisfy the requirements. We present tense a solution to this problem based on analyzing the program to determine whether the program fulfills its own requirements. ",+undertaking+apprehanded+computer programmer+white-tie+dead reckoning+present tense,human,0.9997715353965759
97,213.0,21206,"We propose a decision-theoretic analysis of the idea of S-curve based on the theory of optimal policies for a single homogeneous multi-stage production process, and we show that the S-curve can be used to describe the evolution of the levels of national income. For the purpose of this paper, we assume that the production of a good is a multi-stage process where the stages are located in different countries, and that the stages are of different lengths. The introduction of the S-curve allows us to show that the national income in the long run is determined by the number of stages of production in the country where the longest stage is located. This way of thinking about the S-curve is new. ",chatgpt,1,generated_gpt3,chatgpt,0.9996993541717528,"We propose marriage a decision - theoretic analysis of the idea of S - curve based on the theory of optimal policies for a single homogeneous multi - stage production process, and we show that the S - curve can be used to describe the evolution of the levels of national income. For the purpose of this paper, we assume that the production of a good is a multi - stage process where the stages are located in different countries, and that the stages are of different lengths. The introduction of the S - curve allows us to show that the national income in the long run is determined by the number of stages of production in the country where the longest stage is located. This way of thinking about the S - curve is new. ",+propose marriage,human,0.9344411492347717
98,214.0,21865,"The aim of this study is to develop an algorithm of the stochastic gradient descent which is based on the dynamic programming algorithm. This algorithm is intended to be applied to the solution of the $Q$-approximation problem of the least mean-square error (LMS) estimation. The algorithm is designed by using the dynamic programming algorithm. It contains two main steps. First, we calculate the dynamic programming algorithm for the $Q$th approximation at every step of the algorithm. Then, we solve the $Q$th approximation of the LMS estimator for the $Q$th approximation. ",chatgpt,1,generated_gpt3,chatgpt,0.9998123049736024,"The aim of this study is to develop an algorithmic program of the stochastic gradient descent which is based on the dynamic programming algorithm. This algorithm is intended to be applied to the solution of the $Q$-approximation problem of the least mean - square error (LMS) estimation. The algorithm is designed by using the dynamic programming algorithm. It contains two main steps. First, we calculate the dynamic programming algorithm for the $Q$th approximation at every step of the algorithm. Then, we solve the $Q$th approximation of the LMS estimator for the $Q$th approximation. ",+algorithmic program,chatgpt,0.9998100399971008
99,215.0,21917,"The presence of parasitic wasps (Hymenoptera: Braconidae) infesting honeybee colonies has been reported in numerous places worldwide. In some cases, the wasps parasitize the brood and kill the host. In others, the wasp kills the host and deposits her eggs in the brood. In the latter case, the parasitization of the host by the wasp is a consequence of the host’s behavior, since the host ignores the wasp’s presence and accepts the brood from the victim, even though this may lead to the death of the host. The parasitization of a bee by a wasp is generally considered to be undesirable. In this paper, we first demonstrate that the costs of parasitization by Braconidae wasps depend on the social structure of the honeybee colony. A small-scale parasitization of a single colony by a Braconidae wasp causes a significant reduction of the social interaction between host and colony",chatgpt,1,generated_gpt3,chatgpt,0.9997819066047668,"The presence of parasitic wasps (Hymenoptera: Braconidae) infesting honeybee colonies has been reported in numerous places worldwide. In some cases, the wasps parasitize the brood and kill the host. In others, the wasp kills the host and deposits her eggs in the brood. In the latter case, the parasitization of the host by the wasp is a consequence of the host ’s behavior, since the host ignores the wasp ’s presence and accepts the brood from the victim, even though this may lead to the death of the host. The parasitization of a bee by a wasp is generally considered to be undesirable. In this paper, we first demonstrate that the costs of parasitization by Braconidae wasps depend on the social structure of the honeybee colony. A small - scale parasitization of a single colony by a Braconidae wasp causes a significant reduction of the social interaction between host and colony",,chatgpt,0.9977102279663086
100,216.0,22077,"Wearable computing is a rapidly growing field of research that combines several fields of computer science and engineering to design systems that are incorporated into the human body. We analytically study the problem of wearable computing. We consider the problem of wearable computing in a simple model where a wearable device is part of a human body and communication between the wearable device and the human body is possible but limited to two-way communication. We derive the classical (convex) optimization problem of wearable computing and establish the existence of a set of feasible solutions. The problem is well-posed and a global optimum is obtained. To the best of our knowledge, this is the first result of its kind.",chatgpt,1,generated_gpt3,human,0.5179254412651062,"Wearable computing is a rapidly growing field of enquiry that combines several fields of computer science and engineering to design systems that are merged into the human body. We analytically study the problem of wearable computing. We consider the problem of wearable computing in a simple model where a wearable device is part of a human body and communication between the wearable device and the human body is possible but limited to two - way communication. We derive the classical (convex) optimization problem of wearable computing and establish the existence of a set of feasible solutions. The problem is well - posed and a global optimum is obtained. To the best of our knowledge, this is the first result of its kind. ",+enquiry+merged,human,0.9988935589790344
101,217.0,22453,"We investigate the influence of the honeybee (Apis mellifera)’s immune system on its colony performance. Honeybees are known to be adaptable in their response to pathogens, in particular, they have evolved mechanisms to limit the spread of the pathogens that infect them. However, there is currently no evidence that the honeybee’s immune system has evolved to limit the spread of the pathogens that infect the honeybee. In this paper, we investigate the communication between the immune system and the honeybee’s colony dynamics. We simulate bee colonies according to a honeybee immune system model and a real-world colony of honeybees. We find that the immune system acts to limit the spread of the pathogens. We also find that the immune system is an important determinant of the colony’s performance.",chatgpt,1,generated_gpt3,chatgpt,0.9998030066490172,"We investigate the influence of the honeybee (Apis mellifera) ’s immune system on its colony performance. Honeybees are known to be adaptable in their response to pathogens, in particular, they have evolved mechanisms to limit the spread of the pathogens that infect them. However, there is currently no evidence that the honeybee ’s immune system has evolved to limit the spread of the pathogens that infect the honeybee. In this paper, we investigate the communication between the immune system and the honeybee ’s colony dynamics. We simulate bee colonies according to a honeybee immune system model and a real - world colony of honeybees. We find that the immune system of rules acts to limit the spread of the pathogens. We also find that the immune system is an important determinant of the colony ’s performance. ",+system of rules,chatgpt,0.9997594952583313
102,218.0,23035,"Anthropogenic species are able to exploit resources that were once inaccessible to them, and so are able to invade, proliferate and eventually cause one of the most serious extinctions in the history of life. However, the invasion process may have an initial phase of limited expansion before it expands more widely. This paper considers the importance of both limited and more widespread phases of invasion. As a model system, we use a system of eight interacting species with a payoff matrix that can be shown to capture the structure of an actual invasion. We show that at the invasion stage, invasion has a limited phase in which it can expand across the matrix. By contrast, invasion is more likely to be more widely distributed, in which case invasion reduces the payoff matrix and therefore reduces the strength of the invasion process and, therefore, its ability to spread.",chatgpt,1,generated_gpt3,chatgpt,0.9998227953910828,"Anthropogenic species are able to exploit resources that were once inaccessible to them, and so are able to invade, proliferate and eventually cause one of the most serious extinctions in the history of life. However, the invasion process may have an initial phase of limited expansion before it expands more widely. This paper considers the importance of both limited and more widespread phases of invasion. As a model system, we use a system of eight interacting species with a payoff matrix that can be shown to capture the structure of an actual invasion. We show that at the invasion stage, invasion has a limited phase in which it can expand across the matrix. By contrast, invasion is more likely to be more widely distributed, in which case invasion reduces the payoff matrix and therefore reduces the strength of the invasion process and, therefore, its ability to spread. ",,chatgpt,0.9998241066932678
103,219.0,23482,"In natural language processing (NLP), the word “emotion” appears repeatedly, on average, in every 1000 words. It is also the first word that appears in the documents, but it is not the last word. This paper presents an empirical analysis of the distribution of the first word and the last word, both in literal and non-literal context. We find that the first word is more common in non-literal contexts, but the last word is more common in literal contexts. Moreover, the last word is less diverse. Based on these observations we show that the last word is more difficult to communicate than the first word. In particular, the last word is more difficult to translate, which suggests that the last word is more difficult to understand.",chatgpt,1,generated_gpt3,chatgpt,0.999827206134796,"In unstilted spoken language processing (NLP), the word “ emotion ” appears repeatedly, on average, in every 1000 words. It is also the first word that appears in the documents, but it is not the last word. This paper presents an empirical analysis of the distribution of the first word and the last word, both in literal and non - literal context. We find that the first word is more common in non - literal contexts, but the last word is more common in literal contexts. Moreover, the last word is less diverse. Based on these observations we show that the last word is more difficult to communicate than the first word. In particular, the last word is more difficult to translate, which suggests that the last word is more difficult to understand. ",+unstilted+spoken language,chatgpt,0.7964419722557068
104,220.0,23621,"We consider the problem of finding the most-suitable combination of two risk factors. We distinguish two cases: first, we consider the problem of choosing the best risk factor by itself; second, we consider the problem of choosing the best combination of risk factors. In the first case, the best-suitable combination of two risk factors can be found by maximizing the expected utility of the associated utility function. In the second case, the optimal risk factors can be found by maximizing the expected utility of the associated utility function, which is the sum of the utilities of the two risk factors. We show that the problem of maximizing the expected utility of a utility function is equivalent to the problem of minimizing the expectation of a random variable. We study this equivalence, and show that the expected utility of a utility function is the sum of the expected utilities of the two risk factors. ",chatgpt,1,generated_gpt3,chatgpt,0.9998138546943665,"We consider the problem of finding the most - suitable combination of two risk factors. We distinguish two cases: first, we consider the problem of choosing the best risk factor by itself; second, we consider the problem of choosing the best combining of risk factors. In the first case, the best - suitable combination of two risk factors can be found by maximizing the hoped-for utility of the associated utility function. In the second case, the optimum lay on the line factors can be found by maximizing the expected utility of the associated utility function, which is the sum of the utilities of the two risk factors. We show that the problem of maximizing the expected utility of a utility function is combining weight to the problem of minimizing the expectation of a hit-or-miss variable quantity. We study this equivalence, and show that the expected utility of a utility function is the sum of the expected utilities of the two risk factors. ",+combining+hoped-for+optimum+lay on the line+combining weight+hit-or-miss+variable quantity,human,0.9228892922401428
105,221.0,23706,"In this paper we present an algorithm for computing the maximum likelihood estimates of the parameters of a linear regression model, without the need of nonlinearity or non-concaveity of the model. For a given model, we show that the maximum likelihood estimates are also the maximum likelihood estimates of the parameters in a least squares sense. We also show that the algorithm is applicable to continuous and count data sets. We have implemented this algorithm in the R statistical language, and have tested it with the data sets of the R package ‘lm’. The algorithm is implemented in the MATLAB language.",chatgpt,1,generated_gpt3,chatgpt,0.999804675579071,"In this paper we present an algorithm for computing the maximum likelihood estimates of the parameters of a linear regression model, without the have got of nonlinearity or non - concaveity of the model. For a given model, we show that the maximum likelihood estimates are also the maximum likeliness estimates of the parameters in a least squares sense. We also show that the algorithm is applicable to continuous and count data sets. We have implemented this algorithm in the R statistical language, and have tested it with the data sets of the R package ‘ lm ’. The algorithm is implemented in the MATLAB language. ",+have got+likeliness,human,0.9724226593971252
106,222.0,23908,"In this study, we investigate the effects of a population's genetic structure on the dynamics of the force of selection. We model the evolution of a population of identical individuals, where each individual has a genetic identity function. We consider two types of genetic structure: complete and incomplete. The complete structure describes a population with a single dominant gene, while the incomplete structure describes a population with two recessive genes. We examine how the dynamics of selection depend on the genetic structure of the population. The results of our study suggest that the dynamics of selection depend on the number of alleles present in the population. It is also found that selection is stronger in a population with a small number of loci than in a population with an intermediate number of loci.",chatgpt,1,generated_gpt3,chatgpt,0.9998063445091248,"In this study, we investigate the effects of a population's genetic structure on the dynamics of the force of selection. We model the evolution of a population of superposable individuals, where each individual has a genetic identity function. We consider two types of genetic structure: complete and incomplete. The complete structure describes a population with a single dominant gene, while the incomplete structure describes a population with two recessive genes. We examine how the dynamics of selection depend on the genetic structure of the population. The results of our study suggest that the dynamics of selection depend on the number of alleles present in the population. It is also found that selection is stronger in a population with a small number of loci than in a population with an intermediate number of loci. ",+superposable,chatgpt,0.99981290102005
107,223.0,24543,"This paper presents a new method for the analysis of continuous-time Stochastic blockmodels. A new set of discrete-time Stochastic blockmodels are introduced, which allow the analysis of continuous-time Stochastic blockmodels. The blockmodels are derived under the assumptions of convexity and continuity. The asymptotic equations and convergence estimates are also derived. The analytical results are applied to the study of continuous-time Stochastic blockmodels.",chatgpt,1,generated_gpt3,chatgpt,0.9930818676948548,"This paper presents a new method for the analytic thinking of continuous - clock time Stochastic blockmodels. A new set of discrete - time Stochastic blockmodels are introduced, which allow the analysis of continuous - time Stochastic blockmodels. The blockmodels are derived under the assumptions of convexity and continuity. The asymptotic equations and convergence estimates are also derived. The analytical results are applied to the study of continuous - time Stochastic blockmodels. ",+analytic thinking+clock time,human,0.9991920590400696
108,224.0,25026,"When applied to polygenic traits, multifactorial traits exhibit non-additive polygenic effects (NAPE). This is due to the fact that the polygenic effect of one gene depends on the polygenic effect of the other genes in the same locus. In this study, we use the log-odds ratio of the polygenic effect of the top 10 genes in the same locus to estimate the NAPE. We find that the average NAPE is 0.01, which is much lower than the previously reported values. The NAPE is mostly due to the effect of a locus in the same chromosome.",chatgpt,1,generated_gpt3,chatgpt,0.9997971653938292,"When applied to polygenic traits, multifactorial traits exhibit non - additive polygenic effects (NAPE). This is due to the fact that the polygenic effect of one gene depends on the polygenic effect of the other genes in the same locus. In this study, we use the log - odds ratio of the polygenic effect of the top 10 genes in the same locus to estimate the NAPE. We find that the average NAPE is 0. 01, which is much lower than the previously reported values. The NAPE is mostly due to the effect of a locus in the same chromosome. ",,chatgpt,0.975550651550293
109,225.0,25084,"In ecology, the term ‘community’ is used to describe the set of species that form a functional entity. In this paper, we consider the problem of identifying ‘communities’ in ecological networks. We first derive a general definition of the set of nodes that are community members, and show that it can be a union of two or more closed connected subgraphs, for example, ‘i’ or ‘j.’ We then define the neighborhood of a node ‘i’ as the set of species ‘s’ such that ‘i’ is neighbors with ‘s.’ In the case of two nodes ‘i’ and ‘j,’ we define the neighborhood of ‘i’ as the set of species ‘s’ such that ‘i’ and ‘j’ are neighbors with ‘s.’ The community ‘c’ is",chatgpt,1,generated_gpt3,chatgpt,0.9998242259025574,"In ecology, the term ‘ community ’ is used to describe the set of species that form a functional entity. In this paper, we consider the problem of identifying ‘ communities ’ in ecological networks. We first derive a general definition of the set of nodes that are community members, and show that it can be a union of two or more closed connected subgraphs, for example, ‘ i ’ or ‘ j. ’ We then define the neighborhood of a node ‘ i ’ as the set of species ‘s ’ such that ‘ i ’ is neighbors with ‘ s. ’ In the case of two nodes ‘ i ’ and ‘ j, ’ we define the neighborhood of ‘ i ’ as the set of species ‘s ’ such that ‘ i ’ and ‘ j ’ are neighbors with ‘ s. ’ The community ‘ c ’ is",,chatgpt,0.9998102784156799
110,226.0,25304,"In recent years, the insurance industry has been experiencing rapid growth and has become an important part of the economy. The industry could not continue to grow without the accurate estimation of risk. In this paper, we propose a multivariate statistical model for the insurance risk estimation and propose a computational algorithm for calculating the insurance risk. Firstly, given the data of the insured person, we fit a multivariate Gaussian distribution for each insured person, and then we take the logarithm of the observed data. Secondly, we employ a hierarchical Bayesian approach to estimate the risk of the insured person. Thirdly, the Bayesian information criterion (BIC) is used to evaluate the goodness of fit. Fourthly, we develop a hypothesis testing procedure to evaluate the correctness of our model. We present a simulation study and demonstrate the effectiveness of the proposed method.",chatgpt,1,generated_gpt3,chatgpt,0.999808132648468,"In recent years, the insurance industry has been experiencing rapid growth and has become an important part of the economy. The industry could not continue to grow without the accurate estimation of risk. In this paper, we propose a multivariate statistical model for the insurance risk estimation and propose a computational algorithmic program for calculating the insurance policy risk. first off, given the data of the insured somebody, we fit a multivariate Gaussian distribution for each insured person, and then we take the logarithm of the observed data. Secondly, we employ a hierarchical Bayesian approach to estimate the risk of the insured person. Thirdly, the Bayesian information criterion (BIC) is used to evaluate the goodness of fit. Fourthly, we develop a hypothesis testing procedure to evaluate the correctness of our model. We present a simulation study and demonstrate the effectiveness of the proposed method. ",+algorithmic program+insurance policy+first off+somebody,human,0.6931466460227966
111,227.0,25613,"In this paper we extend the notion of minimum spanning trees to the case where the graph is undirected. We show that if a graph is undirected with n nodes, then there is a minimum spanning tree with minimum weight and maximum weight if and only if one of the following conditions holds: 1. The weight of the minimum spanning tree is at least 1.2n1/2. 2. The weight of the minimum spanning tree is at least 1.2n1/2.3. The weight of the minimum spanning tree is at least 1.2n.4. The weight of the minimum spanning tree is at least 1.2n.5. The weights of all other minimum spanning trees are at least 1.2n1/2.",chatgpt,1,generated_gpt3,chatgpt,0.9992817044258118,"In this paper we stretch forth the notion of minimum spanning trees to the case where the graph is undirected. We show that if a graph is undirected with n nodes, then there is a minimum spanning tree with lower limit weight and maximum weight if and only if one of the following conditions holds: 1. The weight of the minimum spanning tree is at least 1. 2n1/2. 2. The weight of the minimum spanning tree is at least 1. 2n1/2. 3. The weight of the minimum spanning tree is at least 1. 2n. 4. The weight of the minimum spanning tree is at least 1. 2n. 5. The weights of all other minimum spanning trees are at least 1. 2n1/2. ",+stretch forth+lower limit,human,0.9233987927436829
112,228.0,26106,"In this paper, we develop a linear programming model for multi-objective optimization problems where the objective function consists of a weighted sum of objective functions. We define an objective function that relates the objective function of the original multi-objective problem to an objective function of the sub-problem. In this paper, we show that the objective function that relates the objective function of the original multi-objective problem to the objective function of the sub-problem is a monotonically increasing function of the objective function of the original multi-objective problem. This result can be used to find the optimal objective function for the original multi-objective problem.",chatgpt,1,generated_gpt3,chatgpt,0.999786913394928,"In this paper, we develop a unsubdivided computer programming model for multi - objective optimization problems where the objective function consists of a adjusted sum of objective functions. We define an objective function that relates the objective function of the underivative multi - objective problem to an objective function of the sub - problem. In this paper, we show that the objective mathematical function that relates the objective mathematical function of the original multi - objective problem to the objective function of the sub - problem is a monotonically increasing function of the objective function of the original multi - objective problem. This result can be used to find the optimal objective function for the original multi - objective problem. ",+unsubdivided+computer programming+adjusted+underivative+mathematical function+mathematical function,chatgpt,0.9801504611968994
113,229.0,26376,"The effect of parental care in a social species is influenced by the genetic relatedness of the offspring. The relatedness between offspring is determined by their genetic relatedness, the strength of the association between the parents which determines the consequences of care, and by the strength of the association between the offspring which determines the likelihood that the offspring will be cared for. The relatedness between offspring is determined by the genetic relatedness, their sharing of the same genome, their similarity in life history and their spatial proximity. The relatedness between parents is determined by their genetic relatedness, the strength of the association between the parents which determines the consequences of care, and by the strength of the association between the parents which determines the likelihood that the offspring will be cared for. The strength of the association between the parents is determined by the strength of their social interactions and the social environment. The strength of the association between the offspring is determined by their similarity in life history and their spatial proximity. The relatedness between parents and offspring",chatgpt,1,generated_gpt3,chatgpt,0.9997785687446594,"The effect of parental care in a social species is influenced by the genetic relatedness of the offspring. The relatedness between offspring is determined by their genetic relatedness, the strength of the association between the parents which determines the consequences of care, and by the strength of the association between the offspring which determines the likelihood that the offspring will be cared for. The relatedness between offspring is determined by the genetic relatedness, their sharing of the same genome, their similarity in life history and their spatial proximity. The relatedness between parents is determined by their genetic relatedness, the strength of the association between the parents which determines the consequences of care, and by the strength of the association between the parents which determines the likelihood that the offspring will be cared for. The strength of the association between the parents is determined by the strength of their social interactions and the social environment. The strength of the association between the offspring is determined by their similarity in life history and their spatial proximity. The relatedness between parents and offspring",,chatgpt,0.9997785687446594
114,230.0,26584,"In this paper, we supplement the work of Hall and Hsu and Weis and Weis (2012). Our analysis is based on a discrete network model with homogeneous nodes and cliques, which can be extended to a continuum network model by adding homogeneous edges. We argue that the main difference between the discrete and continuum models is the number of nodes. This allows us to demonstrate that the original results for the continuum case are independent of the number of nodes, and that the results for the discrete case are also independent of the number of nodes. We show that the continuum case is more realistic because of the more complicated underlying structure. We also argue that the original result for the discrete case is only valid if the underlying network is a random graph, which is a special case of a random network. We show that the results for the continuum case are independent of the underlying network.",chatgpt,1,generated_gpt3,chatgpt,0.999799907207489,"In this paper, we supplement the work of student residence and Hsu and Weis and Weis (2012). Our analysis is based on a discrete network model with homogeneous nodes and cliques, which can be extended to a continuum network model by adding homogeneous edges. We argue that the main difference between the discrete and continuum models is the number of nodes. This allows us to demonstrate that the original results for the continuum case are independent of the number of nodes, and that the results for the discrete case are also independent of the number of nodes. We show that the continuum case is more realistic because of the more complicated underlying structure. We also argue that the original result for the discrete case is only valid if the underlying network is a random graph, which is a special case of a random network. We show that the results for the continuum case are independent of the underlying network. ",+student residence,chatgpt,0.9998127818107605
115,231.0,26689,"Recent studies have shown that the genetic variation and diversity in humans is considerably higher than commonly believed. However, it remains unknown how this genetic diversity translates into phenotypic diversity, and it is also not known whether this diversity is maintained and passed on to the offspring. By using the first 200 sequenced genomes, we show that the average human genome contains a large number of linked genetic polymorphisms (LGs). We also demonstrate that the mean number of LGs per genome is significantly higher in Europeans than in East Asians, and that the distribution of LGs is bimodal among Europeans, with a higher number of LGs in the right-hand tail of the tail. Furthermore, we show that the average number of LGs per genome is significantly lower in the most recent common ancestor of both the high and low diversity genomes, and we also show that this difference is not driven by a higher number of LGs in the high diversity genomes. The number of LGs per genome is also lower",chatgpt,1,generated_gpt3,chatgpt,0.9997968077659608,"Recent studies have shown that the genetic variation and diversity in humans is considerably higher than commonly believed. However, it remains unknown region how this genetic diversity translates into phenotypic diversity, and it is also not known whether this diversity is maintained and passed on to the offspring. By using the first 200 sequenced genomes, we show that the average human genome contains a large number of linked genetic polymorphisms (LGs). We also demonstrate that the mean number of LGs per genome is significantly higher in Europeans than in East Asians, and that the distribution of LGs is bimodal among Europeans, with a higher number of LGs in the right - hand tail of the tail. Furthermore, we show that the average number of LGs per genome is significantly lower in the most recent common ancestor of both the high and low diversity genomes, and we also show that this difference is not driven by a higher number of LGs in the high diversity genomes. The number of LGs per genome is also lower",+unknown region,chatgpt,0.5543778538703918
116,,884,"In this paper we present a secure implementation architecture of a coprocessor for the TLSv1.2 protocol, on an FPGA. Techniques were used that increase the resistance of the design to side channel attacks, and also protect the private key data from software based attacks. The processor was implemented with a secure true random number generator which incorporates failure detection and thorough post-processing of the random bitstream. The design also includes hardware for signature generation and verification; based on elliptic curve algorithms. The algorithms used for performing the elliptic curve arithmetic were chosen to provide resistance against SPA and DPA attacks. Implementations of the AES and SHA256 algorithms are also included in order to provide full hardware acceleration for a specific suite of the TLSv1.2 protocol. The design is analysed for area and speed on a Virtex 5 FPGA.",human,0,micpro_original,human,0.9998075366020204,"In this paper we present a secure implementation architecture of a coprocessor for the TLSv1.2 protocol, on an FPGA. Techniques were used that increase the resistance of the design to side channel attacks, and also protect the private key data from software based attacks. The processor was implemented with a secure true random number generator which incorporates failure detection and thorough post-processing of the random bitstream. The design also includes hardware for signature generation and verification; based on elliptic curve algorithms. The algorithms used for performing the elliptic curve arithmetic were chosen to provide resistance against SPA and DPA attacks. Implementations of the AES and SHA256 algorithms are also included in order to provide full hardware acceleration for a specific suite of the TLSv1.2 protocol. The design is analysed for area and speed on a Virtex 5 FPGA.",,human,0.9998075366020203
117,,7577,"In this paper we present a new temporal partitioning methodology used for the data-path part of an algorithm for the reconfigurable embedded system design. This temporal partitioning uses an assessing trade-offs in time constraint, design size and field programmable gate arrays device parameters (circuit speed, reconfiguration time). The originality of our method is that we use the dynamic reconfiguration in order to minimize the number of cells needed to implement the data-path of an application under a time constraint. Our method consists, by taking into account the used technology, in evaluating the algorithm area and operators execution time from data flow graph. Thus, we deduce the right number of reconfigurations and the algorithm partitioning for Run-Time Reconfiguration implementation. This method allows avoiding an oversizing of implementation resources needed. This optimizing approach can be useful for the design of an embedded device or system. Our approach is illustrated by various reconfigurable implementations of real time image processing data-path.",human,0,micpro_original,human,0.8117949962615967,"In this paper we present a new temporal partitioning methodology used for the data-path part of an algorithm for the reconfigurable embedded system design. This temporal partitioning uses an assessing trade-offs in time constraint, design size and field programmable gate arrays device parameters (circuit speed, reconfiguration time). The originality of our method is that we use the dynamic reconfiguration in order to minimize the number of cells needed to implement the data-path of an application under a time constraint. Our method consists, by taking into account the used technology, in evaluating the algorithm area and operators execution time from data flow graph. Thus, we deduce the right number of reconfigurations and the algorithm partitioning for Run-Time Reconfiguration implementation. This method allows avoiding an oversizing of implementation resources needed. This optimizing approach can be useful for the design of an embedded device or system. Our approach is illustrated by various reconfigurable implementations of real time image processing data-path.",,human,0.8117951154708862
118,,25340,"Aérospatiale Avions uses a method, a language and tools to obtain detailed specifications of the Airbus onboard flight control systems under its responsibility. The origins of this workshop developed by Aérospatiale lie in the SAO (computer assisted specification) language based on a graphics formalism, known to all electronics and automation experts, which covers the field of flight control laws and operational logics. The possibility of automatically generating software from this formalism has allowed Aérospatiale to develop tools for real-time simulation and critical system specification validation as early as the design stage. The specifications thus validated are then automatically translated into ‘aircraft’ software, guaranteeing that the behaviour of the onboard system will be identical to the behaviour observed during simulation. This method and these tools were put to the test and proven within the scope of the Airbus A340 development. The quality of the specifications thus obtained enabled a significant reduction in the number of modifications to the onboard systems and led to substantial cuts in cost and debugging times. The example of debugging the Airbus A340 fly-by-wire specifications is used to back up the paper, which will also present the future specification integrated workshop, CSAO (computer-aided specification and design), developed by Aérospatiale Avions and Vérilog.",human,0,micpro_original,human,0.9998241066932678,"Aérospatiale Avions uses a method, a language and tools to obtain detailed specifications of the Airbus onboard flight control systems under its responsibility. The origins of this workshop developed by Aérospatiale lie in the SAO (computer assisted specification) language based on a graphics formalism, known to all electronics and automation experts, which covers the field of flight control laws and operational logics. The possibility of automatically generating software from this formalism has allowed Aérospatiale to develop tools for real-time simulation and critical system specification validation as early as the design stage. The specifications thus validated are then automatically translated into ‘aircraft’ software, guaranteeing that the behaviour of the onboard system will be identical to the behaviour observed during simulation. This method and these tools were put to the test and proven within the scope of the Airbus A340 development. The quality of the specifications thus obtained enabled a significant reduction in the number of modifications to the onboard systems and led to substantial cuts in cost and debugging times. The example of debugging the Airbus A340 fly-by-wire specifications is used to back up the paper, which will also present the future specification integrated workshop, CSAO (computer-aided specification and design), developed by Aérospatiale Avions and Vérilog.",,human,0.9998241066932678
119,,7555,This paper is concerned with the fundamental problem of how to make the design of large systems intellectually manageable. The literature is littered with methodologies which favour this or that approach to the problem. Nearly always they reflect the rather narrow interests of their sponsors. The situation is now acute as well as chronic as we try to design larger and more reliable systems in a shorter time. An approach that has proved successful for creating large real-time and database based products is described.,human,0,micpro_original,human,0.8050665259361267,This paper is concerned with the fundamental problem of how to make the design of large systems intellectually manageable. The literature is littered with methodologies which favour this or that approach to the problem. Nearly always they reflect the rather narrow interests of their sponsors. The situation is now acute as well as chronic as we try to design larger and more reliable systems in a shorter time. An approach that has proved successful for creating large real-time and database based products is described.,,human,0.8050657510757446
120,,3710,"The merits of microprocessor development on a host mini-computer system are described, using crossassemblers and crosscompilers. Aspects of cost are developed particularly and a crossdevelopment system is proposed as being far lower in cost than stand-alone microprocessor development systems, providing that there is already a computer available. For multiple users, crossdevelopment systems may be more economical than individual systems, even if the host computer is purchased specifically for that purpose. The second section describes the author's development system. A PDP-11/60 is used for writing and generating object programs for the MC6800. A commercial crossassembler, a crossassembler written by the author and a crosscompiler written by the author are used. Construction details for the system are provided and examples of typical development sessions are given.",human,0,micpro_original,human,0.9953248500823976,"The merits of microprocessor development on a host mini-computer system are described, using crossassemblers and crosscompilers. Aspects of cost are developed particularly and a crossdevelopment system is proposed as being far lower in cost than stand-alone microprocessor development systems, providing that there is already a computer available. For multiple users, crossdevelopment systems may be more economical than individual systems, even if the host computer is purchased specifically for that purpose. The second section describes the author's development system. A PDP-11/60 is used for writing and generating object programs for the MC6800. A commercial crossassembler, a crossassembler written by the author and a crosscompiler written by the author are used. Construction details for the system are provided and examples of typical development sessions are given.",,human,0.9953248500823975
121,,5528,"Correlation is a mathematical technique normally found in texts on analogue signal processing, control theory or pattern recognition. However, correlation is a technique that can be applied to digital signals just as well as to analogue signals. Indeed, it is now possible to obtain ready-made digital correlators. What then is correlation? In the simplest of terms, correlation involves ‘sliding’ two sequences of signals against each other and then attempting to determine how closely the sequences resemble each other as they move with respect to each other. This application note provides an overview of digital correlation and demonstrates how correlation techniques can be applied to the detection of particular patterns in a data stream, to the synchronization of detectors in demodulation processes and to the determination of the time delay between two digital sequences.",human,0,micpro_original,human,0.9958525896072388,"Correlation is a mathematical technique normally found in texts on analogue signal processing, control theory or pattern recognition. However, correlation is a technique that can be applied to digital signals just as well as to analogue signals. Indeed, it is now possible to obtain ready-made digital correlators. What then is correlation? In the simplest of terms, correlation involves ‘sliding’ two sequences of signals against each other and then attempting to determine how closely the sequences resemble each other as they move with respect to each other. This application note provides an overview of digital correlation and demonstrates how correlation techniques can be applied to the detection of particular patterns in a data stream, to the synchronization of detectors in demodulation processes and to the determination of the time delay between two digital sequences.",,human,0.9958525896072388
122,,12526,"The CMOS technology scaling brings new challenges in temperature, reliability, performance and leakage power. Most of the thermal management techniques compromise performance to control thermal behavior of the system by slowing down or turning off processors. In this paper, we use Stochastic Activity Networks (SANs) to model and evaluate the power consumption of a multi-core system with respect to thermal constraints. The Dynamic Voltage and Frequency Scaling (DVFS) technique is used, in our proposed model, for dynamically controlling the temperature of cores. We define multiple thresholds for the temperature of cores and apply the DVFS technique, by assigning lower voltage/frequency to the core with higher temperature. Results obtained from analytically solving the proposed SAN model are compared with the data gathered from experiments on a quad-core system. The accuracy of the proposed model in evaluating power consumption of six CPU-intensive applications is higher than 90% when compared with the experimental data.",human,0,micpro_original,human,0.905917763710022,"The CMOS technology scaling brings new challenges in temperature, reliability, performance and leakage power. Most of the thermal management techniques compromise performance to control thermal behavior of the system by slowing down or turning off processors. In this paper, we use Stochastic Activity Networks (SANs) to model and evaluate the power consumption of a multi-core system with respect to thermal constraints. The Dynamic Voltage and Frequency Scaling (DVFS) technique is used, in our proposed model, for dynamically controlling the temperature of cores. We define multiple thresholds for the temperature of cores and apply the DVFS technique, by assigning lower voltage/frequency to the core with higher temperature. Results obtained from analytically solving the proposed SAN model are compared with the data gathered from experiments on a quad-core system. The accuracy of the proposed model in evaluating power consumption of six CPU-intensive applications is higher than 90% when compared with the experimental data.",,human,0.9059175252914429
123,,3157,"In 2016, Renes et al. were the first to propose complete addition formulas for Elliptic Curve Cryptography (ECC) on Weierstrass curves. With these formulas, the same set of equations can be used for point addition and point doubling, which makes software and hardware implementations less vulnerable to side-channel (SCA) attacks. Further, all inputs are valid, so there is no need for conditional statements handling special cases such as the point at infinity. This paper presents the first ASIC design of the complete addition formulas of Renes et al. Each computation layer in the design is balanced, from the field arithmetic to the point multiplication. The design explores two datapaths: a full-width Montgomery Multiplier ALU (MMALU) with a built-in adder and a serialized version of the MMALU. The interface sizes of the MMALU are optimized through an exploration of the design parameters. The register file size is minimized through an optimal scheduling of the modular operations. The top-level point multiplication is implemented using the Montgomery ladder algorithm, with the additional option of randomizing the execution order of the point operations as a countermeasure against SCA attacks. The implementation results after synthesis are generated using the open source NANGATE45 library.",human,0,micpro_original,human,0.9997686743736268,"In 2016, Renes et al. were the first to propose complete addition formulas for Elliptic Curve Cryptography (ECC) on Weierstrass curves. With these formulas, the same set of equations can be used for point addition and point doubling, which makes software and hardware implementations less vulnerable to side-channel (SCA) attacks. Further, all inputs are valid, so there is no need for conditional statements handling special cases such as the point at infinity. This paper presents the first ASIC design of the complete addition formulas of Renes et al. Each computation layer in the design is balanced, from the field arithmetic to the point multiplication. The design explores two datapaths: a full-width Montgomery Multiplier ALU (MMALU) with a built-in adder and a serialized version of the MMALU. The interface sizes of the MMALU are optimized through an exploration of the design parameters. The register file size is minimized through an optimal scheduling of the modular operations. The top-level point multiplication is implemented using the Montgomery ladder algorithm, with the additional option of randomizing the execution order of the point operations as a countermeasure against SCA attacks. The implementation results after synthesis are generated using the open source NANGATE45 library.",,human,0.9997686743736267
124,,550,"A driving unit for artificial hearts has been produced. This unit uses sodium solution for the transmission of force between the power source and the artificial ventricle instead of the commonly used compressed air and thus avoids the danger of an air embolism. The power source of the hydraulic driving unit is an electromagnet; its armature drives a rolling membrane pump (safety chamber) which moves the transmission fluid (sodium solution) to the artificial ventricle (ellipsoid heart). The incompressible connection between the safety chamber and the ellipsoid heart allows direct control of the membrane motion in the ellipsoid heart and of the volume of pumped blood by measuring the armature stroke. The stroke is measured continuously with an optical position sensor; preprogrammable positions determine the systole end and diastole end respectively, so that the membrane need not strike the housing and is not exposed to additional stress. This volume-controlled mode is characterized by automatic selfregulation according to Starling's law. The microcomputer is programmed in a two-level technique. In the interrupt level the solenoid current is calculated according to the operating mode and armature position every 10 ms which is sufficient to simulate continuous operation. In the main-program level every operational parameter (maximum force, systole end, diastole end; systole duration and beat frequency for the frequency-fixing mode) is displayed on the CRT and can be changed via keyboard entry without any interruption in pumping. The durations of systole and diastole, respectively, are determined by the pumping force, the friction in the hydraulic transmission system and the preload and afterload respectively. The preload and afterload can be calculated from a time-force relation without invasive measurements. The driving unit has shown its haemodynamic efficiency in several in vivo experiments and has been running for 8 months in an in vitro durability test.",human,0,micpro_original,human,0.9998005032539368,"A driving unit for artificial hearts has been produced. This unit uses sodium solution for the transmission of force between the power source and the artificial ventricle instead of the commonly used compressed air and thus avoids the danger of an air embolism. The power source of the hydraulic driving unit is an electromagnet; its armature drives a rolling membrane pump (safety chamber) which moves the transmission fluid (sodium solution) to the artificial ventricle (ellipsoid heart). The incompressible connection between the safety chamber and the ellipsoid heart allows direct control of the membrane motion in the ellipsoid heart and of the volume of pumped blood by measuring the armature stroke. The stroke is measured continuously with an optical position sensor; preprogrammable positions determine the systole end and diastole end respectively, so that the membrane need not strike the housing and is not exposed to additional stress. This volume-controlled mode is characterized by automatic selfregulation according to Starling's law. The microcomputer is programmed in a two-level technique. In the interrupt level the solenoid current is calculated according to the operating mode and armature position every 10 ms which is sufficient to simulate continuous operation. In the main-program level every operational parameter (maximum force, systole end, diastole end; systole duration and beat frequency for the frequency-fixing mode) is displayed on the CRT and can be changed via keyboard entry without any interruption in pumping. The durations of systole and diastole, respectively, are determined by the pumping force, the friction in the hydraulic transmission system and the preload and afterload respectively. The preload and afterload can be calculated from a time-force relation without invasive measurements. The driving unit has shown its haemodynamic efficiency in several in vivo experiments and has been running for 8 months in an in vitro durability test.",,human,0.9998005032539368
125,,25074,"The safety of oil and gas production platforms, petrochemical and general process plants demands an automatic and reliable shutdown system to protect equipment, processes, the environment and personnel. A shutdown system responds to sensors detecting excessive variations in the operating parameters of the plant. The system takes into account multiple input combinations and initiates the appropriate outputs in the form of predetermined shutdown sequences. A fully programmable triplicated microprocessor system has been developed which incoprporates self checking, high reliability, programmability and other features, not easily attained using conventional techniques.",human,0,micpro_original,human,0.9998279809951782,"The safety of oil and gas production platforms, petrochemical and general process plants demands an automatic and reliable shutdown system to protect equipment, processes, the environment and personnel. A shutdown system responds to sensors detecting excessive variations in the operating parameters of the plant. The system takes into account multiple input combinations and initiates the appropriate outputs in the form of predetermined shutdown sequences. A fully programmable triplicated microprocessor system has been developed which incoprporates self checking, high reliability, programmability and other features, not easily attained using conventional techniques.",,human,0.9998279809951782
126,,17538,"In high-performance processors, the accuracy of branch prediction plays a significant role in enhancing computer execution. A new hardware approach is presented in this paper to dynamically predict branch directions using path information. As an execution path contains large information, we compress the large information using a technique based on the linear feedback shift register (LFSR) that is widely used in testing and error correction coding. A modified version of LFSR, called windowed LFSR, is developed to calculate the signature of a path in one cycle. Since the windowed LFSR has a very regular structure, it can be easily implemented. For most of the benchmarks, the proposed prediction scheme shows better prediction accuracy than the pattern-based predictions.",human,0,micpro_original,chatgpt,0.9161754846572876,"In high-performance processors, the accuracy of branch prediction plays a significant role in enhancing computer execution. A new hardware approach is presented in this paper to dynamically predict branch directions using path information. As an execution path contains large information, we compress the large information using a technique based on the linear feedback shift register (LFSR) that is widely used in testing and error correction coding. A modified version of LFSR, called windowed LFSR, is developed to calculate the signature of a path in one cycle. Since the windowed LFSR has a very regular structure, it can be easily implemented. For most of the benchmarks, the proposed prediction scheme shows better prediction accuracy than the pattern-based predictions.",,chatgpt,0.9161759614944458
127,,4056,"A simple algorithm enabling the component frequencies of a dual-tone signal to be found by microprocessor analysis of the pattern of intervals between zero crossings of the waveform, even in the presence of tone amplitude differences is described. The algorithm was developed for telecommunication signalling systems and has been used in an experimental receiver for signals from pushbutton telephones. With certain constraints it is suitable for the detection of any two-tone signal.",human,0,micpro_original,chatgpt,0.5120067000389099,"A simple algorithm enabling the component frequencies of a dual-tone signal to be found by microprocessor analysis of the pattern of intervals between zero crossings of the waveform, even in the presence of tone amplitude differences is described. The algorithm was developed for telecommunication signalling systems and has been used in an experimental receiver for signals from pushbutton telephones. With certain constraints it is suitable for the detection of any two-tone signal.",,chatgpt,0.5120095610618591
128,,11277,"The integration of Multi-Processors System-on-Chip (MPSoCs) into the Internet-of-Things (IoT) context brings new opportunities, but also represent risks. Tight real-time constraints and security requirements should be considered simultaneously when designing MPSoCs. Network-on-Chip (NoCs) are specially critical when meeting these two conflicting characteristics. For instance the NoC design has a huge influence in the security of the system. A vital threat to system security are so-called side-channel attacks based on the NoC communication observations. To this end, we propose a NoC security mechanism suitable for hard real-time systems, in which schedulability is a vital design requirement. We present three contributions. First, we show the impact of the NoC routing in the security of the system. Second, we propose a packet route randomisation mechanism to increase NoC resilience against side-channel attacks. Third, using an evolutionary optimisation approach, we effectively apply route randomisation while controlling its impact on hard real-time performance guarantees. Extensive experimental evidence based on analytical and simulation models supports our findings.",human,0,micpro_original,human,0.9997528195381165,"The integration of Multi-Processors System-on-Chip (MPSoCs) into the Internet-of-Things (IoT) context brings new opportunities, but also represent risks. Tight real-time constraints and security requirements should be considered simultaneously when designing MPSoCs. Network-on-Chip (NoCs) are specially critical when meeting these two conflicting characteristics. For instance the NoC design has a huge influence in the security of the system. A vital threat to system security are so-called side-channel attacks based on the NoC communication observations. To this end, we propose a NoC security mechanism suitable for hard real-time systems, in which schedulability is a vital design requirement. We present three contributions. First, we show the impact of the NoC routing in the security of the system. Second, we propose a packet route randomisation mechanism to increase NoC resilience against side-channel attacks. Third, using an evolutionary optimisation approach, we effectively apply route randomisation while controlling its impact on hard real-time performance guarantees. Extensive experimental evidence based on analytical and simulation models supports our findings.",,human,0.9997528195381165
129,,1918,"The receiving end of a data communication system requires a digital filter — an equalizer — to correct distortions induced by the channel. The optimum tap coefficients of an equalizer need to be determined in realtime for a time-varying channel. A microprocessor-based implementation of an equalizer is proposed, and the hardware/software design given. Results show that a microprocessor-based implementation of an equalizer for data communication systems using voice channels is feasible in realtime. The system was implemented on a PC XT based on 8088/8087 processors, but is easily adaptable to faster microprocessors that are available today.",human,0,micpro_original,human,0.9963885545730592,"The receiving end of a data communication system requires a digital filter — an equalizer — to correct distortions induced by the channel. The optimum tap coefficients of an equalizer need to be determined in realtime for a time-varying channel. A microprocessor-based implementation of an equalizer is proposed, and the hardware/software design given. Results show that a microprocessor-based implementation of an equalizer for data communication systems using voice channels is feasible in realtime. The system was implemented on a PC XT based on 8088/8087 processors, but is easily adaptable to faster microprocessors that are available today.",,human,0.9963885545730591
130,,2270,"Modern processors incorporate complex arithmetic units that can work with large word-lengths. Those units are useful for applications that require high precision. There are however, many applications for which the use of reduced precision is sufficient. In those cases, one possibility is to use the large word-length arithmetic units to implement reduced precision operations with additional error detection. In this paper, this idea is explored for the case of matrix multiplications. A technique is presented and evaluated. The results show that it can detect most errors and that for large matrixes the overhead in terms of execution time is small.",human,0,micpro_original,chatgpt,0.9254339337348938,"Modern processors incorporate complex arithmetic units that can work with large word-lengths. Those units are useful for applications that require high precision. There are however, many applications for which the use of reduced precision is sufficient. In those cases, one possibility is to use the large word-length arithmetic units to implement reduced precision operations with additional error detection. In this paper, this idea is explored for the case of matrix multiplications. A technique is presented and evaluated. The results show that it can detect most errors and that for large matrixes the overhead in terms of execution time is small.",,chatgpt,0.9254335165023804
131,,9747,"The Third Working Draft of the Modula-2 International Standard has been registered with the International Standards Organization (ISO) as a draft proposal. Successive drafts have been prepared by the British Standards Institute (BSI) Modula-2 panel IST/5/13 as guided by consensus decisions of the ISO/IEC working group JTC1/ SC22/WG13. In the proposed standard, the semantics of conforming MODULA-2 programs, and of standard library modules, are formally defined using the denotational specification language vdm-sl . The semantics are also described by an informal English language definition. Apart from delivering the specified behaviour for programs, certain conformance requirements are imposed on implementations of MODULA-2. The paper summarizes major language changes and clarifications including rules for use before declaration, import and export, numeric types and conversions, set types, structured value constructors, open arrays and protected modules.",human,0,micpro_original,human,0.9997535347938538,"The Third Working Draft of the Modula-2 International Standard has been registered with the International Standards Organization (ISO) as a draft proposal. Successive drafts have been prepared by the British Standards Institute (BSI) Modula-2 panel IST/5/13 as guided by consensus decisions of the ISO/IEC working group JTC1/ SC22/WG13. In the proposed standard, the semantics of conforming MODULA-2 programs, and of standard library modules, are formally defined using the denotational specification language vdm-sl . The semantics are also described by an informal English language definition. Apart from delivering the specified behaviour for programs, certain conformance requirements are imposed on implementations of MODULA-2. The paper summarizes major language changes and clarifications including rules for use before declaration, import and export, numeric types and conversions, set types, structured value constructors, open arrays and protected modules.",,human,0.9997535347938538
132,,25190,"The paper presents a method for software specification in the domain of industrial process control. As a result of ever more demanding applications, particularly in the area of PLC based control, there is an emerging need for introducing methods that include an appropriate conceptual model and define the specification process. An approach to the method development is presented, based on an architectural extension of the Ward–Mellor structured real time software engineering method. The core of the method is a conceptual model of the equipment control specification, which consists of two essential parts: the first is that of process equipment entities, and the second is the functional part. The former is further decomposed in relation to two aspects of the process equipment entities: the structural aspect and the classification aspect. In order to make the conceptual model more clear some segments of the real process control application dealing with the hydrolysis process in the production of TiO2 are given.",human,0,micpro_original,human,0.9960594177246094,"The paper presents a method for software specification in the domain of industrial process control. As a result of ever more demanding applications, particularly in the area of PLC based control, there is an emerging need for introducing methods that include an appropriate conceptual model and define the specification process. An approach to the method development is presented, based on an architectural extension of the Ward–Mellor structured real time software engineering method. The core of the method is a conceptual model of the equipment control specification, which consists of two essential parts: the first is that of process equipment entities, and the second is the functional part. The former is further decomposed in relation to two aspects of the process equipment entities: the structural aspect and the classification aspect. In order to make the conceptual model more clear some segments of the real process control application dealing with the hydrolysis process in the production of TiO2 are given.",,human,0.9960594177246094
133,,15139,"HARP is a new multiple-instruction-issue architecture developed at the University of Hertfordshire. This paper describes the essential features of the HARP machine model and presents two compile-time scheduling techniques, called local and conditional compaction, which have been developed for the architecture. Local compaction schedules the instructions within a basic block. Conditional compaction uses HARP's conditional execution mechanism to schedule instructions across basic block boundaries. This paper reports performance measurements obtained using simulations of the model. These results indicate that a HARP processor will achieve sustained instruction execution rates in excess of two sequential instructions per cycle for compiled, integer, general-purpose computations.",human,0,micpro_original,human,0.9231066703796388,"HARP is a new multiple-instruction-issue architecture developed at the University of Hertfordshire. This paper describes the essential features of the HARP machine model and presents two compile-time scheduling techniques, called local and conditional compaction, which have been developed for the architecture. Local compaction schedules the instructions within a basic block. Conditional compaction uses HARP's conditional execution mechanism to schedule instructions across basic block boundaries. This paper reports performance measurements obtained using simulations of the model. These results indicate that a HARP processor will achieve sustained instruction execution rates in excess of two sequential instructions per cycle for compiled, integer, general-purpose computations.",,human,0.9231059551239014
134,,2605,"This paper introduces the design and implementation of a simple yet efficient technique for Infinite Impulse Response (IIR) bandpass filters that can be used for simultaneous multiple frequency estimation applications. The technique, which is implemented on a dsPIC microcontroller, uses variable data sampling rates and a table of predefined filter coefficients simultaneously to dynamically adjust its pass band precisely to the range of interest. The technique developed may be deployed to provide a more reliable and efficient algorithm which can be flexibly utilised as part of a modular development of high performance, reliable sensing systems. The developed system is described in detail in the context of the e-Monitoring of a milling cutting process. In this case spindle speed and spindle load signals from a machine tool have been used as a source of information relating to the health of the cutting process being undertaken.",human,0,micpro_original,human,0.8877717852592468,"This paper introduces the design and implementation of a simple yet efficient technique for Infinite Impulse Response (IIR) bandpass filters that can be used for simultaneous multiple frequency estimation applications. The technique, which is implemented on a dsPIC microcontroller, uses variable data sampling rates and a table of predefined filter coefficients simultaneously to dynamically adjust its pass band precisely to the range of interest. The technique developed may be deployed to provide a more reliable and efficient algorithm which can be flexibly utilised as part of a modular development of high performance, reliable sensing systems. The developed system is described in detail in the context of the e-Monitoring of a milling cutting process. In this case spindle speed and spindle load signals from a machine tool have been used as a source of information relating to the health of the cutting process being undertaken.",,human,0.8877716660499573
135,,26098,"Although microarrays are already having a tremendous impact on biomedical science, they still present great computational challenges. We examine a particular problem involving the computation of linear regressions on a large number of vector combinations in a high-dimensional parameter space, a problem that was found to be virtually intractable on a PC cluster. We observe that characteristics of this problem map particularly well to FPGAs and confirm this with an implementation that results in a 1600-fold speed-up over an optimized serial implementation. Some of the other contributions involve the data routing structure, the analysis of bit-width allocation, and the handling of missing data. Since this problem is representative of many in functional genomics, part of the overall significance of this work is that it points to a new area of applicability for FPGA coprocessors.",human,0,micpro_original,human,0.9998270869255066,"Although microarrays are already having a tremendous impact on biomedical science, they still present great computational challenges. We examine a particular problem involving the computation of linear regressions on a large number of vector combinations in a high-dimensional parameter space, a problem that was found to be virtually intractable on a PC cluster. We observe that characteristics of this problem map particularly well to FPGAs and confirm this with an implementation that results in a 1600-fold speed-up over an optimized serial implementation. Some of the other contributions involve the data routing structure, the analysis of bit-width allocation, and the handling of missing data. Since this problem is representative of many in functional genomics, part of the overall significance of this work is that it points to a new area of applicability for FPGA coprocessors.",,human,0.9998270869255066
136,,8964,"In 1983, a competition was announced to build a microprocessor-based robot to play pingpong. The paper describes one of the systems built in response to the article, ‘Robat Charlie’ which is the current European robot pingpong champion after winning the finals of the competition in September 1985. System details and associated software are outlined, and a discussion of the robot's vision system and mechanics are given.",human,0,micpro_original,human,0.9997745156288148,"In 1983, a competition was announced to build a microprocessor-based robot to play pingpong. The paper describes one of the systems built in response to the article, ‘Robat Charlie’ which is the current European robot pingpong champion after winning the finals of the competition in September 1985. System details and associated software are outlined, and a discussion of the robot's vision system and mechanics are given.",,human,0.9997745156288147
137,,718,"The complexity achievable within a custom chip or on a PCB loaded with standard combinational or sequential elements, even without the use of VLSI components such as microprocessors, requires the use of automatic methods for the generation of test patterns if the task is to be completed within an acceptable time and at an acceptable cost. This paper reviews the current status of some aspects of the test process as applied to such circuits, and of the principles of structured design methodologies intended to reduce the difficulties of test pattern generation (TPG). The paper starts by reviewing the fault models on which most automatic TPG (ATPG) methods are based, and goes on to discuss some of the available ATPG methods themselves. The problems involved in TPG for sequential circuits are briefly discussed to show the motivation behind structured design for testability using the scan-in scan-out (SISO) principle. The main implications of SISO are described, as are some of the applications of these principles to the construction of testable PCBs.",human,0,micpro_original,human,0.999716341495514,"The complexity achievable within a custom chip or on a PCB loaded with standard combinational or sequential elements, even without the use of VLSI components such as microprocessors, requires the use of automatic methods for the generation of test patterns if the task is to be completed within an acceptable time and at an acceptable cost. This paper reviews the current status of some aspects of the test process as applied to such circuits, and of the principles of structured design methodologies intended to reduce the difficulties of test pattern generation (TPG). The paper starts by reviewing the fault models on which most automatic TPG (ATPG) methods are based, and goes on to discuss some of the available ATPG methods themselves. The problems involved in TPG for sequential circuits are briefly discussed to show the motivation behind structured design for testability using the scan-in scan-out (SISO) principle. The main implications of SISO are described, as are some of the applications of these principles to the construction of testable PCBs.",,human,0.9997163414955139
138,,5077,"Reconfigurable hardware offers new ways of accelerating computing by implementing hardware accelerators at run time. In this article, we present an approach allowing a hardware/software codesign of applications in which implementation can be chosen at run time depending on available resources. We propose a platform supporting this flow and describe its different implementations used to prove the feasibility of our approach. This platform allows the underlying hardware to be virtualized in order to have a generic architecture that can be used to run applications. Partial dynamic reconfiguration is used over Xilinx Virtex 5 boards to enhance reconfiguration capabilities.",human,0,micpro_original,human,0.9997251629829408,"Reconfigurable hardware offers new ways of accelerating computing by implementing hardware accelerators at run time. In this article, we present an approach allowing a hardware/software codesign of applications in which implementation can be chosen at run time depending on available resources. We propose a platform supporting this flow and describe its different implementations used to prove the feasibility of our approach. This platform allows the underlying hardware to be virtualized in order to have a generic architecture that can be used to run applications. Partial dynamic reconfiguration is used over Xilinx Virtex 5 boards to enhance reconfiguration capabilities.",,human,0.9997251629829407
139,,18038,"Recently, we have witnessed a phenomenal growth in the Internet/Intranet coupled with rapid deployment of new services. Information dissemination over the network has become one of the most important activities in our daily life. The existing systems, however, often suffer from notorious long delay experienced by clients, especially during peak hours. This is caused by the combination of a variety of factors including inadequate link bandwidth, server overload and network congestion. Server replication has been shown to be one of the most effective mechanisms to cope with this problem. The basic idea is to replicate the information across a network so that the clients’ requests can be spread out. The major issue is in which locations inside the network this replication takes place at, i.e. where to place the replicated servers. In this paper we investigate the server replication in a controlled distributed environment. The salient feature of this environment is that the decision of where to replicate information can be determined by a single authority, the Intranet being the typical example. We consider the problem of placing multiple replicated servers within a network, given there exist multiple target web servers as information providers. We formulate this as an optimization problem by taking into consideration the characteristics of the network topology. We first show that this is an NP-complete problem, and then present a number of heuristics-based algorithms for server replications. Finally, in order to investigate various tradeoffs in terms of cost and algorithm complexity, we carry out comparison studies among different heuristic algorithms.",human,0,micpro_original,human,0.9998160004615784,"Recently, we have witnessed a phenomenal growth in the Internet/Intranet coupled with rapid deployment of new services. Information dissemination over the network has become one of the most important activities in our daily life. The existing systems, however, often suffer from notorious long delay experienced by clients, especially during peak hours. This is caused by the combination of a variety of factors including inadequate link bandwidth, server overload and network congestion. Server replication has been shown to be one of the most effective mechanisms to cope with this problem. The basic idea is to replicate the information across a network so that the clients’ requests can be spread out. The major issue is in which locations inside the network this replication takes place at, i.e. where to place the replicated servers. In this paper we investigate the server replication in a controlled distributed environment. The salient feature of this environment is that the decision of where to replicate information can be determined by a single authority, the Intranet being the typical example. We consider the problem of placing multiple replicated servers within a network, given there exist multiple target web servers as information providers. We formulate this as an optimization problem by taking into consideration the characteristics of the network topology. We first show that this is an NP-complete problem, and then present a number of heuristics-based algorithms for server replications. Finally, in order to investigate various tradeoffs in terms of cost and algorithm complexity, we carry out comparison studies among different heuristic algorithms.",,human,0.9998160004615784
140,,12442,"The BOAR emulation system is targeted to hardware/software (HW/SW) codevelopment of advanced embedded DSP and telecom systems. The challenge of the BOAR system is efficient customization of programmable hardware, and dedicated partitioning routine to target applications and structures, which allows quite high overall system performance. The system allows multiple configurations for communication between processors and field programmable gate arrays (FPGAs) making the BOAR system an efficient tool for real-time HW/SW coverification. The reprogrammable hardware of the emulation tool is based on four Xilinx 4000-series devices, two Texas TMS320C50 signal processors and one Motorola MC68302 microcontroller. With current devices the BOAR hardware provides approximately 40–70 kgates of logic capacity in DSP applications. The emulation capacity can be expanded by connecting several similar boards in chain. The system has also a versatile internal reprogrammable test environment for test bench development, performance evaluations and design debugging. The logic development environment is based on the Synopsys synthesis tools and an automatic design management software, which performs resource mapping and performance-driven design partitioning between FPGAs. The emulation hardware is currently connected to logic and software development environments via an RS-232C bus. The BOAR emulation system has been found a very efficient platform for real-life prototyping of different types of DSP algorithms and systems, and validating correct functionality of a VHDL macro library.",human,0,micpro_original,human,0.9344683289527892,"The BOAR emulation system is targeted to hardware/software (HW/SW) codevelopment of advanced embedded DSP and telecom systems. The challenge of the BOAR system is efficient customization of programmable hardware, and dedicated partitioning routine to target applications and structures, which allows quite high overall system performance. The system allows multiple configurations for communication between processors and field programmable gate arrays (FPGAs) making the BOAR system an efficient tool for real-time HW/SW coverification. The reprogrammable hardware of the emulation tool is based on four Xilinx 4000-series devices, two Texas TMS320C50 signal processors and one Motorola MC68302 microcontroller. With current devices the BOAR hardware provides approximately 40–70 kgates of logic capacity in DSP applications. The emulation capacity can be expanded by connecting several similar boards in chain. The system has also a versatile internal reprogrammable test environment for test bench development, performance evaluations and design debugging. The logic development environment is based on the Synopsys synthesis tools and an automatic design management software, which performs resource mapping and performance-driven design partitioning between FPGAs. The emulation hardware is currently connected to logic and software development environments via an RS-232C bus. The BOAR emulation system has been found a very efficient platform for real-life prototyping of different types of DSP algorithms and systems, and validating correct functionality of a VHDL macro library.",,human,0.9344677925109863
141,,18446,"DRAM technology requires refresh operations to be performed in order to avoid data loss due to capacitance leakage. Refresh operations consume a significant amount of dynamic energy, which increases with the storage capacity. To reduce this amount of energy, prior work has focused on reducing refreshes in off-chip memories. However, this problem also appears in on-chip eDRAM memories implemented in current low-level caches. The refresh energy can dominate the dynamic consumption when a high percentage of the chip area is devoted to eDRAM cache structures. Replacement algorithms for high-associativity low-level caches select the victim block avoiding blocks more likely to be reused soon. This paper combines the state-of-the-art MRUT replacement algorithm with a novel refresh policy. Refresh operations are performed based on information produced by the replacement algorithm. The proposed refresh policy is implemented on top of an energy-aware eDRAM cache architecture, which implements bank-prediction and swap operations to save energy. Experimental results show that, compared to a conventional eDRAM design, the proposed energy-aware cache can achieve by 72% refresh energy savings. Considering the entire on-chip memory hierarchy consumption, the overall energy savings are 30%. These benefits come with minimal impact on performance (by 1.2%) and area overhead (by 0.4%).",human,0,micpro_original,human,0.9997643828392028,"DRAM technology requires refresh operations to be performed in order to avoid data loss due to capacitance leakage. Refresh operations consume a significant amount of dynamic energy, which increases with the storage capacity. To reduce this amount of energy, prior work has focused on reducing refreshes in off-chip memories. However, this problem also appears in on-chip eDRAM memories implemented in current low-level caches. The refresh energy can dominate the dynamic consumption when a high percentage of the chip area is devoted to eDRAM cache structures. Replacement algorithms for high-associativity low-level caches select the victim block avoiding blocks more likely to be reused soon. This paper combines the state-of-the-art MRUT replacement algorithm with a novel refresh policy. Refresh operations are performed based on information produced by the replacement algorithm. The proposed refresh policy is implemented on top of an energy-aware eDRAM cache architecture, which implements bank-prediction and swap operations to save energy. Experimental results show that, compared to a conventional eDRAM design, the proposed energy-aware cache can achieve by 72% refresh energy savings. Considering the entire on-chip memory hierarchy consumption, the overall energy savings are 30%. These benefits come with minimal impact on performance (by 1.2%) and area overhead (by 0.4%).",,human,0.9997643828392029
142,,23443,"This paper presents a concept for automated architecture synthesis for adaptive multiprocessors on chip, in particular for Field-Programmable Gate-Array (FPGA) devices. Given a parallel program, the intent is to simultaneously allocate processor resources and the corresponding communication network, and at the same time, to map the parallel application to get an optimum application-specific architecture. This approach builds up on a previously proposed design platform that automates system integration and FPGA synthesis for such architectures. As a result, the overall concept offers an automated design approach from application mapping to system and FPGA configuration. The automated synthesis is based on combinatorial optimization. Automation is possible because a solvable Integer Linear Programming (ILP) model that captures all necessary design trade-off parameters of such systems has been found. Experimental results to study the feasibility of the automated synthesis indicate that problems with sizes that can be encountered in the embedded domain can be readily solved. Results obtained underscore the need for an automated synthesis for design space exploration.",human,0,micpro_original,human,0.9838030338287354,"This paper presents a concept for automated architecture synthesis for adaptive multiprocessors on chip, in particular for Field-Programmable Gate-Array (FPGA) devices. Given a parallel program, the intent is to simultaneously allocate processor resources and the corresponding communication network, and at the same time, to map the parallel application to get an optimum application-specific architecture. This approach builds up on a previously proposed design platform that automates system integration and FPGA synthesis for such architectures. As a result, the overall concept offers an automated design approach from application mapping to system and FPGA configuration. The automated synthesis is based on combinatorial optimization. Automation is possible because a solvable Integer Linear Programming (ILP) model that captures all necessary design trade-off parameters of such systems has been found. Experimental results to study the feasibility of the automated synthesis indicate that problems with sizes that can be encountered in the embedded domain can be readily solved. Results obtained underscore the need for an automated synthesis for design space exploration.",,human,0.9838029146194458
143,,6820,"Multiple mobile robot systems working together to achieve a task have many advantages over single robot systems. However, the planning and execution of a task which is to be undertaken by multiple robots is extremely difficult. To date no tools exist which allow such systems to be engineered. One of the key questions that arises when developing such systems is: does communication between the robots aid the completion of the task, and if so what information should be communicated? This paper presents the results of an investigation undertaken to address the above question. The approach adopted is to utilize genetic programming (GP) with the aim of evolving a controller, and letting the evolution process determine what information should be communicated and how best to use this information. A number of experiments were performed with the aim of determining the communication requirements. The results of these experiments are presented in this paper. It is shown that the GP system evolved controllers whose performance benefitted as a result of the communication process.",human,0,micpro_original,human,0.5574622750282288,"Multiple mobile robot systems working together to achieve a task have many advantages over single robot systems. However, the planning and execution of a task which is to be undertaken by multiple robots is extremely difficult. To date no tools exist which allow such systems to be engineered. One of the key questions that arises when developing such systems is: does communication between the robots aid the completion of the task, and if so what information should be communicated? This paper presents the results of an investigation undertaken to address the above question. The approach adopted is to utilize genetic programming (GP) with the aim of evolving a controller, and letting the evolution process determine what information should be communicated and how best to use this information. A number of experiments were performed with the aim of determining the communication requirements. The results of these experiments are presented in this paper. It is shown that the GP system evolved controllers whose performance benefitted as a result of the communication process.",,human,0.5574626922607422
144,,25786,"The design and implementation of a communication system for an underwater remotely operated vehicle (ROV) are presented. The system is assumed to be composed of a remotely operated vehicle driven by a set of seven thrusters, a tethering cable including power supply cables and two fibres for communication (the function of which is to transmit and monitor information) and a surface master controlling computer. A second slave computer with its appropriate A D and D A interfacing cards is included in the ROV. The design and implementation of the fibre optic communication system between these two computers are presented in this paper. The appropriate control information is acquired and suitable control commands are send to the ROV through this system. With special RS-232C port handling, transfer rates up to 115.2 kbps are achieved. This communication system is fast enough for telemetry and control signals and can be used in a variety of related applications.",human,0,micpro_original,chatgpt,0.5653858780860901,"The design and implementation of a communication system for an underwater remotely operated vehicle (ROV) are presented. The system is assumed to be composed of a remotely operated vehicle driven by a set of seven thrusters, a tethering cable including power supply cables and two fibres for communication (the function of which is to transmit and monitor information) and a surface master controlling computer. A second slave computer with its appropriate A D and D A interfacing cards is included in the ROV. The design and implementation of the fibre optic communication system between these two computers are presented in this paper. The appropriate control information is acquired and suitable control commands are send to the ROV through this system. With special RS-232C port handling, transfer rates up to 115.2 kbps are achieved. This communication system is fast enough for telemetry and control signals and can be used in a variety of related applications.",,chatgpt,0.5653857588768005
145,,7725,"Many studies on the automotive engine as a control system have been investigated in order to meet emission regulations and to improve the fuel economy. Sophisticated control systems which, up to the introduction of the microcomputer were a problem, are now easy to realize. In this paper, the fuel supply and ignition timing are considered as the principal control variables of the automotive engine. The main feature of a carburetor is characterized by its Venturi geometry and fuel nozzle arrangement. The ignition timing is controlled by a distributor which is composed by a spring and a centrifugal governor.",human,0,micpro_original,human,0.6169378161430359,"Many studies on the automotive engine as a control system have been investigated in order to meet emission regulations and to improve the fuel economy. Sophisticated control systems which, up to the introduction of the microcomputer were a problem, are now easy to realize. In this paper, the fuel supply and ignition timing are considered as the principal control variables of the automotive engine. The main feature of a carburetor is characterized by its Venturi geometry and fuel nozzle arrangement. The ignition timing is controlled by a distributor which is composed by a spring and a centrifugal governor.",,human,0.6169371604919434
146,,1795,"The paper presents the design of a high-speed network simulation machine, developed at Durham University. The simulator uses configured arrays of transputers to create a multiprocessor environment for the modelling of circuit and packet switched networks, taking advantage of the unique features of the transputer for parallel implementation of such models. The design divides easily into hardware and software considerations and the paper concludes with some initial results on performance and projections for the fully populated system.",human,0,micpro_original,human,0.999749481678009,"The paper presents the design of a high-speed network simulation machine, developed at Durham University. The simulator uses configured arrays of transputers to create a multiprocessor environment for the modelling of circuit and packet switched networks, taking advantage of the unique features of the transputer for parallel implementation of such models. The design divides easily into hardware and software considerations and the paper concludes with some initial results on performance and projections for the fully populated system.",,human,0.999749481678009
147,,14335,"Floating point digital signal processing technology has become the primary method for real time signal processing in most digital systems presently. However, the challenges in the implementation of floating point arithmetic on FPGA are that, the hardware modules are larger, have longer latency and high power consumption. In this work, a novel efficient reversible floating point fused arithmetic unit architecture is proposed confirming to IEEE 754 standard. By utilizing reversible logic circuits and implementation with adiabatic logic, power efficiency is achieved. The hardware complexity is reduced by employing fused elements and latency is improved by decomposing the operands in the realization of floating point multiplier and square root. To validate the design, the proposed unit was used for realization of FFT and FIR filter which are important applications of a DSP processor. As detection is one of the core baseband processing operations in digital communication receivers and the detection speed determines the data rates that can be achieved, the proposed unit has been used to implement the detection function. Simulation results and comparative studies with existing works demonstrate that the proposed unit efficiently utilizes the number of gates, has reduced quantum cost and produced less garbage outputs with low latency, thereby making the design a computational and power efficient one.",human,0,micpro_original,human,0.9996647834777832,"Floating point digital signal processing technology has become the primary method for real time signal processing in most digital systems presently. However, the challenges in the implementation of floating point arithmetic on FPGA are that, the hardware modules are larger, have longer latency and high power consumption. In this work, a novel efficient reversible floating point fused arithmetic unit architecture is proposed confirming to IEEE 754 standard. By utilizing reversible logic circuits and implementation with adiabatic logic, power efficiency is achieved. The hardware complexity is reduced by employing fused elements and latency is improved by decomposing the operands in the realization of floating point multiplier and square root. To validate the design, the proposed unit was used for realization of FFT and FIR filter which are important applications of a DSP processor. As detection is one of the core baseband processing operations in digital communication receivers and the detection speed determines the data rates that can be achieved, the proposed unit has been used to implement the detection function. Simulation results and comparative studies with existing works demonstrate that the proposed unit efficiently utilizes the number of gates, has reduced quantum cost and produced less garbage outputs with low latency, thereby making the design a computational and power efficient one.",,human,0.9996647834777832
148,,6106,"The design of a control package for industrial use on small-scale processes is discussed. A modular system is described, made up of intelligent satellite and peripheral controllers plus various cards to provide communication with the industrial plant itself. The design is intended for operators who have little or no experience of computing. It is based on a language called paracode , an interpretive language structured to provide a number of control sequences which can run independently of each other or in parallel. Two applications of the design—a clean-in-place system and a beer-fermentation controller—are used as illustrations.",human,0,micpro_original,human,0.8047519326210022,"The design of a control package for industrial use on small-scale processes is discussed. A modular system is described, made up of intelligent satellite and peripheral controllers plus various cards to provide communication with the industrial plant itself. The design is intended for operators who have little or no experience of computing. It is based on a language called paracode , an interpretive language structured to provide a number of control sequences which can run independently of each other or in parallel. Two applications of the design—a clean-in-place system and a beer-fermentation controller—are used as illustrations.",,human,0.8047530055046082
149,,5912,"To support advanced features such as hybrid engine control, intelligent energy management, and advanced driver assistance systems, automotive embedded systems must use advanced technologies. As a result, systems are becoming distributed and include dozens of Electronic Control Units (ECU). On the one hand, this tendency raises the issue of robustness and reliability, due to the increase in the error ratio with the integration level and the clock frequency. On the other hand, due to a lack of automation, software Validation and Verification (V&amp;V) tends to swallow up 40% to 50% of the total development cost. The ``Enhanced Quality Using Intensive Test Analysis on Simulators'' (EQUITAS 1 1 This work was financially supported by Bpifrance AAP FUI16 project EQUITAS and the General Counsel of Essonne (Conseil Général de l'Essonne-France). It is supported by competitiveness clusters System@tic, iTrans and ID4CAR ) project aims (1) to improve reliability and functional safety and (2) to limit the impact of software V&amp;V on embedded systems costs and time-to-market. These two achievements are obtained by (1) developing a continuous tool-chain to automate the V&amp;V process, (2) improving the relevance of the test campaigns by detecting redundant tests using equivalence classes, (3) providing assistance for hardware failure effect analysis (FMEA) and finally (4) assessing the tool-chain under the ISO 26262 requirements.",human,0,micpro_original,human,0.985085427761078,"To support advanced features such as hybrid engine control, intelligent energy management, and advanced driver assistance systems, automotive embedded systems must use advanced technologies. As a result, systems are becoming distributed and include dozens of Electronic Control Units (ECU). On the one hand, this tendency raises the issue of robustness and reliability, due to the increase in the error ratio with the integration level and the clock frequency. On the other hand, due to a lack of automation, software Validation and Verification (V&amp;V) tends to swallow up 40% to 50% of the total development cost. The ``Enhanced Quality Using Intensive Test Analysis on Simulators'' (EQUITAS 1 1 This work was financially supported by Bpifrance AAP FUI16 project EQUITAS and the General Counsel of Essonne (Conseil Général de l'Essonne-France). It is supported by competitiveness clusters System@tic, iTrans and ID4CAR ) project aims (1) to improve reliability and functional safety and (2) to limit the impact of software V&amp;V on embedded systems costs and time-to-market. These two achievements are obtained by (1) developing a continuous tool-chain to automate the V&amp;V process, (2) improving the relevance of the test campaigns by detecting redundant tests using equivalence classes, (3) providing assistance for hardware failure effect analysis (FMEA) and finally (4) assessing the tool-chain under the ISO 26262 requirements.",,human,0.9850854277610779
150,,11369,"This paper presents an improved interconnect network for Mesh of Clusters (MoC) Field-Programmable Gate Array (FPGA) architecture. Proposed architecture has a depopulated intra-cluster interconnect with flexible Rent's parameter. It presents new multi-levels Switch Box (SB) interconnect which unifies a downward and an upward unidirectional networks based on the Butterfly-Fat-Tree (BFT) topology. To improve the routability of proposed MoC-based FPGA, long routing segments are introduced as a function of channel width with adjustable span. Compared to basic Versatile Place and Route (VPR) Mesh architecture, a saving of 32% of area and 30% of power was achieved with proposed MoC-based architecture. Based on analytical and experimental methods, we identified and explored architecture parameters that control the interconnect flexibility of the proposed MoC-based FPGA such as Rent's parameter, cluster size, Look-Up-Table (LUT) size, long wires span and percentage. Experimental results show that architecture with LUT size 4 and Cluster arity 8 is the best trade-off between power consumption and density. It can also be noted that in general long wires span equal to 4 and percentage between 20% and 30% produce most efficient results in terms of density and power.",human,0,micpro_original,human,0.9996383190155028,"This paper presents an improved interconnect network for Mesh of Clusters (MoC) Field-Programmable Gate Array (FPGA) architecture. Proposed architecture has a depopulated intra-cluster interconnect with flexible Rent's parameter. It presents new multi-levels Switch Box (SB) interconnect which unifies a downward and an upward unidirectional networks based on the Butterfly-Fat-Tree (BFT) topology. To improve the routability of proposed MoC-based FPGA, long routing segments are introduced as a function of channel width with adjustable span. Compared to basic Versatile Place and Route (VPR) Mesh architecture, a saving of 32% of area and 30% of power was achieved with proposed MoC-based architecture. Based on analytical and experimental methods, we identified and explored architecture parameters that control the interconnect flexibility of the proposed MoC-based FPGA such as Rent's parameter, cluster size, Look-Up-Table (LUT) size, long wires span and percentage. Experimental results show that architecture with LUT size 4 and Cluster arity 8 is the best trade-off between power consumption and density. It can also be noted that in general long wires span equal to 4 and percentage between 20% and 30% produce most efficient results in terms of density and power.",,human,0.9996383190155029
151,,3984,"A basic requirement in the measurement of flow rate using a target fluidic flowmeter is the use of Fourier Transform to determine the flow oscillation frequencies and from thence the flow rate. In such a flowmeter, the inlet fluid jet impinges upon a rectangular target, thereby setting up flow oscillations about the target. The oscillation frequency is directly proportional to the flow rate. A digital flow-rate display has been designed and developed to convert the signal from a fibre–film probe of a constant temperature anemometer into direct reading of flow rate. This low-cost device is able to perform as good as a commercial dynamic signal analyzer, with the additional feature of a direct read-out of flow rate.",human,0,micpro_original,human,0.9534928798675536,"A basic requirement in the measurement of flow rate using a target fluidic flowmeter is the use of Fourier Transform to determine the flow oscillation frequencies and from thence the flow rate. In such a flowmeter, the inlet fluid jet impinges upon a rectangular target, thereby setting up flow oscillations about the target. The oscillation frequency is directly proportional to the flow rate. A digital flow-rate display has been designed and developed to convert the signal from a fibre–film probe of a constant temperature anemometer into direct reading of flow rate. This low-cost device is able to perform as good as a commercial dynamic signal analyzer, with the additional feature of a direct read-out of flow rate.",,human,0.9534931778907776
152,,20879,"The features of a DMA device designed for use with 286, 186, 086 and 088 Intel-type buses are given. The SAB 82258 is also an I/O controller. The principles of data transfer in various modes are detailed, with some examples of data transfer which may be controlled or modified to suit various environments. Methods of interfacing with a CPU are outlined. This includes programming and the use of the multiplexer channel.",human,0,micpro_original,human,0.9259361028671264,"The features of a DMA device designed for use with 286, 186, 086 and 088 Intel-type buses are given. The SAB 82258 is also an I/O controller. The principles of data transfer in various modes are detailed, with some examples of data transfer which may be controlled or modified to suit various environments. Methods of interfacing with a CPU are outlined. This includes programming and the use of the multiplexer channel.",,human,0.9259359836578369
153,,10487,"In this final article the techniques for implementing the address decoding strategies described in the previous article in this series1 are considered. These techniques are divided into four groups according to the type of device used — random logic, n line to m line decoders, PROMs or programmable logic arrays. The merits of the different techniques are discussed.",human,0,micpro_original,human,0.9987112283706664,"In this final article the techniques for implementing the address decoding strategies described in the previous article in this series1 are considered. These techniques are divided into four groups according to the type of device used — random logic, n line to m line decoders, PROMs or programmable logic arrays. The merits of the different techniques are discussed.",,human,0.9987112283706665
154,,24163,"Microprocessor and digital LSI technology used for process control in heavy industrial plant is subject to interference from electromagnetic (EM) radiation in the vicinity of these sites. This paper considers the tests that are necessary to demonstrate with some level of confidence that such equipment will not malfunction in the EM environment. Susceptibility failure mechanisms are analysed with reference to the transistor junction, the basic building block of modern electronics. The effects of system coupling are considered. Conducted susceptibility test methods include power injection, voltage injection and current injection. The application of these tests is discussed together with some alternative test methods and the ramifications for large system testing. Some minimum testing requirements are presented.",human,0,micpro_original,human,0.9996683597564696,"Microprocessor and digital LSI technology used for process control in heavy industrial plant is subject to interference from electromagnetic (EM) radiation in the vicinity of these sites. This paper considers the tests that are necessary to demonstrate with some level of confidence that such equipment will not malfunction in the EM environment. Susceptibility failure mechanisms are analysed with reference to the transistor junction, the basic building block of modern electronics. The effects of system coupling are considered. Conducted susceptibility test methods include power injection, voltage injection and current injection. The application of these tests is discussed together with some alternative test methods and the ramifications for large system testing. Some minimum testing requirements are presented.",,human,0.9996683597564697
155,,15745,"This article discusses the possibilities of using FPGAs in order to construct fast PLCs that execute serial-cyclic program control loop. The PLCs bistable function blocks of the IEC 61131-3 standard with particular emphasis on the possible FPGA implementations are presented. The FPGA hardware support is implemented in such a way that it does not interfere with the normal, serial-cyclic program execution. It enables a significant reduction in the timing of the result execution after loading new input data in the memory cells. Moreover, such an implementation of bistable function blocks allows for seamless use in any programming language. Both the IL text language and the graphical languages (LD and FBD) can freely use the advantages of such a solution.",human,0,micpro_original,chatgpt,0.9998286962509156,"This article discusses the possibilities of using FPGAs in order to construct fast PLCs that execute serial-cyclic program control loop. The PLCs bistable function blocks of the IEC 61131-3 standard with particular emphasis on the possible FPGA implementations are presented. The FPGA hardware support is implemented in such a way that it does not interfere with the normal, serial-cyclic program execution. It enables a significant reduction in the timing of the result execution after loading new input data in the memory cells. Moreover, such an implementation of bistable function blocks allows for seamless use in any programming language. Both the IL text language and the graphical languages (LD and FBD) can freely use the advantages of such a solution.",,chatgpt,0.9998286962509155
156,,15441,"This paper describes the development of a cross-assembler, denoted XTIC25, which runs on Motorola's EXORMacs system. First a suitable data format called TIC25 is derived from the TMS320C25 digital signal microprocessor. This data format is then used in conjunction with software written using a combination of subsets of Pascal and MC68000 assembler routines, based on a two-pass algorithm. The cross-assembler is validated using several test programs and is capable of developing software, using the TMS320C25 assembler, within a microcomputer environment. In this way, the use of a host microcomputer for developing digital signal processing software is demonstrated.",human,0,micpro_original,chatgpt,0.9995019435882568,"This paper describes the development of a cross-assembler, denoted XTIC25, which runs on Motorola's EXORMacs system. First a suitable data format called TIC25 is derived from the TMS320C25 digital signal microprocessor. This data format is then used in conjunction with software written using a combination of subsets of Pascal and MC68000 assembler routines, based on a two-pass algorithm. The cross-assembler is validated using several test programs and is capable of developing software, using the TMS320C25 assembler, within a microcomputer environment. In this way, the use of a host microcomputer for developing digital signal processing software is demonstrated.",,chatgpt,0.9995019435882568
157,,7007,"The Fast Fourier Transform (FFT) is a digital signal processing (DSP) function most commonly used one in many applications such as imaging, wireless communication, and multimedia. The FFT processors are consists of butterfly structure operations, which includes multiplication, addition and subtraction of complex value data. In this paper, an FFT butterfly structure is designed using the Vedic multiplier for high speed applications. In this Vedic multiplier, Urdhava Triyakbhyam algorithm is utilized to improve its efficiency. Then, detector block is introduced to identify the unwanted portion of the input data to be processed in the data processing unit. Therefore, data computation time is reduced in the detector based Vedic multiplier that supports full range and half range input data. The detector is developed based on Boolean function, to detect the valid ranges of two input operands during input data computation. The detector result is used to select the operand with half range input data for Vedic multiplication and it is disabled the surplus computation. So, it reduces the switching activities in the logic gates and proportionally reduces the power consumption. The proposed design-I is consists of Vedic algorithm and the detection unit. Then, the 3-1-1-2 compressor is designed and it is utilized in the multiplier. The proposed design-II is developed with modified Vedic algorithm, detection unit and proposed 3-1-1-2 compressor. Finally, the radix-2, radix-4, and radix-8 FFT butterflies are implemented using the detection unit based Vedic multiplier, the 3-1-1-2 compressor based multiplier and various existing multiplier. The proposed design-I and proposed design-II is designed and implemented in Spartan-6, Virtex-4 and Virtex-5 FPGA family devices. The proposed reconfigurable Vedic multiplier is simulated and synthesized using Synopsys tools using the 90 nm standard cell library.",human,0,micpro_original,human,0.9997040629386902,"The Fast Fourier Transform (FFT) is a digital signal processing (DSP) function most commonly used one in many applications such as imaging, wireless communication, and multimedia. The FFT processors are consists of butterfly structure operations, which includes multiplication, addition and subtraction of complex value data. In this paper, an FFT butterfly structure is designed using the Vedic multiplier for high speed applications. In this Vedic multiplier, Urdhava Triyakbhyam algorithm is utilized to improve its efficiency. Then, detector block is introduced to identify the unwanted portion of the input data to be processed in the data processing unit. Therefore, data computation time is reduced in the detector based Vedic multiplier that supports full range and half range input data. The detector is developed based on Boolean function, to detect the valid ranges of two input operands during input data computation. The detector result is used to select the operand with half range input data for Vedic multiplication and it is disabled the surplus computation. So, it reduces the switching activities in the logic gates and proportionally reduces the power consumption. The proposed design-I is consists of Vedic algorithm and the detection unit. Then, the 3-1-1-2 compressor is designed and it is utilized in the multiplier. The proposed design-II is developed with modified Vedic algorithm, detection unit and proposed 3-1-1-2 compressor. Finally, the radix-2, radix-4, and radix-8 FFT butterflies are implemented using the detection unit based Vedic multiplier, the 3-1-1-2 compressor based multiplier and various existing multiplier. The proposed design-I and proposed design-II is designed and implemented in Spartan-6, Virtex-4 and Virtex-5 FPGA family devices. The proposed reconfigurable Vedic multiplier is simulated and synthesized using Synopsys tools using the 90 nm standard cell library.",,human,0.9997040629386902
158,,9541,"The paper considers part of a robot system for the loading of roller mounted containers with parcel post. A parallel implementation was developed of a loading algorithm which calculates the placement of the parcels. It is suitable for branch-and-bound applications and consists of identical calculation processes in a ring, with distributed control. Special attention was given to load balancing and termination. The implementation was written in OCCAM 2 and can be run on an arbitrary number of transputers. The performance increases with the number of transputers used, right up to the available limit of 36 transputers. Measurements showed an almost linear performance acceleration for up to eight processors. Dynamic load balancing was achieved with satisfactory results.",human,0,micpro_original,human,0.9997490048408508,"The paper considers part of a robot system for the loading of roller mounted containers with parcel post. A parallel implementation was developed of a loading algorithm which calculates the placement of the parcels. It is suitable for branch-and-bound applications and consists of identical calculation processes in a ring, with distributed control. Special attention was given to load balancing and termination. The implementation was written in OCCAM 2 and can be run on an arbitrary number of transputers. The performance increases with the number of transputers used, right up to the available limit of 36 transputers. Measurements showed an almost linear performance acceleration for up to eight processors. Dynamic load balancing was achieved with satisfactory results.",,human,0.9997490048408508
159,,3428,"Over the course of the last two decades, continuous advances in the stereo vision field have been documented. In this paper we present an analysis of the efficiency for the stereo vision algorithm of the Census Transform algorithm. In addition to the conventional correlation method based on Hamming distance minimization, we use two similarity measures: the Tanimoto and the Dixon-Koehler distances. Then, we compare its performance in terms of accuracy and hardware resources needed for implementation. These comparisons are performed by introducing a generalized model for each hardware architecture, scalable depending on design parameters such as Census Transform window size and maximum disparity range.",human,0,micpro_original,chatgpt,0.5256978273391724,"Over the course of the last two decades, continuous advances in the stereo vision field have been documented. In this paper we present an analysis of the efficiency for the stereo vision algorithm of the Census Transform algorithm. In addition to the conventional correlation method based on Hamming distance minimization, we use two similarity measures: the Tanimoto and the Dixon-Koehler distances. Then, we compare its performance in terms of accuracy and hardware resources needed for implementation. These comparisons are performed by introducing a generalized model for each hardware architecture, scalable depending on design parameters such as Census Transform window size and maximum disparity range.",,chatgpt,0.5256986021995544
160,,4328,"Interconnection becomes one of main concerns in current and future microprocessor designs from both performance and consumption. Three-dimensional integration technology, with its capability to shorten the wire length, is a promising method to mitigate the interconnection related issues. In this paper we implement a novel high-performance processor architecture based 3D on-chip cache to show the potential performance and power benefits achievable through 3D integration technology. We separate other logic module and cache module and stack 3D cache with the processor which reduces the global interconnection, power consumption and improves access speed. The performance of 3D processor and 3D cache at different node is simulated using 3D Cacti tools and theoretical algorithms. The results show that comparing with 2D, power consumption of the storage system is reduced by about 50%, access time and cycle time of the processor increase 18.57% and 21.41%, respectively. The reduced percentage of the critical path delay is up to 81.17%.",human,0,micpro_original,human,0.9998302459716796,"Interconnection becomes one of main concerns in current and future microprocessor designs from both performance and consumption. Three-dimensional integration technology, with its capability to shorten the wire length, is a promising method to mitigate the interconnection related issues. In this paper we implement a novel high-performance processor architecture based 3D on-chip cache to show the potential performance and power benefits achievable through 3D integration technology. We separate other logic module and cache module and stack 3D cache with the processor which reduces the global interconnection, power consumption and improves access speed. The performance of 3D processor and 3D cache at different node is simulated using 3D Cacti tools and theoretical algorithms. The results show that comparing with 2D, power consumption of the storage system is reduced by about 50%, access time and cycle time of the processor increase 18.57% and 21.41%, respectively. The reduced percentage of the critical path delay is up to 81.17%.",,human,0.9998302459716797
161,,18966,"The Cascade Vulnerability Problem is a potential problem which must be faced when using the interconnected accredited system approach of the Trusted Network Interpretation. It belongs to a subset of the problem set that addresses the issue of whether the interconnection of secure systems via a secure channel results in a secure distributed system. The Cascade Vulnerability Problem appears when an adversary can take advantage of network connections to compromise information across a range of sensitivity levels that is greater than the accreditation range of any of the component systems s/he must defeat to do so. The general Cascade Vulnerability Problem is presented, the basic properties of the most important detection algorithms are described, a brief comparative analysis is conducted, and a new approach based on simulated annealing for its correction is presented.",human,0,micpro_original,human,0.94478839635849,"The Cascade Vulnerability Problem is a potential problem which must be faced when using the interconnected accredited system approach of the Trusted Network Interpretation. It belongs to a subset of the problem set that addresses the issue of whether the interconnection of secure systems via a secure channel results in a secure distributed system. The Cascade Vulnerability Problem appears when an adversary can take advantage of network connections to compromise information across a range of sensitivity levels that is greater than the accreditation range of any of the component systems s/he must defeat to do so. The general Cascade Vulnerability Problem is presented, the basic properties of the most important detection algorithms are described, a brief comparative analysis is conducted, and a new approach based on simulated annealing for its correction is presented.",,human,0.9447885155677795
162,,12538,"The fractional Fourier transform is a time–frequency distribution and an extension of the classical Fourier transform. There are several known applications of the fractional Fourier transform in the areas of signal processing, especially in signal restoration and noise removal. This paper provides an introduction to the fractional Fourier transform and its applications. These applications demand the implementation of the discrete fractional Fourier transform on a digital signal processor (DSP). The details of the implementation of the discrete fractional Fourier transform on ADSP-2192 are provided. The effect of finite register length on implementation of discrete fractional Fourier transform matrix is discussed in some detail. This is followed by the details of the implementation and a theoretical model for the fixed-point errors involved in the implementation of this algorithm. It is hoped that this implementation and fixed-point error analysis will lead to a better understanding of the issues involved in finite register length implementation of the discrete fractional Fourier transform and will help the signal processing community make better use of the transform.",human,0,micpro_original,chatgpt,0.9997971653938292,"The fractional Fourier transform is a time–frequency distribution and an extension of the classical Fourier transform. There are several known applications of the fractional Fourier transform in the areas of signal processing, especially in signal restoration and noise removal. This paper provides an introduction to the fractional Fourier transform and its applications. These applications demand the implementation of the discrete fractional Fourier transform on a digital signal processor (DSP). The details of the implementation of the discrete fractional Fourier transform on ADSP-2192 are provided. The effect of finite register length on implementation of discrete fractional Fourier transform matrix is discussed in some detail. This is followed by the details of the implementation and a theoretical model for the fixed-point errors involved in the implementation of this algorithm. It is hoped that this implementation and fixed-point error analysis will lead to a better understanding of the issues involved in finite register length implementation of the discrete fractional Fourier transform and will help the signal processing community make better use of the transform.",,chatgpt,0.9997971653938293
163,,3812,"An online fault tolerant routing algorithm for 2D mesh Networks-on-Chip is presented in this work. It combines an adaptive routing algorithm with neighbor fault-awareness and a new traffic-balancing metric. To be able to cope with runtime permanent and temporary failures that may result in message corruption, message loss or deadlocks, the routing algorithm is enhanced with packet retransmission and a new message recovery scheme. Simulation results, for various network sizes, different traffic patterns, under an unconstrained number of node and link faults, temporary and/or permanent, demonstrate the scalability and efficiency of the proposed algorithm to tolerate multiple failures likely encountered in deep submicron technologies. As the experiments have shown, the proposed algorithm maintains high reliability of more than 97.68% for a 2D mesh network of 16 × 16 and in the presence of 384 simultaneous link faults. For the same network and in the extreme scenario of 103 routers being simultaneously faulty, the obtained reliability is more than 93.40%.",human,0,micpro_original,human,0.999710738658905,"An online fault tolerant routing algorithm for 2D mesh Networks-on-Chip is presented in this work. It combines an adaptive routing algorithm with neighbor fault-awareness and a new traffic-balancing metric. To be able to cope with runtime permanent and temporary failures that may result in message corruption, message loss or deadlocks, the routing algorithm is enhanced with packet retransmission and a new message recovery scheme. Simulation results, for various network sizes, different traffic patterns, under an unconstrained number of node and link faults, temporary and/or permanent, demonstrate the scalability and efficiency of the proposed algorithm to tolerate multiple failures likely encountered in deep submicron technologies. As the experiments have shown, the proposed algorithm maintains high reliability of more than 97.68% for a 2D mesh network of 16 × 16 and in the presence of 384 simultaneous link faults. For the same network and in the extreme scenario of 103 routers being simultaneously faulty, the obtained reliability is more than 93.40%.",,human,0.999710738658905
164,,21063,"This paper presents the design of a soft IP for JPEG compression targeted for high performance in a FPGA device. The JPEG compressor architecture achieves high throughput with a deep and optimized pipeline and with a multiplierless datapath architecture. The JPEG compressor architecture was designed in a hierarchical and modular fashion and the details of the global architecture and of its modules are presented in this paper. A modular and strictly structural VHDL design is followed to develop the JPEG compressor soft IP. The VHDL codes were synthesized to Altera and Xilinx FPGAs. Synthesis results and relevant performance comparisons with related works are presented. Our high throughput compressor is able to compress 39.8 millions of pixels per second when mapped onto an Altera FLEX 10KE FPGA. Our JPEG soft IP mapped to FLEX 10KE low cost FPGA is able to compress 115 images per second in SDTV resolution (720 × 480 pixels). Considering this SDTV resolution our design is worthy as a core of an M-JPEG video compressor, reaching a real time processing rate of 30 fps, once mapped to the FLEX 10KE FPGA device.",human,0,micpro_original,human,0.9998186230659484,"This paper presents the design of a soft IP for JPEG compression targeted for high performance in a FPGA device. The JPEG compressor architecture achieves high throughput with a deep and optimized pipeline and with a multiplierless datapath architecture. The JPEG compressor architecture was designed in a hierarchical and modular fashion and the details of the global architecture and of its modules are presented in this paper. A modular and strictly structural VHDL design is followed to develop the JPEG compressor soft IP. The VHDL codes were synthesized to Altera and Xilinx FPGAs. Synthesis results and relevant performance comparisons with related works are presented. Our high throughput compressor is able to compress 39.8 millions of pixels per second when mapped onto an Altera FLEX 10KE FPGA. Our JPEG soft IP mapped to FLEX 10KE low cost FPGA is able to compress 115 images per second in SDTV resolution (720 × 480 pixels). Considering this SDTV resolution our design is worthy as a core of an M-JPEG video compressor, reaching a real time processing rate of 30 fps, once mapped to the FLEX 10KE FPGA device.",,human,0.9998186230659485
165,,13015,"Many microprocessor emulation systems provide little or no I/O facilities for the processor being emulated. It is assumed that the user already has some target hardware which will provide these. However, this is often not the case, especially in the early stages of a project. An interface which may be used with a 68000 or a 6800 microprocessor emulator to provide a selection of I/O facilities is described. In conclusion, similar more flexible facilities that could be provided by microprocessor development systems manufacturers are indicated.",human,0,micpro_original,chatgpt,0.9942131638526917,"Many microprocessor emulation systems provide little or no I/O facilities for the processor being emulated. It is assumed that the user already has some target hardware which will provide these. However, this is often not the case, especially in the early stages of a project. An interface which may be used with a 68000 or a 6800 microprocessor emulator to provide a selection of I/O facilities is described. In conclusion, similar more flexible facilities that could be provided by microprocessor development systems manufacturers are indicated.",,chatgpt,0.9942131638526917
166,,8472,The concept of the automated office has now become a possibility with the advent of low cost computing power. The authors discuss the concept in some detail and review the equipment currently available together with some of its limitations. Future developmental trends are described and the ultimate implications of the fully-automated office are discussed.,human,0,micpro_original,human,0.9969846606254578,The concept of the automated office has now become a possibility with the advent of low cost computing power. The authors discuss the concept in some detail and review the equipment currently available together with some of its limitations. Future developmental trends are described and the ultimate implications of the fully-automated office are discussed.,,human,0.9969846606254578
167,,4704,"The 8-bit microprocessor has not been rendered obsolete by newer and more powerful 16- and 32-bit microprocessors. Many applications are more than adequately served by 8-bit microprocessors, with their lower-cost address and data buses. However, there is a ‘grey area’ in which 8-bit chips are not sufficient but 16-bit chips represent an uneconomic ‘overkill’. A significant limitation of 8-bit microprocessors with 16-bit address buses is their inability to access more than 64 kbyte of data. This application shows how the SN54/74LS610-3 series of memory mapping units enables an 8-bit microprocessor to access memories much larger than 64 kbyte without significantly increasing the chip count of the system. Note that a memory mapping unit cannot simply be engineered into a microprocessor system without adding the appropriate software, as the 8-bit microprocessor can still address only 64 kbyte of logical address space at any instant. Software is usually included in the operating system to map 64 kbyte of the physical memory onto the logical address space.",human,0,micpro_original,human,0.7300511598587036,"The 8-bit microprocessor has not been rendered obsolete by newer and more powerful 16- and 32-bit microprocessors. Many applications are more than adequately served by 8-bit microprocessors, with their lower-cost address and data buses. However, there is a ‘grey area’ in which 8-bit chips are not sufficient but 16-bit chips represent an uneconomic ‘overkill’. A significant limitation of 8-bit microprocessors with 16-bit address buses is their inability to access more than 64 kbyte of data. This application shows how the SN54/74LS610-3 series of memory mapping units enables an 8-bit microprocessor to access memories much larger than 64 kbyte without significantly increasing the chip count of the system. Note that a memory mapping unit cannot simply be engineered into a microprocessor system without adding the appropriate software, as the 8-bit microprocessor can still address only 64 kbyte of logical address space at any instant. Software is usually included in the operating system to map 64 kbyte of the physical memory onto the logical address space.",,human,0.7300496101379395
168,,15542,"In this paper, the hardware implementations of six representative stream ciphers are compared in terms of performance, consumed area and the throughput-to-area ratio. The stream ciphers used for the comparison are ZUC, Snow3g, Grain V1, Mickey V2, Trivium and E0. ZUC, Snow3g and E0 have been used for the security part of well known standards, especially wireless communication protocols. In addition, Grain V1, Mickey V2 and Trivium are currently selected as the final portfolio of stream ciphers for Profile 2 (Hardware) by the eStream project. The designs were implemented by using VHDL language and for the hardware implementations a FPGA device was used. The highest throughput has been achieved by Snow3g with 3330 Mbps at 104 MHz and the lowest throughput has been achieved by E0 with 187 Mbps at 187 MHz. Also, the most efficient cipher for hardware implementation in terms of throughput-to-area ratio is Mickey V2 cipher while the worst cipher for hardware implementation is Grain V1.",human,0,micpro_original,human,0.999776303768158,"In this paper, the hardware implementations of six representative stream ciphers are compared in terms of performance, consumed area and the throughput-to-area ratio. The stream ciphers used for the comparison are ZUC, Snow3g, Grain V1, Mickey V2, Trivium and E0. ZUC, Snow3g and E0 have been used for the security part of well known standards, especially wireless communication protocols. In addition, Grain V1, Mickey V2 and Trivium are currently selected as the final portfolio of stream ciphers for Profile 2 (Hardware) by the eStream project. The designs were implemented by using VHDL language and for the hardware implementations a FPGA device was used. The highest throughput has been achieved by Snow3g with 3330 Mbps at 104 MHz and the lowest throughput has been achieved by E0 with 187 Mbps at 187 MHz. Also, the most efficient cipher for hardware implementation in terms of throughput-to-area ratio is Mickey V2 cipher while the worst cipher for hardware implementation is Grain V1.",,human,0.999776303768158
169,,19576,"Despite that it has been recognized that decimal arithmetic is more suitable than binary arithmetic for human-centric applications, binary arithmetic is still predominant in today’s computers. One approach to bridging this gap involves converting the decimal operands to binary, performing arithmetic in binary, and converting the result back to decimal. Based on this approach, this paper presents novel high-performance decimal-to-binary conversion circuits to support decimal arithmetic over different FPGAs families. Our circuits are based on a simple, yet effective idea. Bits of the BCD inputs are grouped into a number of groups. The contribution of each group to the overall binary result is computed separately. Then these contributions are added to form the final binary result. The performance evaluation presented in this paper indicates that the proposed circuits perform significantly better than existing BCD-to-binary conversion circuits. Furthermore, for a given FPGA family, the comparison reveals that certain bit-grouping may perform better than others. In addition, we have studied the growth in area and time for each bit-grouping scheme with respect to the number of digits in the BCD input.",human,0,micpro_original,chatgpt,0.9996851682662964,"Despite that it has been recognized that decimal arithmetic is more suitable than binary arithmetic for human-centric applications, binary arithmetic is still predominant in today’s computers. One approach to bridging this gap involves converting the decimal operands to binary, performing arithmetic in binary, and converting the result back to decimal. Based on this approach, this paper presents novel high-performance decimal-to-binary conversion circuits to support decimal arithmetic over different FPGAs families. Our circuits are based on a simple, yet effective idea. Bits of the BCD inputs are grouped into a number of groups. The contribution of each group to the overall binary result is computed separately. Then these contributions are added to form the final binary result. The performance evaluation presented in this paper indicates that the proposed circuits perform significantly better than existing BCD-to-binary conversion circuits. Furthermore, for a given FPGA family, the comparison reveals that certain bit-grouping may perform better than others. In addition, we have studied the growth in area and time for each bit-grouping scheme with respect to the number of digits in the BCD input.",,chatgpt,0.9996851682662964
170,,2018,"In this work, we extend our previous manuscript regarding a systematic study of data remanence effects on an intrinsic Static Random Access Memory Physical Unclonable Function (SRAM PUF) implemented on a Commercial Off-The-Shelf (COTS) device in the temperature range between [formula omitted]C and [formula omitted]C. As the experimental results of our previous work show, an attack against intrinsic SRAM PUFs, which takes advantage of data remanence effects exhibited due to low temperatures, is possible, resulting in the attacker being able to know the PUF response, with high probability. As demonstrated in our previous work, this attack is highly resistant to memory erasure techniques and can be used to manipulate the cryptographic keys produced by the SRAM PUF. In this work, we examine and discuss potential countermeasures against this attack in more detail, and investigate whether this attack can be performed using an experimental setup that does not guarantee a high degree of thermal isolation. Additionally, we also examine and discuss whether very low temperatures can be used to perform another relevant type of attack against SRAM PUFs, based on whether very low temperature can prevent the SRAM from being overwritten. Finally, we also discuss related works and the generalisation of our results in more detail.",human,0,micpro_original,human,0.9979903697967528,"In this work, we extend our previous manuscript regarding a systematic study of data remanence effects on an intrinsic Static Random Access Memory Physical Unclonable Function (SRAM PUF) implemented on a Commercial Off-The-Shelf (COTS) device in the temperature range between [formula omitted]C and [formula omitted]C. As the experimental results of our previous work show, an attack against intrinsic SRAM PUFs, which takes advantage of data remanence effects exhibited due to low temperatures, is possible, resulting in the attacker being able to know the PUF response, with high probability. As demonstrated in our previous work, this attack is highly resistant to memory erasure techniques and can be used to manipulate the cryptographic keys produced by the SRAM PUF. In this work, we examine and discuss potential countermeasures against this attack in more detail, and investigate whether this attack can be performed using an experimental setup that does not guarantee a high degree of thermal isolation. Additionally, we also examine and discuss whether very low temperatures can be used to perform another relevant type of attack against SRAM PUFs, based on whether very low temperature can prevent the SRAM from being overwritten. Finally, we also discuss related works and the generalisation of our results in more detail.",,human,0.9979904890060425
171,,10752,"The reducing of the width of quantum reversible circuits makes multiple-valued reversible logic a very promising research area. Ternary logic is one of the most popular types of multiple-valued reversible logic, along with the Subtractor, which is among the major components of the ALU of a classical computer and complex hardware. In this paper the authors will be presenting an improved design of a ternary reversible half subtractor circuit. The authors shall compare the improved design with the existing designs and shall highlight the improvements made after which the authors will propose a new ternary reversible full subtractor circuit. Ternary Shift gates and ternary Muthukrishnan–Stroud gates were used to build such newly designed complex circuits and it is believed that the proposed designs can be used in ternary quantum computers. The minimization of the number of constant inputs and garbage outputs, hardware complexity, quantum cost and delay time is an important issue in reversible logic design. In this study a significant improvement as compared to the existing designs has been achieved in as such that with the reduction in the number of ternary shift and Muthukrishnan-Stroud gates used the authors have produced ternary subtractor circuits.",human,0,micpro_original,human,0.9998236298561096,"The reducing of the width of quantum reversible circuits makes multiple-valued reversible logic a very promising research area. Ternary logic is one of the most popular types of multiple-valued reversible logic, along with the Subtractor, which is among the major components of the ALU of a classical computer and complex hardware. In this paper the authors will be presenting an improved design of a ternary reversible half subtractor circuit. The authors shall compare the improved design with the existing designs and shall highlight the improvements made after which the authors will propose a new ternary reversible full subtractor circuit. Ternary Shift gates and ternary Muthukrishnan–Stroud gates were used to build such newly designed complex circuits and it is believed that the proposed designs can be used in ternary quantum computers. The minimization of the number of constant inputs and garbage outputs, hardware complexity, quantum cost and delay time is an important issue in reversible logic design. In this study a significant improvement as compared to the existing designs has been achieved in as such that with the reduction in the number of ternary shift and Muthukrishnan-Stroud gates used the authors have produced ternary subtractor circuits.",,human,0.9998236298561096
172,,20559,"Reverse engineering is a great peril for hardware security especially when functional behavior extraction is required. In this paper a new automated mechanism is proposed to encrypt routing topology of the design which leads to hinder reverse engineering during the foundry/fabrication process. Moreover, new special standard cells (Wire Scrambling cells) are proposed corresponding with an automatic design flow to insert the WS-cells inside the netlist with the aim of maximum effectiveness of obfuscation and minimum overhead. The highlight feature of this mechanism is that it can be performed without detailed information about the functionality and structure of the design and hence, it can be automated easily. This methodology is implemented using an academic physical design framework (EduCAD). Experimental results show that reverse engineering can be hindered considerably in cost of negligible overheads in area, power consumption and total wire length.",human,0,micpro_original,human,0.999822437763214,"Reverse engineering is a great peril for hardware security especially when functional behavior extraction is required. In this paper a new automated mechanism is proposed to encrypt routing topology of the design which leads to hinder reverse engineering during the foundry/fabrication process. Moreover, new special standard cells (Wire Scrambling cells) are proposed corresponding with an automatic design flow to insert the WS-cells inside the netlist with the aim of maximum effectiveness of obfuscation and minimum overhead. The highlight feature of this mechanism is that it can be performed without detailed information about the functionality and structure of the design and hence, it can be automated easily. This methodology is implemented using an academic physical design framework (EduCAD). Experimental results show that reverse engineering can be hindered considerably in cost of negligible overheads in area, power consumption and total wire length.",,human,0.9998224377632141
173,,24516,"Our aim is to investigate the suitability of hardware multithreading for real-time event handling in combination with appropriate real-time scheduling techniques. We designed and evaluated a multithreaded microcontroller based on a Java processor core. Java threads are used as Interrupt Service Threads (ISTs) instead of the Interrupt Service Routines (ISRs) of conventional processors. Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context switching overhead. A so-called priority manager implements several real-time scheduling algorithms in hardware. We show the feasibility of a hardware real-time scheduler integrated deeply into the processor pipeline with a VHDL design and its synthesis. Evaluations with a software simulator and real-time applications as benchmarks show that hardware multithreading reaches a 1.2–1.4 performance increase for hard real-time applications (multithreading without latency utilization) and a 2.0–2.6 speedup by latency utilization for programs without hard real-time requirements. With respect to real-time scheduling on a multithreaded microcontroller, the Least Laxity First (LLF) scheme outperforms the Fixed Priority Preemptive (FPP), Earliest Deadline First (EDF), and Guaranteed Percentage (GP) schemes, but suffers from the highest implementation costs.",human,0,micpro_original,human,0.9998273253440856,"Our aim is to investigate the suitability of hardware multithreading for real-time event handling in combination with appropriate real-time scheduling techniques. We designed and evaluated a multithreaded microcontroller based on a Java processor core. Java threads are used as Interrupt Service Threads (ISTs) instead of the Interrupt Service Routines (ISRs) of conventional processors. Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context switching overhead. A so-called priority manager implements several real-time scheduling algorithms in hardware. We show the feasibility of a hardware real-time scheduler integrated deeply into the processor pipeline with a VHDL design and its synthesis. Evaluations with a software simulator and real-time applications as benchmarks show that hardware multithreading reaches a 1.2–1.4 performance increase for hard real-time applications (multithreading without latency utilization) and a 2.0–2.6 speedup by latency utilization for programs without hard real-time requirements. With respect to real-time scheduling on a multithreaded microcontroller, the Least Laxity First (LLF) scheme outperforms the Fixed Priority Preemptive (FPP), Earliest Deadline First (EDF), and Guaranteed Percentage (GP) schemes, but suffers from the highest implementation costs.",,human,0.9998273253440857
174,,2380,"It is widely held that developing critical thinking is one of the goals of science education. Although there is much valuable work in the area, the field lacks a coherent and defensible conception of critical thinking. As a result, many efforts to foster critical thinking in science rest on misconceptions about the nature of critical thinking. This paper examines some of the misconceptions, in particular the characterization of critical thinking in terms of processes or skills and the separation of critical thinking and knowledge. It offers a more philosophically sound and justifiable conception of critical thinking, and demonstrates how this conception could be used to ground science education practice. ",human,0,sdg_abstracts_original,human,0.9993245601654052,"It is widely held that developing critical thinking is one of the goals of science education. Although there is much valuable work in the area, the field lacks a coherent and defensible conception of critical thinking. As a result, many efforts to foster critical thinking in science rest on misconceptions about the nature of critical thinking. This paper examines some of the misconceptions, in particular the characterization of critical thinking in terms of processes or skills and the separation of critical thinking and knowledge. It offers a more philosophically sound and justifiable conception of critical thinking, and demonstrates how this conception could be used to ground science education practice. ",,human,0.9993245601654053
175,,216,"A 70-year-old man referred for treatment of a left lower lung tumor was shown in chest computed tomography to have a homogeneous round tumor 45 mm in diameter with an enhanced thin wall in the lower lobe of the left lung. No specific finding was seen in material obtained by transbronchial and computed tomography (CT)-guided lung tissue biopsy, so the presumptive diagnosis was a lung abscess. Despite antibiotics administered for 2 weeks, radiography showed the tumor had grown, necessitating left lower lobectomy. The permanent section was diagnosed as diffuse large B-cell lymphoma. Because CT findings for the tumor suggested a lung abscess and the central part of the tumor consisted of fibrotic and necrotic tissue, we had difficulty establishing a final diagnosis. The literature showed primary pulmonary lymphomas yielded a variety of findings radiographically, making surgery paramount for ascertaining a final diagnosis.",human,0,sdg_abstracts_original,human,0.9998316764831544,"A 70-year-old man referred for treatment of a left lower lung tumor was shown in chest computed tomography to have a homogeneous round tumor 45 mm in diameter with an enhanced thin wall in the lower lobe of the left lung. No specific finding was seen in material obtained by transbronchial and computed tomography (CT)-guided lung tissue biopsy, so the presumptive diagnosis was a lung abscess. Despite antibiotics administered for 2 weeks, radiography showed the tumor had grown, necessitating left lower lobectomy. The permanent section was diagnosed as diffuse large B-cell lymphoma. Because CT findings for the tumor suggested a lung abscess and the central part of the tumor consisted of fibrotic and necrotic tissue, we had difficulty establishing a final diagnosis. The literature showed primary pulmonary lymphomas yielded a variety of findings radiographically, making surgery paramount for ascertaining a final diagnosis.",,human,0.9998316764831543
176,,20846,"Objective: To summarize the clinical development of dabrafenib and to highlight the clinically relevant distinct characteristics of dabrafenib in contrast to vemurafenib. Data Source: An English-language literature search of MEDLINE/PubMed (1966-June 2013), using the keywords GSK2118436, dabrafenib, vemurafenib, selective BRAF inhibitor, and advanced melanoma, was conducted. Data were also obtained from package inserts, meeting abstracts, and clinical registries. Study Selection and Data Extraction: All relevant published articles on dabrafenib and vemurafenib were reviewed. Clinical trial registries and meeting abstracts were used for information about ongoing studies. Data Synthesis: BRAFV600E mutation confers constitutive BRAK kinase activation in melanoma cells, promoting tumor growth. This discovery led to the development of BRAF kinase inhibitors like vemurafenib and dabrafenib. Dabrafenib has been approved to treat patients with BRAFV600E-positive unresectable or metastatic melanoma based on its clinical benefit demonstrated in a randomized phase III study. It has also been shown to be safe and effective in patients with BRAF mutant advanced melanoma involving the brain. Dabrafenib is well tolerated, with the most common adverse effects being hyperkeratosis, headache, pyrexia, and arthralgia. Currently, there is no evidence to suggest that one BRAF inhibitor is superior to the other. With similar efficacy, therapy selection will likely be influenced by differential tolerability and cost. Conclusions: Dabrafenib joins vemurafenib to confirm the superior clinical outcome of the BRAF inhibitors when compared with dacarbazine in patients with BRAFV600E-positive advanced melanoma. Active research is ongoing to expand its utility into the adjuvant setting and to circumvent rapid emergence of drug resistance. ",human,0,sdg_abstracts_original,human,0.9998268485069276,"Objective: To summarize the clinical development of dabrafenib and to highlight the clinically relevant distinct characteristics of dabrafenib in contrast to vemurafenib. Data Source: An English-language literature search of MEDLINE/PubMed (1966-June 2013), using the keywords GSK2118436, dabrafenib, vemurafenib, selective BRAF inhibitor, and advanced melanoma, was conducted. Data were also obtained from package inserts, meeting abstracts, and clinical registries. Study Selection and Data Extraction: All relevant published articles on dabrafenib and vemurafenib were reviewed. Clinical trial registries and meeting abstracts were used for information about ongoing studies. Data Synthesis: BRAFV600E mutation confers constitutive BRAK kinase activation in melanoma cells, promoting tumor growth. This discovery led to the development of BRAF kinase inhibitors like vemurafenib and dabrafenib. Dabrafenib has been approved to treat patients with BRAFV600E-positive unresectable or metastatic melanoma based on its clinical benefit demonstrated in a randomized phase III study. It has also been shown to be safe and effective in patients with BRAF mutant advanced melanoma involving the brain. Dabrafenib is well tolerated, with the most common adverse effects being hyperkeratosis, headache, pyrexia, and arthralgia. Currently, there is no evidence to suggest that one BRAF inhibitor is superior to the other. With similar efficacy, therapy selection will likely be influenced by differential tolerability and cost. Conclusions: Dabrafenib joins vemurafenib to confirm the superior clinical outcome of the BRAF inhibitors when compared with dacarbazine in patients with BRAFV600E-positive advanced melanoma. Active research is ongoing to expand its utility into the adjuvant setting and to circumvent rapid emergence of drug resistance. ",,human,0.9998268485069275
177,,20310,"Intercellular junctions and particle arrays in the developing and mature dorsal ocelli of the honeybee Apis mellifera have been studied with conventional and freeze-fracture electron microscopy. Four types of junctions are found in the lentigenic and retinogenic part during development. These are desmosomes, septate junctions, tight junctions, and gap junctions. Gap junctions and septate junctions are found between differentiating photoreceptor cells only as long as the rhabdoms are beginning to form. Their disappearance after differentiation indicates that they could play a part in cell determination. Desmosomes connect photoreceptor cells into the early imaginai stage and then disappear. Other junctions, once they have formed, remain for the life of the animal, but can change considerably in structure, distribution and frequency. The cells of the perineurium surrounding the ocellus are connected by septate and gap junctions, which may be the basis of the blood-eye barrier. Rhombic particle arrays on the E-face of the glial membrane attached to the photoreceptor cell membrane first appear in small groups one day before emergence. In the further course of life these arrays become more extensive and apparent. Their significance may be to play some role in receptor function. ",human,0,sdg_abstracts_original,human,0.9991205334663392,"Intercellular junctions and particle arrays in the developing and mature dorsal ocelli of the honeybee Apis mellifera have been studied with conventional and freeze-fracture electron microscopy. Four types of junctions are found in the lentigenic and retinogenic part during development. These are desmosomes, septate junctions, tight junctions, and gap junctions. Gap junctions and septate junctions are found between differentiating photoreceptor cells only as long as the rhabdoms are beginning to form. Their disappearance after differentiation indicates that they could play a part in cell determination. Desmosomes connect photoreceptor cells into the early imaginai stage and then disappear. Other junctions, once they have formed, remain for the life of the animal, but can change considerably in structure, distribution and frequency. The cells of the perineurium surrounding the ocellus are connected by septate and gap junctions, which may be the basis of the blood-eye barrier. Rhombic particle arrays on the E-face of the glial membrane attached to the photoreceptor cell membrane first appear in small groups one day before emergence. In the further course of life these arrays become more extensive and apparent. Their significance may be to play some role in receptor function. ",,human,0.9991205334663391
178,,1752,"Ecological restoration often depends on substantial funding to be initiated and sustained. So far, the dominant strategies to fund it have been philanthropic altruism or regulatory compulsion. Both make important contributions, but the commercial sector is a further source that has thus far not played a significant role relative to its potential. Just as philanthropic and regulatory strategies are both modulated by the law, so too law shapes the commercial sector's participation in ecological restoration. Corporate law and its market context are particularly significant, and they are both a potential hindrance and opportunity. Some options for law reform are available to improve the contribution of the business sector to restoration.",human,0,sdg_abstracts_original,human,0.9997108578681946,"Ecological restoration often depends on substantial funding to be initiated and sustained. So far, the dominant strategies to fund it have been philanthropic altruism or regulatory compulsion. Both make important contributions, but the commercial sector is a further source that has thus far not played a significant role relative to its potential. Just as philanthropic and regulatory strategies are both modulated by the law, so too law shapes the commercial sector's participation in ecological restoration. Corporate law and its market context are particularly significant, and they are both a potential hindrance and opportunity. Some options for law reform are available to improve the contribution of the business sector to restoration.",,human,0.9997108578681946
179,,1467,"Background: Evidence suggests possible synergetic effects of multiple lifestyle behaviors on health risks like obesity and other health outcomes. A better insight in the clustering of those behaviors, could help to identify groups who are at risk in developing chronic diseases. This study examines the prevalence and clustering of physical activity, sedentary and dietary patterns among European adolescents and investigates if the identified clusters could be characterized by socio-demographic factors. Methods. The study comprised a total of 2084 adolescents (45.6% male), from eight European cities participating in the HELENA (Healthy Lifestyle in Europe by Nutrition in Adolescence) study. Physical activity and sedentary behavior were measured using self-reported questionnaires and diet quality was assessed based on dietary recall. Based on the results of those three indices, cluster analyses were performed. To identify gender differences and associations with socio-demographic variables, chi-square tests were executed. Results: Five stable and meaningful clusters were found. Only 18% of the adolescents showed healthy and 21% unhealthy scores on all three included indices. Males were highly presented in the cluster with high levels of moderate to vigorous physical activity (MVPA) and low quality diets. The clusters with low levels of MVPA and high quality diets comprised more female adolescents. Adolescents with low educated parents had diets of lower quality and spent more time in sedentary activities. In addition, the clusters with high levels of MVPA comprised more adolescents of the younger age category. Conclusion: In order to develop effective primary prevention strategies, it would be important to consider multiple health indices when identifying high risk groups. ",human,0,sdg_abstracts_original,human,0.999618411064148,"Background: Evidence suggests possible synergetic effects of multiple lifestyle behaviors on health risks like obesity and other health outcomes. A better insight in the clustering of those behaviors, could help to identify groups who are at risk in developing chronic diseases. This study examines the prevalence and clustering of physical activity, sedentary and dietary patterns among European adolescents and investigates if the identified clusters could be characterized by socio-demographic factors. Methods. The study comprised a total of 2084 adolescents (45.6% male), from eight European cities participating in the HELENA (Healthy Lifestyle in Europe by Nutrition in Adolescence) study. Physical activity and sedentary behavior were measured using self-reported questionnaires and diet quality was assessed based on dietary recall. Based on the results of those three indices, cluster analyses were performed. To identify gender differences and associations with socio-demographic variables, chi-square tests were executed. Results: Five stable and meaningful clusters were found. Only 18% of the adolescents showed healthy and 21% unhealthy scores on all three included indices. Males were highly presented in the cluster with high levels of moderate to vigorous physical activity (MVPA) and low quality diets. The clusters with low levels of MVPA and high quality diets comprised more female adolescents. Adolescents with low educated parents had diets of lower quality and spent more time in sedentary activities. In addition, the clusters with high levels of MVPA comprised more adolescents of the younger age category. Conclusion: In order to develop effective primary prevention strategies, it would be important to consider multiple health indices when identifying high risk groups. ",,human,0.999618411064148
180,,24678,"The design of a PWM zero-voltage-switching bidirectional converter is presented. The bidirectional converter employs a buck and boost topology using FET devices for both switches. Zero-voltage-switching is obtained by selecting the power stage inductance so that the switches always turn on when the MOSFET body diode is conducting. A four module, multi-phase topology is employed to significantly reduce the input and output ripple current. Besides obtaining high efficiency and light weight, the converter is also shown to possess excellent small-signal characteristics. It is applied to the design of the battery charger / discharger for the NASA EOS satellite. Both theoretical and experimental results are presented.",human,0,sdg_abstracts_original,human,0.9998306035995485,"The design of a PWM zero-voltage-switching bidirectional converter is presented. The bidirectional converter employs a buck and boost topology using FET devices for both switches. Zero-voltage-switching is obtained by selecting the power stage inductance so that the switches always turn on when the MOSFET body diode is conducting. A four module, multi-phase topology is employed to significantly reduce the input and output ripple current. Besides obtaining high efficiency and light weight, the converter is also shown to possess excellent small-signal characteristics. It is applied to the design of the battery charger / discharger for the NASA EOS satellite. Both theoretical and experimental results are presented.",,human,0.9998306035995483
181,,1306,"The significance of simulation with virtual prototyping is emphasized, and the MSS (Mechanical System Simulation) technology and ADAMS platform are introduced. Virtual prototyping is first adopted into fault simulation and mission reliability fields. On the analysis of the topology and joint relationship of a type of artillery, its virtual prototyping is established in ADAMS environment. A simulation example is illustrated to show the influencing rules of different fault factors on system performance. The deficiencies of tradition fuzzy evaluation method are pointed out and the concept of dynamic membership degree is put forward. The membership function can be confirmed according to the simulation results of virtual prototyping. Based on the evaluation results, commander can determine whether the equipments are capable for the training or battle missions or not.",human,0,sdg_abstracts_original,human,0.9998252987861632,"The significance of simulation with virtual prototyping is emphasized, and the MSS (Mechanical System Simulation) technology and ADAMS platform are introduced. Virtual prototyping is first adopted into fault simulation and mission reliability fields. On the analysis of the topology and joint relationship of a type of artillery, its virtual prototyping is established in ADAMS environment. A simulation example is illustrated to show the influencing rules of different fault factors on system performance. The deficiencies of tradition fuzzy evaluation method are pointed out and the concept of dynamic membership degree is put forward. The membership function can be confirmed according to the simulation results of virtual prototyping. Based on the evaluation results, commander can determine whether the equipments are capable for the training or battle missions or not.",,human,0.9998252987861633
182,,14480,"Building-integrated micro-wind turbines are promising low-cost renewable energy devices. However, the take-up of micro-wind turbines in high density suburban environments is still very limited due to issues such as: a) low wind speeds; b) high turbulence intensity; and c) the perception of potentially high levels of aerodynamic noise generated by the turbines. The wind flow field above the roof of buildings in this environment is different to that over flat terrain or around isolated buildings. The effect of the local suburban topology on the wind speed and turbulence intensity fields in a given locality is therefore an important determinant of the optimal location of micro-wind turbines. This paper presents a numerical study of above roof wind flow characteristics in three suburban landscapes characterized by houses with different roof profiles, namely: pitched roofs, pyramidal roofs and flat roofs. Computational Fluid Dynamic (CFD) technique has been used to simulate the wind flow in such environments and to find the optimum turbine mounting locations. Results show how the wind flow characteristics are strongly dependent on the profile of the roofs. It is found that turbines mounted on flat roofs are likely to yield higher and more consistent power for the same turbine hub elevation than the other roof profiles. ",human,0,sdg_abstracts_original,human,0.9997833371162416,"Building-integrated micro-wind turbines are promising low-cost renewable energy devices. However, the take-up of micro-wind turbines in high density suburban environments is still very limited due to issues such as: a) low wind speeds; b) high turbulence intensity; and c) the perception of potentially high levels of aerodynamic noise generated by the turbines. The wind flow field above the roof of buildings in this environment is different to that over flat terrain or around isolated buildings. The effect of the local suburban topology on the wind speed and turbulence intensity fields in a given locality is therefore an important determinant of the optimal location of micro-wind turbines. This paper presents a numerical study of above roof wind flow characteristics in three suburban landscapes characterized by houses with different roof profiles, namely: pitched roofs, pyramidal roofs and flat roofs. Computational Fluid Dynamic (CFD) technique has been used to simulate the wind flow in such environments and to find the optimum turbine mounting locations. Results show how the wind flow characteristics are strongly dependent on the profile of the roofs. It is found that turbines mounted on flat roofs are likely to yield higher and more consistent power for the same turbine hub elevation than the other roof profiles. ",,human,0.9997833371162415
183,,3443,"Valsartan, a type 1 selective receptor for angiotensin, is primarily used in the treatment of hypertension and, in a minority, to treat coronary artery disease and heart failure. Among its adverse effects, it can be mentioned malaise, dizziness and headache. Furthermore, in the conventional form of release, it may cause patient noncompliance with treatment and need for multiple dosages, which lead to the possibility of reduction in therapeutic efficiency. To avoid these disadvantages, the present work aimed at the production of valsartan particles for delayed and extended release of the drug. The composition of the particles was based on the polymer blend of sericin and alginate. Sericin, present in the cocoons of the Bombyx mori silkworm, is a water soluble globular protein discharged into effluent from the silk industry. Its chemical structure allows it to be used for the formation of blends with other polymers, aiming at the improvement of its properties. Alginate, which was commercially obtained, is a polysaccharide obtained from brown algae, widely used in pharmaceutical applications. In order to evaluate the incorporation of valsartan into the aforementioned polymer blend, formulations with alginate only and formulations with different amounts of alginate initially added were developed. The evaluation was accomplished by means of incorporation efficiency and in vitro dissolution tests in simulated gastrointestinal environment. Sericin was shown to increase both the drug incorporation efficiency and the release time in enteric media. The best result was obtained for the formulation composed of the sericin and alginate blend, with the lowest amount of alginate. An 82.75 ± 2.61 % incorporation efficiency and extended release (about 28 hours) was achieved.",human,0,sdg_abstracts_original,human,0.9997997879981996,"Valsartan, a type 1 selective receptor for angiotensin, is primarily used in the treatment of hypertension and, in a minority, to treat coronary artery disease and heart failure. Among its adverse effects, it can be mentioned malaise, dizziness and headache. Furthermore, in the conventional form of release, it may cause patient noncompliance with treatment and need for multiple dosages, which lead to the possibility of reduction in therapeutic efficiency. To avoid these disadvantages, the present work aimed at the production of valsartan particles for delayed and extended release of the drug. The composition of the particles was based on the polymer blend of sericin and alginate. Sericin, present in the cocoons of the Bombyx mori silkworm, is a water soluble globular protein discharged into effluent from the silk industry. Its chemical structure allows it to be used for the formation of blends with other polymers, aiming at the improvement of its properties. Alginate, which was commercially obtained, is a polysaccharide obtained from brown algae, widely used in pharmaceutical applications. In order to evaluate the incorporation of valsartan into the aforementioned polymer blend, formulations with alginate only and formulations with different amounts of alginate initially added were developed. The evaluation was accomplished by means of incorporation efficiency and in vitro dissolution tests in simulated gastrointestinal environment. Sericin was shown to increase both the drug incorporation efficiency and the release time in enteric media. The best result was obtained for the formulation composed of the sericin and alginate blend, with the lowest amount of alginate. An 82.75 ± 2.61 % incorporation efficiency and extended release (about 28 hours) was achieved.",,human,0.9997997879981995
184,,2512,"Objective: In patients with advanced kidney disease, metabolic and nutritional derangements induced by uremia interact and reinforce each other in a deleterious vicious circle. Literature addressing the effect of dialysis initiation on changes in body composition (BC) is limited and contradictory. The aim of this study was to evaluate changes in BC in a large international cohort of incident hemodialysis patients. Methods: A total of 8,227 incident adult end-stage renal disease patients with BC evaluation within the initial first 6 months of baseline, defined as 6 months after renal replacement therapy initiation, were considered. BC, including fat tissue index (FTI) and lean tissue index (LTI), were evaluated by Body Composition Monitor (BCM, Fresenius Medical Care, Bad Homburg, Germany). Exclusion criteria at baseline were lack of a BCM measurement before or after baseline, body mass index (BMI) < 18.5 kg/m2, presence of metastatic solid tumors, treatment with a catheter, and prescription of less or more than 3 treatments per week. Maximum follow-up was 2 years. Descriptive analysis was performed comparing current values with the baseline in each interval (delta analysis). Linear mixed models considering the correlation structure of the repeated measurements were used to evaluate factors associated with different trends in FTI and LTI. Results: BMI increased about 0.6 kg/m2 over 24 months from baseline. This was associated with increase in FTI of about 0.95 kg/m2 and a decrease in LTI of about 0.4 kg/m2. Female gender, diabetic status, and low baseline FTI were associated with a significant greater increase of FTI. Age > 67 years, diabetes, male gender, high baseline LTI, and low baseline FTI were associated with a significant greater decrease of LTI. Conclusions: With the transition to hemodialysis, end-stage renal disease patients presented with distinctive changes in BC. These were mainly associated with gender, older age, presence of diabetes, low baseline FTI, and high baseline LTI. BMI increases did not fully represent the changes in BC.",human,0,sdg_abstracts_original,human,0.9312383532524108,"Objective: In patients with advanced kidney disease, metabolic and nutritional derangements induced by uremia interact and reinforce each other in a deleterious vicious circle. Literature addressing the effect of dialysis initiation on changes in body composition (BC) is limited and contradictory. The aim of this study was to evaluate changes in BC in a large international cohort of incident hemodialysis patients. Methods: A total of 8,227 incident adult end-stage renal disease patients with BC evaluation within the initial first 6 months of baseline, defined as 6 months after renal replacement therapy initiation, were considered. BC, including fat tissue index (FTI) and lean tissue index (LTI), were evaluated by Body Composition Monitor (BCM, Fresenius Medical Care, Bad Homburg, Germany). Exclusion criteria at baseline were lack of a BCM measurement before or after baseline, body mass index (BMI) < 18.5 kg/m2, presence of metastatic solid tumors, treatment with a catheter, and prescription of less or more than 3 treatments per week. Maximum follow-up was 2 years. Descriptive analysis was performed comparing current values with the baseline in each interval (delta analysis). Linear mixed models considering the correlation structure of the repeated measurements were used to evaluate factors associated with different trends in FTI and LTI. Results: BMI increased about 0.6 kg/m2 over 24 months from baseline. This was associated with increase in FTI of about 0.95 kg/m2 and a decrease in LTI of about 0.4 kg/m2. Female gender, diabetic status, and low baseline FTI were associated with a significant greater increase of FTI. Age > 67 years, diabetes, male gender, high baseline LTI, and low baseline FTI were associated with a significant greater decrease of LTI. Conclusions: With the transition to hemodialysis, end-stage renal disease patients presented with distinctive changes in BC. These were mainly associated with gender, older age, presence of diabetes, low baseline FTI, and high baseline LTI. BMI increases did not fully represent the changes in BC.",,human,0.9312388300895691
185,,18224,"Background Encouraging healthy lifestyles in children is a challenge. This project aimed to improve lifestyles of younger peers by engaging adolescent creators (ACs) to design and implement peer-led and social marketing (SM) health-promoting activities. Methods A 10-month parallel-cluster randomised controlled school-based pilot study was performed in disadvantaged neighbourhoods in Reus (Spain) spanning two academic years (2015-2016/2016-2017). Eight primary schools (n=375 children) and four high schools (n=94ACs) were randomly placed in the intervention group. The 94 ACs (12-14 years) designed and implemented four SM activities for their younger peers (9-11 years). Eight primary schools (n=327 children) and three high schools (n=98 adolescents) served as the control group and received no intervention. Primary (physical activity and fruit consumption) and secondary outcomes (screen time, vegetables, soft drinks, sweets and fast food consumptions) were assessed with validated questionnaires at baseline and at the end of the study. Results After 10 months, fruit consumption and physical activity were maintained in the children who consumed ≥1 fruit/day and spent ≥6 hours/week physical activity. However, compared with the controls, the intervention significantly increased the physical activity of girls to 15.6 min/week, whereas the percentage of girls who consumed sweets, soft drinks and fast food decreased significantly by 8.4%, 14.5% and 5.9%, respectively. Additionally, the percentage of ≥2 hour/weekday of screen time by boys decreased significantly by 8.2%. Conclusion The European Youth Tackling Obesity-Kids, SM and peer-led intervention, effectively increased physical activity hours/week in girls, but was not effective in improving the percentage of children who consumed the recommended fruit. Moreover, the percentages of girls who consumed sweets, soft drinks and fast food and boys screen time decreased. Trial registration number NCT02702336; Pre-results.",human,0,sdg_abstracts_original,human,0.9997863173484802,"Background Encouraging healthy lifestyles in children is a challenge. This project aimed to improve lifestyles of younger peers by engaging adolescent creators (ACs) to design and implement peer-led and social marketing (SM) health-promoting activities. Methods A 10-month parallel-cluster randomised controlled school-based pilot study was performed in disadvantaged neighbourhoods in Reus (Spain) spanning two academic years (2015-2016/2016-2017). Eight primary schools (n=375 children) and four high schools (n=94ACs) were randomly placed in the intervention group. The 94 ACs (12-14 years) designed and implemented four SM activities for their younger peers (9-11 years). Eight primary schools (n=327 children) and three high schools (n=98 adolescents) served as the control group and received no intervention. Primary (physical activity and fruit consumption) and secondary outcomes (screen time, vegetables, soft drinks, sweets and fast food consumptions) were assessed with validated questionnaires at baseline and at the end of the study. Results After 10 months, fruit consumption and physical activity were maintained in the children who consumed ≥1 fruit/day and spent ≥6 hours/week physical activity. However, compared with the controls, the intervention significantly increased the physical activity of girls to 15.6 min/week, whereas the percentage of girls who consumed sweets, soft drinks and fast food decreased significantly by 8.4%, 14.5% and 5.9%, respectively. Additionally, the percentage of ≥2 hour/weekday of screen time by boys decreased significantly by 8.2%. Conclusion The European Youth Tackling Obesity-Kids, SM and peer-led intervention, effectively increased physical activity hours/week in girls, but was not effective in improving the percentage of children who consumed the recommended fruit. Moreover, the percentages of girls who consumed sweets, soft drinks and fast food and boys screen time decreased. Trial registration number NCT02702336; Pre-results.",,human,0.9997863173484802
186,,22335,"This chapter discusses the roles of cell surface carbohydrates in development, while focusing on embryo implantation, spermatogenesis, and tissue maturation. The outer surface of mammalian cells is covered by glycoproteins and glycolipids. Substantial biochemical and immunochemical evidence suggests that cell surface carbohydrates play significant roles in development and health. Functional studies of cell surface carbohydrates still leave many questions unanswered. In the last decade, genetic approaches and sophisticated chemical analyses have enabled us to reveal the function of specific carbohydrate structures in vivo, and as a result the role of carbohydrates in development and disease is understood. In the field of reproductive biology and embryology, it has been assumed that cell surface carbohydrates play important roles. These hypotheses are difficult to test because embryonic development is dynamic and the material of interest is often too limited to allow chemical analysis. Analyzing early stage embryos requires well-trained hands and skills that many biochemists and molecular biologists have only recently developed. Nonetheless, many attractive hypotheses await testing by new technologies. ",human,0,sdg_abstracts_original,human,0.9995999932289124,"This chapter discusses the roles of cell surface carbohydrates in development, while focusing on embryo implantation, spermatogenesis, and tissue maturation. The outer surface of mammalian cells is covered by glycoproteins and glycolipids. Substantial biochemical and immunochemical evidence suggests that cell surface carbohydrates play significant roles in development and health. Functional studies of cell surface carbohydrates still leave many questions unanswered. In the last decade, genetic approaches and sophisticated chemical analyses have enabled us to reveal the function of specific carbohydrate structures in vivo, and as a result the role of carbohydrates in development and disease is understood. In the field of reproductive biology and embryology, it has been assumed that cell surface carbohydrates play important roles. These hypotheses are difficult to test because embryonic development is dynamic and the material of interest is often too limited to allow chemical analysis. Analyzing early stage embryos requires well-trained hands and skills that many biochemists and molecular biologists have only recently developed. Nonetheless, many attractive hypotheses await testing by new technologies. ",,human,0.9995999932289124
187,,1153,"The approach to the formation of indicators of the main directions of socio-economic development in the space of characteristics of regional differentiation is presented. At this stage of research, the basis of differentiation characteristics includes five components: the scale of the economy, the assessment of technical efficiency, the assessment of the trend of technical efficiency, the first and second main components of the GRP structure. The indicator of each direction which was built in the basis is maximally correlated with the indicator formed on the basis of the corresponding group of indicators. Eight indicators following main areas have been formed: production of goods and services, material well-being, quality of the population, quality of the social sphere, internal security. Their features and the analysis of interrelation are given. Indicators can be interpreted in terms of differentiation characteristics. Changes in these characteristics can be predicted as a result of the implementation of Federal and regional investment projects and, as a result, to assess the impact of such projects on various areas of socio-economic development of the regions. Therefore, the basis of regional differentiation characteristics is considered as a tool of project management.",human,0,sdg_abstracts_original,human,0.955389142036438,"The approach to the formation of indicators of the main directions of socio-economic development in the space of characteristics of regional differentiation is presented. At this stage of research, the basis of differentiation characteristics includes five components: the scale of the economy, the assessment of technical efficiency, the assessment of the trend of technical efficiency, the first and second main components of the GRP structure. The indicator of each direction which was built in the basis is maximally correlated with the indicator formed on the basis of the corresponding group of indicators. Eight indicators following main areas have been formed: production of goods and services, material well-being, quality of the population, quality of the social sphere, internal security. Their features and the analysis of interrelation are given. Indicators can be interpreted in terms of differentiation characteristics. Changes in these characteristics can be predicted as a result of the implementation of Federal and regional investment projects and, as a result, to assess the impact of such projects on various areas of socio-economic development of the regions. Therefore, the basis of regional differentiation characteristics is considered as a tool of project management.",,human,0.9553894400596619
188,,10975,"Background: Trauma is a significant contributor to global disease, and low-income countries disproportionately shoulder this burden. Education and training are critical components in the effort to address the surgical workforce shortage. Educators can tailor training to a diverse background of health professionals in low-resource settings using competency-based curricula. We present a process for the development of a competency-based curriculum for low-resource settings in the context of craniomaxillofacial (CMF) trauma education. Methods: CMF trauma surgeons representing 7 low-, middle-, and high-income countries conducted a standardized educational curriculum development program. Patient problems related to facial injuries were identified and ranked from highest to lowest morbidity. Higher morbidity problems were categorized into 4 modules with agreed upon competencies. Methods of delivery (lectures, case discussions, and practical exercises) were selected to optimize learning of each competency. Results: A facial injuries educational curriculum (1.5 days event) was tailored to health professionals with diverse training backgrounds who care for CMF trauma patients in low-resource settings. A backward planned, competency-based curriculum was organized into four modules titled: acute (emergent), eye (periorbital injuries and sight preserving measures), mouth (dental injuries and fracture care), and soft tissue injury treatments. Four courses have been completed with pre- and post-course assessments completed. Conclusions: Surgeons and educators from a diverse geographic background found the backward planning curriculum development method effective in creating a competency-based facial injuries (trauma) course for health professionals in low-resource settings, where contextual aspects of shortages of surgical capacity, equipment, and emergency transportation must be considered.",human,0,sdg_abstracts_original,human,0.9996324777603148,"Background: Trauma is a significant contributor to global disease, and low-income countries disproportionately shoulder this burden. Education and training are critical components in the effort to address the surgical workforce shortage. Educators can tailor training to a diverse background of health professionals in low-resource settings using competency-based curricula. We present a process for the development of a competency-based curriculum for low-resource settings in the context of craniomaxillofacial (CMF) trauma education. Methods: CMF trauma surgeons representing 7 low-, middle-, and high-income countries conducted a standardized educational curriculum development program. Patient problems related to facial injuries were identified and ranked from highest to lowest morbidity. Higher morbidity problems were categorized into 4 modules with agreed upon competencies. Methods of delivery (lectures, case discussions, and practical exercises) were selected to optimize learning of each competency. Results: A facial injuries educational curriculum (1.5 days event) was tailored to health professionals with diverse training backgrounds who care for CMF trauma patients in low-resource settings. A backward planned, competency-based curriculum was organized into four modules titled: acute (emergent), eye (periorbital injuries and sight preserving measures), mouth (dental injuries and fracture care), and soft tissue injury treatments. Four courses have been completed with pre- and post-course assessments completed. Conclusions: Surgeons and educators from a diverse geographic background found the backward planning curriculum development method effective in creating a competency-based facial injuries (trauma) course for health professionals in low-resource settings, where contextual aspects of shortages of surgical capacity, equipment, and emergency transportation must be considered.",,human,0.9996324777603149
189,,18607,"This study aimed at exploring whether cyberspace and the anonymity it provides can be useful in coping with leisure constraints that originate in gender stereotypes. The study was based on a survey that measured perceptions of parallel online and offline forms of masculine, feminine, and gender neutral leisure activities among 240 Internet users belonging to one of two age groups: adolescents and seniors. Findings indicated that offline feminine activities were perceived as more suitable for both genders in their online form. By contrast, the offline leisure activities labeled masculine were still perceived as such in their online form, with older participants even considering them more masculine in that mode. These findings suggest that online leisure plays a dual role, rendering activities more accessible to men of all ages but further discriminating against older women. It seems to broaden age and gender disparities and perpetuates power relations inherent in patriarchal societies.",human,0,sdg_abstracts_original,human,0.9998302459716796,"This study aimed at exploring whether cyberspace and the anonymity it provides can be useful in coping with leisure constraints that originate in gender stereotypes. The study was based on a survey that measured perceptions of parallel online and offline forms of masculine, feminine, and gender neutral leisure activities among 240 Internet users belonging to one of two age groups: adolescents and seniors. Findings indicated that offline feminine activities were perceived as more suitable for both genders in their online form. By contrast, the offline leisure activities labeled masculine were still perceived as such in their online form, with older participants even considering them more masculine in that mode. These findings suggest that online leisure plays a dual role, rendering activities more accessible to men of all ages but further discriminating against older women. It seems to broaden age and gender disparities and perpetuates power relations inherent in patriarchal societies.",,human,0.9998302459716797
190,,2472,"The formulation of a strategic plan for the conservation of terrestrial biodiversity in the Cape Floristic Region (CFR; 87,892 km2) requires an objective and spatially explicit assessment of the representativeness of major habitat categories (surrogates for biodiversity) currently under protection. A GIS layer of statutory and non-statutory conservation areas was used, along with layers of many biological and physical features, to explore the configuration of conserved areas relative to key biological and physical indicators. Three analyses were performed. (1) Recursive partitioning, a classification-tree analysis technique, was used to contrast features of protected areas with non-protected areas. (2) The conservation status of 16 primary and 88 secondary Broad Habitat Units (BHUs; derived on the basis of topography, geology, homogeneous climatic zones, and floristic composition) was assessed in terms of prescribed conservation targets. (3) The extent to which protected areas are able to sustain ecological and evolutionary processes was explored by assessing the extent of spatial components of these processes for all conservation areas. About 20% of the CFR is protected in some form of conservation area, mostly concentrated on sandstone substrates, and areas with high altitude and steep slopes. The reservation bias towards upland areas has seriously constrained representation of biodiversity pattern and processes. Most of the habitat diversity is poorly represented in the current conservation area system with only 9% of the remaining primary BHUs in the lowlands conserved. However, almost 50% of the Mountain Fynbos Complex is conserved (largely exceeding its conservation target). Spatial components of the ecological processes identified are poorly captured by the conservation area network although faunal and floral migration is possible in the uplands due to the strong spatial connectivity of the protected network. ",human,0,sdg_abstracts_original,human,0.9996864795684814,"The formulation of a strategic plan for the conservation of terrestrial biodiversity in the Cape Floristic Region (CFR; 87,892 km2) requires an objective and spatially explicit assessment of the representativeness of major habitat categories (surrogates for biodiversity) currently under protection. A GIS layer of statutory and non-statutory conservation areas was used, along with layers of many biological and physical features, to explore the configuration of conserved areas relative to key biological and physical indicators. Three analyses were performed. (1) Recursive partitioning, a classification-tree analysis technique, was used to contrast features of protected areas with non-protected areas. (2) The conservation status of 16 primary and 88 secondary Broad Habitat Units (BHUs; derived on the basis of topography, geology, homogeneous climatic zones, and floristic composition) was assessed in terms of prescribed conservation targets. (3) The extent to which protected areas are able to sustain ecological and evolutionary processes was explored by assessing the extent of spatial components of these processes for all conservation areas. About 20% of the CFR is protected in some form of conservation area, mostly concentrated on sandstone substrates, and areas with high altitude and steep slopes. The reservation bias towards upland areas has seriously constrained representation of biodiversity pattern and processes. Most of the habitat diversity is poorly represented in the current conservation area system with only 9% of the remaining primary BHUs in the lowlands conserved. However, almost 50% of the Mountain Fynbos Complex is conserved (largely exceeding its conservation target). Spatial components of the ecological processes identified are poorly captured by the conservation area network although faunal and floral migration is possible in the uplands due to the strong spatial connectivity of the protected network. ",,human,0.9996864795684814
191,,21130,"Extractive distillation (ED) processes are widely used for the separation of aromatic and non-aromatic hydrocarbons. Approximate boiling points of components and azeotropes in the mixture need solvents to aid the separation processes. A considerable mass flowrate and recovery of solvents in separation processes leads to significant energy requirement. This study provides a new extractive distillation process for the efficient separation of aromatic and non-aromatic hydrocarbons aided by sulfolane solution. A flash tank and a semi-lean solution stream are introduced to modify existing extractive distillation processes to reduce the reboiler heat duty of the entrainer recovery column and improve the separation performance of the extractive distillation column. The NRTL-RK property method and rigorous unit models in Aspen Plus are used to simulate new and existing processes. No-databank model parameters are regressed to improve the accuracy of simulations. A coordinative strategy is proposed to optimize the significant operating parameters for new and existing processes by combining Aspen Plus with MATLAB. Compared with the optimal existing extractive distillation process, the new extractive distillation process reduces the operating costs by 8.9% when heat integration is not considered, and 13.04% of the total annual cost can be reduced when a heat exchanger network is considered.",human,0,sdg_abstracts_original,chatgpt,0.9800139665603638,"Extractive distillation (ED) processes are widely used for the separation of aromatic and non-aromatic hydrocarbons. Approximate boiling points of components and azeotropes in the mixture need solvents to aid the separation processes. A considerable mass flowrate and recovery of solvents in separation processes leads to significant energy requirement. This study provides a new extractive distillation process for the efficient separation of aromatic and non-aromatic hydrocarbons aided by sulfolane solution. A flash tank and a semi-lean solution stream are introduced to modify existing extractive distillation processes to reduce the reboiler heat duty of the entrainer recovery column and improve the separation performance of the extractive distillation column. The NRTL-RK property method and rigorous unit models in Aspen Plus are used to simulate new and existing processes. No-databank model parameters are regressed to improve the accuracy of simulations. A coordinative strategy is proposed to optimize the significant operating parameters for new and existing processes by combining Aspen Plus with MATLAB. Compared with the optimal existing extractive distillation process, the new extractive distillation process reduces the operating costs by 8.9% when heat integration is not considered, and 13.04% of the total annual cost can be reduced when a heat exchanger network is considered.",,chatgpt,0.9800138473510742
192,,25903,"To understand the influence of different urban green spaces on outdoor thermal comfort, researchers have focused on developing a thermal comfort range and estimating neutral and preferred temperatures through questionnaire surveys and microclimatic measurements. The simultaneity of investigation among different sites is often neglected, which decreases the accuracy and reliability of the outcomes. To fill this gap and to better address the local variation of outdoor thermal comfort in different urban green spaces, both surveys and measurements were performed simultaneously at three sites, “central grassland,” “pond-side garden” and “grove” in a subtropical urban local area in warm and cool periods. Remarkable differences in microclimatic conditions, thermal sensations, thermal comfort, and both neutral temperatures and preferred temperatures were observed among the three sites and in the two periods. Preferred temperatures were lower than neutral temperatures among different sites in each period or both periods combined, indicating the instinctive preference of people from relatively hot regions for a cooler thermal state. The grove yielded the best cooling capability but was perceived as the hottest and most uncomfortable site in the warm period. The central grassland with the highest air temperature, conversely, was perceived as the most comfortable site. The subjective perceptions were also reflected by lower neutral and preferred temperatures in the grove than those in the central grassland in the warm period. In particular, for preferred temperatures, up to 8.8 °C operative temperature (Top) and 4.3 °C physiological equivalent temperature (PET) differences between these two sites were observed. Hence, a high density of trees does not necessarily result in better outdoor thermal comfort. Future urban planning and management, therefore, should emphasize the types and structures of different urban green spaces. To further comprehensively explore the roles of different urban green spaces in outdoor thermal comfort, high synchronicity among sites during investigation is important.",human,0,sdg_abstracts_original,human,0.9997921586036682,"To understand the influence of different urban green spaces on outdoor thermal comfort, researchers have focused on developing a thermal comfort range and estimating neutral and preferred temperatures through questionnaire surveys and microclimatic measurements. The simultaneity of investigation among different sites is often neglected, which decreases the accuracy and reliability of the outcomes. To fill this gap and to better address the local variation of outdoor thermal comfort in different urban green spaces, both surveys and measurements were performed simultaneously at three sites, “central grassland,” “pond-side garden” and “grove” in a subtropical urban local area in warm and cool periods. Remarkable differences in microclimatic conditions, thermal sensations, thermal comfort, and both neutral temperatures and preferred temperatures were observed among the three sites and in the two periods. Preferred temperatures were lower than neutral temperatures among different sites in each period or both periods combined, indicating the instinctive preference of people from relatively hot regions for a cooler thermal state. The grove yielded the best cooling capability but was perceived as the hottest and most uncomfortable site in the warm period. The central grassland with the highest air temperature, conversely, was perceived as the most comfortable site. The subjective perceptions were also reflected by lower neutral and preferred temperatures in the grove than those in the central grassland in the warm period. In particular, for preferred temperatures, up to 8.8 °C operative temperature (Top) and 4.3 °C physiological equivalent temperature (PET) differences between these two sites were observed. Hence, a high density of trees does not necessarily result in better outdoor thermal comfort. Future urban planning and management, therefore, should emphasize the types and structures of different urban green spaces. To further comprehensively explore the roles of different urban green spaces in outdoor thermal comfort, high synchronicity among sites during investigation is important.",,human,0.9997921586036682
193,,3913,"The catechol-O-methyltransferase (COMT) enzyme catabolizes dopamine. The val158met single nucleotide polymorphism (rs4680) in the COMT gene has received considerable attention as a candidate gene for schizophrenia as well as for frontally mediated cognitive functions. Antisaccade performance is a good measure of frontal lobe integrity. Deficits on the task are considered a trait marker for schizophrenia. The aim of this study was to investigate the association of COMT val158met polymorphism with antisaccade eye movements in schizophrenia patients and healthy controls. Schizophrenia patients (N = 105) and healthy controls (N = 95) underwent infrared oculographic assessment of antisaccades. Subjects were genotyped for COMT val 158met and divided into 3 groups according to genotype (val/val, val/met, and met/met). Patients displayed significantly more reflexive errors, longer and more variable latency, and lower amplitude gain than controls (all P < 0.02). A greater number of val158 alleles was associated with shorter (P = 0.045) and less variable (P = 0.028) antisaccade latency and, nonsignificantly, with lower reflexive error rate (P = 0.056). None of these variables showed a group-by-genotype interaction (P > 0.1). There were no significant associations of genotype with measures of amplitude gain or spatial error (P > 0.2). The results suggest that COMT val158 carrier status is associated with better performance on the antisaccade task. Possible explanations of this finding are discussed.",human,0,sdg_abstracts_original,human,0.999692440032959,"The catechol-O-methyltransferase (COMT) enzyme catabolizes dopamine. The val158met single nucleotide polymorphism (rs4680) in the COMT gene has received considerable attention as a candidate gene for schizophrenia as well as for frontally mediated cognitive functions. Antisaccade performance is a good measure of frontal lobe integrity. Deficits on the task are considered a trait marker for schizophrenia. The aim of this study was to investigate the association of COMT val158met polymorphism with antisaccade eye movements in schizophrenia patients and healthy controls. Schizophrenia patients (N = 105) and healthy controls (N = 95) underwent infrared oculographic assessment of antisaccades. Subjects were genotyped for COMT val 158met and divided into 3 groups according to genotype (val/val, val/met, and met/met). Patients displayed significantly more reflexive errors, longer and more variable latency, and lower amplitude gain than controls (all P < 0.02). A greater number of val158 alleles was associated with shorter (P = 0.045) and less variable (P = 0.028) antisaccade latency and, nonsignificantly, with lower reflexive error rate (P = 0.056). None of these variables showed a group-by-genotype interaction (P > 0.1). There were no significant associations of genotype with measures of amplitude gain or spatial error (P > 0.2). The results suggest that COMT val158 carrier status is associated with better performance on the antisaccade task. Possible explanations of this finding are discussed.",,human,0.999692440032959
194,,11995,"An increase in the frequency of oil spills in the sea, which adversely affects the maritime environment, led to the motivation for this article. Recent advances in technology have now made it possible to provide Unmanned Aerial Vehicles (UAV) systems with great processing capacity. They include all the features of a complete computer, while maintaining a really small size as required by this kind of system. This investigation takes advantage of the functionalities provided by existing MultiAgent Systems (MAS) to accomplish tasks between UAVs. The article presents a case study that uses the capabilities to perform the detection of oil spills.",human,0,sdg_abstracts_original,chatgpt,0.9228807091712952,"An increase in the frequency of oil spills in the sea, which adversely affects the maritime environment, led to the motivation for this article. Recent advances in technology have now made it possible to provide Unmanned Aerial Vehicles (UAV) systems with great processing capacity. They include all the features of a complete computer, while maintaining a really small size as required by this kind of system. This investigation takes advantage of the functionalities provided by existing MultiAgent Systems (MAS) to accomplish tasks between UAVs. The article presents a case study that uses the capabilities to perform the detection of oil spills.",,chatgpt,0.9228811860084534
195,,22345,"⌉Let S be a polynomial ring over an infinite field and let I be a homogeneous ideal of S. Let Td be a polynomial ring whose variables correspond to the monomials of degree d in S. We study the initial ideals of the ideals Vd(I) ⊂ Td that define the Veronese subrings of S/I. In suitable orders, they are easily deduced from the initial ideal of I. We show that in Vd(I) is generated in degree ≤ max (⌉ reg(I)/d ⌈, 2), where reg(I) is the regularity of the ideal I. (In other words, the dth Veronese subrings of any commutative graded ring S/I has a Gröbner basis of degree ≤ max (⌉(I)/d ⌈, 2).) We also give bounds on the regularity of I in terms of the degrees of the generators of in(I) and some combinatorial data. This implies a version of Backelin′s theorem that high Veronese subrings of any ring are homogeneous Koszul algebras in the sense of Priddy [Trans. Amer. Math. Soc, 152 (1970), 39-60]. We also give a general obstruction for a homogeneous ideal I ⊂ S to have an initial ideal in(I) that is generated by quadrics, beyond the obvious requirement that I itself should be generated by quadrics, and the stronger statement that S/I is Koszul. We use the obstruction to show that in certain dimensions, a generic complete intersection of quadrics cannot have an initial ideal that is generated by quadrics. For the application to Backelin′s theorem, we require a result of Backelin whose proof has never appeared. We give a simple proof of a sharpened version, bounding the rate of growth of the degrees of generators for syzygies of any multihomogenous module over a polynomial ring modulo an ideal generated by monomials, following a method of Bruns and Herzog. ",human,0,sdg_abstracts_original,human,0.9998304843902588,"⌉Let S be a polynomial ring over an infinite field and let I be a homogeneous ideal of S. Let Td be a polynomial ring whose variables correspond to the monomials of degree d in S. We study the initial ideals of the ideals Vd(I) ⊂ Td that define the Veronese subrings of S/I. In suitable orders, they are easily deduced from the initial ideal of I. We show that in Vd(I) is generated in degree ≤ max (⌉ reg(I)/d ⌈, 2), where reg(I) is the regularity of the ideal I. (In other words, the dth Veronese subrings of any commutative graded ring S/I has a Gröbner basis of degree ≤ max (⌉(I)/d ⌈, 2).) We also give bounds on the regularity of I in terms of the degrees of the generators of in(I) and some combinatorial data. This implies a version of Backelin′s theorem that high Veronese subrings of any ring are homogeneous Koszul algebras in the sense of Priddy [Trans. Amer. Math. Soc, 152 (1970), 39-60]. We also give a general obstruction for a homogeneous ideal I ⊂ S to have an initial ideal in(I) that is generated by quadrics, beyond the obvious requirement that I itself should be generated by quadrics, and the stronger statement that S/I is Koszul. We use the obstruction to show that in certain dimensions, a generic complete intersection of quadrics cannot have an initial ideal that is generated by quadrics. For the application to Backelin′s theorem, we require a result of Backelin whose proof has never appeared. We give a simple proof of a sharpened version, bounding the rate of growth of the degrees of generators for syzygies of any multihomogenous module over a polynomial ring modulo an ideal generated by monomials, following a method of Bruns and Herzog. ",,human,0.9998304843902588
196,,19583,"The enlargement of the EU to Central and Eastern Europe after 2004 was accompanied by great optimism: more dynamic economic development was expected in the wider Europe and also a general further development of social standards. The banking and debt crisis that started in 2008, however, has revealed structural shortcomings that disrupted and partly reversed the desired trends. The main reasons were inadequate governance options with regard to European economic and financial policy, together with substantial interference in the social dimension in the old and the new Member States. The impact of these processes in terms of convergence and divergence in the EU can be demonstrated on the basis of comparative empirical data series of core indicators in key areas for EU citizens. This impact has been exacerbated by inadequate state social protection and its funding by means of widely varying forms of taxation. A summary composite convergence/divergence index shows the current effects of crisis strategies in the EU regions. ",human,0,sdg_abstracts_original,human,0.9944029450416564,"The enlargement of the EU to Central and Eastern Europe after 2004 was accompanied by great optimism: more dynamic economic development was expected in the wider Europe and also a general further development of social standards. The banking and debt crisis that started in 2008, however, has revealed structural shortcomings that disrupted and partly reversed the desired trends. The main reasons were inadequate governance options with regard to European economic and financial policy, together with substantial interference in the social dimension in the old and the new Member States. The impact of these processes in terms of convergence and divergence in the EU can be demonstrated on the basis of comparative empirical data series of core indicators in key areas for EU citizens. This impact has been exacerbated by inadequate state social protection and its funding by means of widely varying forms of taxation. A summary composite convergence/divergence index shows the current effects of crisis strategies in the EU regions. ",,human,0.9944029450416565
197,,12142,"We discuss the new notion of a lim Cohen-Macaulay sequence of modules over a local ring, and also a somewhat weaker notion, as well as the theory of content for local cohomology modules. We relate both to the problem of proving the direct summand conjecture and other homological conjectures without using almost ring theory and perfectoid space theory, and we also indicate some other open problems whose solution would yield a new proof of the direct summand conjecture.",human,0,sdg_abstracts_original,human,0.9819459319114684,"We discuss the new notion of a lim Cohen-Macaulay sequence of modules over a local ring, and also a somewhat weaker notion, as well as the theory of content for local cohomology modules. We relate both to the problem of proving the direct summand conjecture and other homological conjectures without using almost ring theory and perfectoid space theory, and we also indicate some other open problems whose solution would yield a new proof of the direct summand conjecture.",,human,0.981945812702179
198,,17812,"Advantages of the acousto-optic tunable filter (AOTF), namely its ability for fast scanning and multiple-wavelength diffraction, were exploited to develop a novel, all solid-state, nonmoving parts spectrofluorometer. This instrument is based on the use of two AOTFs: one for excitation and the other for emission. The first AOTF was used to specifically diffract white incident light into a specific wavelength(s) for excitation. Depending on the needs, the second AOTF (i.e., the emission AOTF) can be used as either a very fast dispersive device or a polychromator. In the first configuration, the sample was excited by a single-excitation wavelength; the emitted light was analyzed by the emission AOTF, which was scanned very fast. A speed of 4.8 Å was found to be the fastest speed which the AOTF can be scanned with a reasonable S/N and resolution. With this speed, a spectrum of 150 nm can be measured in 312 μs. Faster scanning is possible but, because of the limitation due to the speed of the acoustic wave, may undesiredly lead to degradation in the S/N and spectral resolution. In the second configuration, both AOTFs were used as a polychromator. Several different rf signals were simultaneously applied into the first AOTF to provide multiple-excitation wavelengths. The emission was simultaneously analyzed at several wavelengths by the emission AOTF. With this configuration, the fluorometer can be used for the analysis of multicomponent samples, and the maximum number of components it can analyze is, in principle, a × b, where a and b are the number of excitation and emission wavelengths, respectively. The multicomponent analysis was successfully performed for four component samples using the preliminary setup where a = b = 2. ",human,0,sdg_abstracts_original,human,0.9998286962509156,"Advantages of the acousto-optic tunable filter (AOTF), namely its ability for fast scanning and multiple-wavelength diffraction, were exploited to develop a novel, all solid-state, nonmoving parts spectrofluorometer. This instrument is based on the use of two AOTFs: one for excitation and the other for emission. The first AOTF was used to specifically diffract white incident light into a specific wavelength(s) for excitation. Depending on the needs, the second AOTF (i.e., the emission AOTF) can be used as either a very fast dispersive device or a polychromator. In the first configuration, the sample was excited by a single-excitation wavelength; the emitted light was analyzed by the emission AOTF, which was scanned very fast. A speed of 4.8 Å was found to be the fastest speed which the AOTF can be scanned with a reasonable S/N and resolution. With this speed, a spectrum of 150 nm can be measured in 312 μs. Faster scanning is possible but, because of the limitation due to the speed of the acoustic wave, may undesiredly lead to degradation in the S/N and spectral resolution. In the second configuration, both AOTFs were used as a polychromator. Several different rf signals were simultaneously applied into the first AOTF to provide multiple-excitation wavelengths. The emission was simultaneously analyzed at several wavelengths by the emission AOTF. With this configuration, the fluorometer can be used for the analysis of multicomponent samples, and the maximum number of components it can analyze is, in principle, a × b, where a and b are the number of excitation and emission wavelengths, respectively. The multicomponent analysis was successfully performed for four component samples using the preliminary setup where a = b = 2. ",,human,0.9998286962509155
199,,22624,Previous Swedish studies led to a recommendation to limit the load on a single axle to 6 Mg and on a tandem axle unit to 8 Mg in order to avoid permanent subsoil compaction in wet soil. In recent years large tyres that can carry large loads at low inflation pressure have been available. In a new project it was studied whether the use of such tyres may change the recommended load limits. Loads of 8-12 Mg on a single axle or 10-20 Mg on a tandem axle unit significantly reduced the porosity in the 300-400 and 400-500 mm soil layer. In the 400-500 mm soil layer in inflation pressure of 50 or 100 kPa compared with 150 kPa did not substantially reduce compaction. The results indicate that no important increase in the previously recommended limits can be proposed. ,human,0,sdg_abstracts_original,human,0.9998180270195008,Previous Swedish studies led to a recommendation to limit the load on a single axle to 6 Mg and on a tandem axle unit to 8 Mg in order to avoid permanent subsoil compaction in wet soil. In recent years large tyres that can carry large loads at low inflation pressure have been available. In a new project it was studied whether the use of such tyres may change the recommended load limits. Loads of 8-12 Mg on a single axle or 10-20 Mg on a tandem axle unit significantly reduced the porosity in the 300-400 and 400-500 mm soil layer. In the 400-500 mm soil layer in inflation pressure of 50 or 100 kPa compared with 150 kPa did not substantially reduce compaction. The results indicate that no important increase in the previously recommended limits can be proposed. ,,human,0.9998180270195007
200,,8640,"Eighteen river- and two ground- water samples were collected on Huaibei plain. The major ions, and hydrogen and oxygen isotope concentrations were determined, and statistical and other analyses carried out. The results showed that all of the waters are alkaline, with high Total Dissolved Solids (TDS) concentrations. Na+ + K+, and SO2-4+ HCO-3 are the dominant anionic and cationic species, respectively, and the waters are mainly Na·K-SO4 and Na·K-HCO3 types. The δD and δ18O in river waters ranged from -53.07‰ to - 22.07‰ and - 6.97‰ to - 1.23‰, with average values of - 38.30‰and - 4.09‰, respectively. The δD and δ18O concentrations in groundwater were lower than in the river water samples. The correlation between δD and δ18O concentrations in the river water can be described by the formula δD = 5.32*δ18O - 16.54, which can also be considered the local evaporation line. The ionic content and character of the river water is mainly controlled by precipitation, evaporation and carbonate weathering, as deduced from the Gibbs diagram and principal component analyses.",human,0,sdg_abstracts_original,human,0.9996165037155152,"Eighteen river- and two ground- water samples were collected on Huaibei plain. The major ions, and hydrogen and oxygen isotope concentrations were determined, and statistical and other analyses carried out. The results showed that all of the waters are alkaline, with high Total Dissolved Solids (TDS) concentrations. Na+ + K+, and SO2-4+ HCO-3 are the dominant anionic and cationic species, respectively, and the waters are mainly Na·K-SO4 and Na·K-HCO3 types. The δD and δ18O in river waters ranged from -53.07‰ to - 22.07‰ and - 6.97‰ to - 1.23‰, with average values of - 38.30‰and - 4.09‰, respectively. The δD and δ18O concentrations in groundwater were lower than in the river water samples. The correlation between δD and δ18O concentrations in the river water can be described by the formula δD = 5.32*δ18O - 16.54, which can also be considered the local evaporation line. The ionic content and character of the river water is mainly controlled by precipitation, evaporation and carbonate weathering, as deduced from the Gibbs diagram and principal component analyses.",,human,0.9996165037155151
201,,20256,"Background: The optimal sequence of systemic palliative chemotherapy in metastatic breast cancer is unknown. Case Report: We report the course of disease in a patient aged 24 years at the onset of disease, who was treated over 17 years for her metastatic ErbB2-positive breast cancer. Seventeen years ago, the patient was diagnosed to have invasive ductal breast cancer and underwent surgical treatment. She received 6 cycles of adjuvant chemotherapy with CMF (cyclophosphamide, methotrexate, 5-fluorouracil). Twenty-eight months after the primary surgery, the patient developed a histologically verified lung metastasis, and subsequently received 8 different lines of chemotherapy, including high-dose cytotoxic therapy and stem cell support as well as trastuzumab. In addition, she received 5 different types of antihormonal treatment. Due to the recent progression of her lung metastasis, 27 cycles of lapatinib and capecitabine were administered. A long-term partial response in the lung was observed. Conclusions: Individualized systemic treatment with the tyrosine kinase inhibitor lapatinib and capecitabine in heavily pretreated patients with Her2-positive metastatic breast cancer may lead to effective palliation for almost 2 years despite extensive pretreatment. Copyright ",human,0,sdg_abstracts_original,human,0.8547834753990173,"Background: The optimal sequence of systemic palliative chemotherapy in metastatic breast cancer is unknown. Case Report: We report the course of disease in a patient aged 24 years at the onset of disease, who was treated over 17 years for her metastatic ErbB2-positive breast cancer. Seventeen years ago, the patient was diagnosed to have invasive ductal breast cancer and underwent surgical treatment. She received 6 cycles of adjuvant chemotherapy with CMF (cyclophosphamide, methotrexate, 5-fluorouracil). Twenty-eight months after the primary surgery, the patient developed a histologically verified lung metastasis, and subsequently received 8 different lines of chemotherapy, including high-dose cytotoxic therapy and stem cell support as well as trastuzumab. In addition, she received 5 different types of antihormonal treatment. Due to the recent progression of her lung metastasis, 27 cycles of lapatinib and capecitabine were administered. A long-term partial response in the lung was observed. Conclusions: Individualized systemic treatment with the tyrosine kinase inhibitor lapatinib and capecitabine in heavily pretreated patients with Her2-positive metastatic breast cancer may lead to effective palliation for almost 2 years despite extensive pretreatment. Copyright ",,human,0.8547833561897278
202,,7035,"We revealed the arrangement of single-wall carbon nanotube (SWNT) which is suitable for the adsorption of hydrogen by means of grand canonical Monte Carlo (GCMC) simulation with simple cylindrical model. Here, we calculated the amount of adsorbed hydrogen with triangular lattice (TL) and square lattice (SL) model for the bundle structure with various kinds of tube diameters (D) and inter-axis distances (Ra). Our results indicate that any arrangements having smaller Ra are not suitable for the storage of hydrogen and the adsorption amount of hydrogen can be achieved the target value (6 wt.% and 45 kg m-3) proposed by Department of Energy (DOE) in United States by SWNTs having larger Ra at 77 K and 1 MPa. Furthermore, these results show that the best arrangement of SWNTs for the adsorption of hydrogen at this condition is TL structure having Ra = 2.159 nm and D = 1.227 nm. ",human,0,sdg_abstracts_original,human,0.9997900128364564,"We revealed the arrangement of single-wall carbon nanotube (SWNT) which is suitable for the adsorption of hydrogen by means of grand canonical Monte Carlo (GCMC) simulation with simple cylindrical model. Here, we calculated the amount of adsorbed hydrogen with triangular lattice (TL) and square lattice (SL) model for the bundle structure with various kinds of tube diameters (D) and inter-axis distances (Ra). Our results indicate that any arrangements having smaller Ra are not suitable for the storage of hydrogen and the adsorption amount of hydrogen can be achieved the target value (6 wt.% and 45 kg m-3) proposed by Department of Energy (DOE) in United States by SWNTs having larger Ra at 77 K and 1 MPa. Furthermore, these results show that the best arrangement of SWNTs for the adsorption of hydrogen at this condition is TL structure having Ra = 2.159 nm and D = 1.227 nm. ",,human,0.9997900128364563
203,,22362,"Inequitable gender norms, including the acceptance of violence in intimate relationships, have been found to be associated with the occurrence of intimate partner violence (IPV) perpetration and victimization. Despite these findings, few studies have considered whether inequitable gender norms are related to IPV severity. This study uses baseline data from a psychotherapeutic intervention targeting heterosexual couples (n = 247) in Lusaka, Zambia, who reported moderate to severe male-perpetrated IPV and male hazardous alcohol use to consider: (a) prevailing gender norms, including those related to IPV; (b) the relationship between IPV acceptance and IPV severity; and (c) the relationship between inequitable gender norms and IPV severity. Multiple linear regression analyses were used to model the relationships between IPV acceptance and inequitable gender norms, and female-reported IPV severity (including threats of violence, physical violence, sexual violence, and total violence), separately among male and female participants. In general, men and women were similar in their patterns of agreement with gender norms, with both highly endorsing items related to household roles. More than three-quarters of men (78.1%) and women (78.5%) indicated overall acceptance of violence in intimate relationships, with no significant differences between men and women in their endorsement of any IPV-related gender norms. Among men, IPV acceptance was associated with a statistically significant increase in IPV perpetration severity in terms of threatening violence (B = 5.86, 95% confidence interval [CI] = [1.84, 9.89]), physical violence (B = 4.54, 95% CI = [0.10, 8.98]), and total violence (B = 11.65, 95% CI = [3.14, 20.16]). There was no association between IPV acceptance and IPV victimization severity among women. Unlike IPV acceptance, there was no evidence for a relationship between inequitable gender norms and IPV severity for either men or women. These findings have implications for the appropriateness of gender transformative interventions in targeting men and women in relationships in which there is ongoing IPV.",human,0,sdg_abstracts_original,human,0.9998251795768738,"Inequitable gender norms, including the acceptance of violence in intimate relationships, have been found to be associated with the occurrence of intimate partner violence (IPV) perpetration and victimization. Despite these findings, few studies have considered whether inequitable gender norms are related to IPV severity. This study uses baseline data from a psychotherapeutic intervention targeting heterosexual couples (n = 247) in Lusaka, Zambia, who reported moderate to severe male-perpetrated IPV and male hazardous alcohol use to consider: (a) prevailing gender norms, including those related to IPV; (b) the relationship between IPV acceptance and IPV severity; and (c) the relationship between inequitable gender norms and IPV severity. Multiple linear regression analyses were used to model the relationships between IPV acceptance and inequitable gender norms, and female-reported IPV severity (including threats of violence, physical violence, sexual violence, and total violence), separately among male and female participants. In general, men and women were similar in their patterns of agreement with gender norms, with both highly endorsing items related to household roles. More than three-quarters of men (78.1%) and women (78.5%) indicated overall acceptance of violence in intimate relationships, with no significant differences between men and women in their endorsement of any IPV-related gender norms. Among men, IPV acceptance was associated with a statistically significant increase in IPV perpetration severity in terms of threatening violence (B = 5.86, 95% confidence interval [CI] = [1.84, 9.89]), physical violence (B = 4.54, 95% CI = [0.10, 8.98]), and total violence (B = 11.65, 95% CI = [3.14, 20.16]). There was no association between IPV acceptance and IPV victimization severity among women. Unlike IPV acceptance, there was no evidence for a relationship between inequitable gender norms and IPV severity for either men or women. These findings have implications for the appropriateness of gender transformative interventions in targeting men and women in relationships in which there is ongoing IPV.",,human,0.9998251795768738
204,,20261,"Two lignites (Mequinenza, Spain) and two subbituminous coals (Teruel, Spain), their demineralized derivatives (HC1/HF + HC1) and their corresponding derived vitrinite concentrates were submitted to hydroliquefaction in tetralin in fixed conditions to study a possible synergism of vitrinite concentrates in the original coal matrix. Sufficiently pure amounts of vitrinite concentrates were isolated by a method based on differential centrifugation in CsCl. The coals were characterized by densimetric and petrographic analyses including rcflectance-frequency distributions. A synergism for vitrinite concentrates related to the demineralized coals has not been found here because all the vitrinite concentrates, once separated, have similar or higher reactivity than in the corresponding original coal matrix. On the other hand, the studied lignite-derived vitrinite concentrates have proved to be much more reactive than the subbituminous-derived ones. Displacements of Absorbance-Density curves and maxima toward higher densities (densimetric analyses data) and appearance of V-4 vitrinite type structures (coal refleetograms) in the case of the subbituminous coals imply differences in chemical structures for the lignite and the subbituminous derived vitrinite concentrates which could explain the differences in reactivity. ",human,0,sdg_abstracts_original,chatgpt,0.912430226802826,"Two lignites (Mequinenza, Spain) and two subbituminous coals (Teruel, Spain), their demineralized derivatives (HC1/HF + HC1) and their corresponding derived vitrinite concentrates were submitted to hydroliquefaction in tetralin in fixed conditions to study a possible synergism of vitrinite concentrates in the original coal matrix. Sufficiently pure amounts of vitrinite concentrates were isolated by a method based on differential centrifugation in CsCl. The coals were characterized by densimetric and petrographic analyses including rcflectance-frequency distributions. A synergism for vitrinite concentrates related to the demineralized coals has not been found here because all the vitrinite concentrates, once separated, have similar or higher reactivity than in the corresponding original coal matrix. On the other hand, the studied lignite-derived vitrinite concentrates have proved to be much more reactive than the subbituminous-derived ones. Displacements of Absorbance-Density curves and maxima toward higher densities (densimetric analyses data) and appearance of V-4 vitrinite type structures (coal refleetograms) in the case of the subbituminous coals imply differences in chemical structures for the lignite and the subbituminous derived vitrinite concentrates which could explain the differences in reactivity. ",,chatgpt,0.9124295115470886
205,,15478,"Objectives: to explore the relationships between maternal distress, breast feeding cessation, breast feeding problems and breast feeding maternal role attainment. Design: longitudinal cohort study. Setting: three urban hospitals within Sydney, Australia. Participants: 449 women were invited to participate in the study, with an 81% response rate. Measurement: self-report questionnaires were used to collect the data in pregnancy (28-36 weeks) and 2 weeks and 3 months after birth. The Edinburgh Postnatal Depression Scale (EPDS) was used to measure postnatal distress, and the Maternal Role Attainment subscale (MRA) of the Maternal Breast Feeding Evaluation Scale (MBFES) was used to measure breast feeding maternal role attainment. Findings: women with high MRA were less likely to stop breast feeding (even when they had breast feeding problems) than women with low MRA. Antenatal EPDS and anxiety scores were not related to breast feeding cessation or breast feeding problems when analysed alone. As hypothesised, the relationship between breast feeding cessation and postnatal distress (EPDS scores) varied according to MRA level. Women who were categorised as high MRA and no longer breast feeding had higher EPDS scores and were more likely to be categorised as distressed (36%) than women who had low MRA (<12%) or women who had high MRA and continued to breast feed (7%). Implications: there is a complex relationship between maternal identity, stopping breast feeding earlier than desired, and psychological distress. Women with strong beliefs about the importance of breast feeding to their maternal role may benefit from psychological assessment and support should they decide to stop breast feeding earlier. ",human,0,sdg_abstracts_original,human,0.8995615839958191,"Objectives: to explore the relationships between maternal distress, breast feeding cessation, breast feeding problems and breast feeding maternal role attainment. Design: longitudinal cohort study. Setting: three urban hospitals within Sydney, Australia. Participants: 449 women were invited to participate in the study, with an 81% response rate. Measurement: self-report questionnaires were used to collect the data in pregnancy (28-36 weeks) and 2 weeks and 3 months after birth. The Edinburgh Postnatal Depression Scale (EPDS) was used to measure postnatal distress, and the Maternal Role Attainment subscale (MRA) of the Maternal Breast Feeding Evaluation Scale (MBFES) was used to measure breast feeding maternal role attainment. Findings: women with high MRA were less likely to stop breast feeding (even when they had breast feeding problems) than women with low MRA. Antenatal EPDS and anxiety scores were not related to breast feeding cessation or breast feeding problems when analysed alone. As hypothesised, the relationship between breast feeding cessation and postnatal distress (EPDS scores) varied according to MRA level. Women who were categorised as high MRA and no longer breast feeding had higher EPDS scores and were more likely to be categorised as distressed (36%) than women who had low MRA (<12%) or women who had high MRA and continued to breast feed (7%). Implications: there is a complex relationship between maternal identity, stopping breast feeding earlier than desired, and psychological distress. Women with strong beliefs about the importance of breast feeding to their maternal role may benefit from psychological assessment and support should they decide to stop breast feeding earlier. ",,human,0.8995603322982788
206,,7194,"Seed dispersal is a primary factor influencing plant community development in salt-marshes. However, studies on seed dispersal in marsh-channel system are relatively scarce, and relevant mathematical modelling is particularly lacking. In this study, we developed a one-dimensional advection–dispersion model to explore the patterns of long-distance floating seed dispersal in tidal channels. Oscillatory tidal current velocity and water depth were assumed to represent the tidal effects. An exponential decay coefficient was introduced to account for the loss of floating seeds due to water-logged deposition and retention by trapping agents along the channel banks. An analytical solution in integral form was derived using the method of Green's function. The developed model was applied to simulate floating Spartina densiflora seed dispersal in a real tidal channel. Seed floating duration and seed collection data reported in the literature were used for model calibration and validation. The leptokurtic seed dispersal kernel in the marsh-channel system was quantified for the first time using the predicted spatio-temporal distribution of deposited and retained seeds. The range of the long-distance floating seed dispersal and its distribution concentration that have important implications for salt marsh protection and restoration activities were evaluated through statistical parameters such as the centroid displacement and the variance of the floating seed cloud and floating seed concentration contours. The developed model provides a useful tool for ecological management of tidal marshes, as data on the long-distance floating seed dispersal are rarely available and hard to gather.",human,0,sdg_abstracts_original,human,0.9997835755348206,"Seed dispersal is a primary factor influencing plant community development in salt-marshes. However, studies on seed dispersal in marsh-channel system are relatively scarce, and relevant mathematical modelling is particularly lacking. In this study, we developed a one-dimensional advection–dispersion model to explore the patterns of long-distance floating seed dispersal in tidal channels. Oscillatory tidal current velocity and water depth were assumed to represent the tidal effects. An exponential decay coefficient was introduced to account for the loss of floating seeds due to water-logged deposition and retention by trapping agents along the channel banks. An analytical solution in integral form was derived using the method of Green's function. The developed model was applied to simulate floating Spartina densiflora seed dispersal in a real tidal channel. Seed floating duration and seed collection data reported in the literature were used for model calibration and validation. The leptokurtic seed dispersal kernel in the marsh-channel system was quantified for the first time using the predicted spatio-temporal distribution of deposited and retained seeds. The range of the long-distance floating seed dispersal and its distribution concentration that have important implications for salt marsh protection and restoration activities were evaluated through statistical parameters such as the centroid displacement and the variance of the floating seed cloud and floating seed concentration contours. The developed model provides a useful tool for ecological management of tidal marshes, as data on the long-distance floating seed dispersal are rarely available and hard to gather.",,human,0.9997835755348206
207,,9860,"This study proposes an “optimal” spectral acceleration based intensity measure (IM) to assess the collapse capacity of generic moment frames vulnerable to the P-delta effect. The IM is derived from the geometric mean of the spectral pseudo-acceleration over a certain period interval. The optimized IM includes for first time a flexible lower limit for the period interval, corresponding to the structural period associated with the exceedance of 95% of the total effective mass. This flexible lower limit bound provides an efficient IM, independently of the contribution of higher modes to the total response. The upper bound period is 1.6 times the fundamental period to account for period elongation due to inelastic deformations and gravity loads. In a parametric study on generic frames, structural parameters are varied to quantify the performance of this IM compared to classical benchmark IMs. The “optimal” IM provides minimum, or close to the minimum, dispersion for the entire set of frames with different fundamental periods of vibration, number of stories, and P-delta vulnerability.",human,0,sdg_abstracts_original,human,0.9993072748184204,"This study proposes an “optimal” spectral acceleration based intensity measure (IM) to assess the collapse capacity of generic moment frames vulnerable to the P-delta effect. The IM is derived from the geometric mean of the spectral pseudo-acceleration over a certain period interval. The optimized IM includes for first time a flexible lower limit for the period interval, corresponding to the structural period associated with the exceedance of 95% of the total effective mass. This flexible lower limit bound provides an efficient IM, independently of the contribution of higher modes to the total response. The upper bound period is 1.6 times the fundamental period to account for period elongation due to inelastic deformations and gravity loads. In a parametric study on generic frames, structural parameters are varied to quantify the performance of this IM compared to classical benchmark IMs. The “optimal” IM provides minimum, or close to the minimum, dispersion for the entire set of frames with different fundamental periods of vibration, number of stories, and P-delta vulnerability.",,human,0.9993072748184204
208,,12735,"This paper introduces a voltage mode digital controller for low-power high-frequency dc-dc switch-mode power supplies (SMPS) that has fast transient response, approaching physical limitations of a given power stage. In steady state, the controller operates as a conventional pulsewidth modulation regulator and during transients it utilizes a novel fast voltage recovery mechanism, based on real-time processing of the output voltage in digital domain. This continuous-time digital signal processing mechanism is implemented with a very simple processor consisting of a set of asynchronous comparators, delay cells, and combinatorial logic. To eliminate the need for current measurement and calculate the optimal switching sequence of the power stage transistors, the processor performs a capacitor charge balance algorithm, which is based on the detection of the output voltage peak/valley point. The effectiveness of the controller is demonstrated on an experimental 5 W, 5 V to 1.8 V, 400 kHz buck converter. The converter recovers from load transients through a single on-off action of the power switch, virtually reaching the shortest possible time, limited by the values of the power stage filter components only. ",human,0,sdg_abstracts_original,chatgpt,0.9876717329025269,"This paper introduces a voltage mode digital controller for low-power high-frequency dc-dc switch-mode power supplies (SMPS) that has fast transient response, approaching physical limitations of a given power stage. In steady state, the controller operates as a conventional pulsewidth modulation regulator and during transients it utilizes a novel fast voltage recovery mechanism, based on real-time processing of the output voltage in digital domain. This continuous-time digital signal processing mechanism is implemented with a very simple processor consisting of a set of asynchronous comparators, delay cells, and combinatorial logic. To eliminate the need for current measurement and calculate the optimal switching sequence of the power stage transistors, the processor performs a capacitor charge balance algorithm, which is based on the detection of the output voltage peak/valley point. The effectiveness of the controller is demonstrated on an experimental 5 W, 5 V to 1.8 V, 400 kHz buck converter. The converter recovers from load transients through a single on-off action of the power switch, virtually reaching the shortest possible time, limited by the values of the power stage filter components only. ",,chatgpt,0.9876718521118164
209,,24510,"This article explores various opportunities for government procurement practices that could contribute to achieving the European Kyoto targets and the diffusion of innovative energy technologies. It examines the potential for conflict between ‘climate-friendly’ public purchasing, on the one hand, and the World Trade Organization's Agreement on Government Procurement (GPA) and the European Directives on public procurement, on the other. The main arguments put forward are, firstly, that the climate mitigation potential of government procurement is significant; and, secondly, that the potential for conflict between climate-friendly procurement and the GPA is limited because of the restricted membership and scope of this agreement. The broader scope of the EU procurement Directives arguably increases the potential for conflict, but it is shown that these rules do not hinder climate-friendly procurement if public entities specify, at an early stage, exactly what they intend to procure, and adhere to the general principles of non-discrimination and transparency. Thirdly, a problem might occur if a public entity wishes to purchase innovative products for which no standard specifications exist and for which there are only a few suppliers. Finally, it is recommended that climate-friendly procurement should be implemented to the fullest extent possible. ",human,0,sdg_abstracts_original,human,0.9991618394851683,"This article explores various opportunities for government procurement practices that could contribute to achieving the European Kyoto targets and the diffusion of innovative energy technologies. It examines the potential for conflict between ‘climate-friendly’ public purchasing, on the one hand, and the World Trade Organization's Agreement on Government Procurement (GPA) and the European Directives on public procurement, on the other. The main arguments put forward are, firstly, that the climate mitigation potential of government procurement is significant; and, secondly, that the potential for conflict between climate-friendly procurement and the GPA is limited because of the restricted membership and scope of this agreement. The broader scope of the EU procurement Directives arguably increases the potential for conflict, but it is shown that these rules do not hinder climate-friendly procurement if public entities specify, at an early stage, exactly what they intend to procure, and adhere to the general principles of non-discrimination and transparency. Thirdly, a problem might occur if a public entity wishes to purchase innovative products for which no standard specifications exist and for which there are only a few suppliers. Finally, it is recommended that climate-friendly procurement should be implemented to the fullest extent possible. ",,human,0.9991618394851685
210,,18265,"This paper proposes a distributed scheme for demand response and user adaptation in smart grid networks. Our system model considers scarce distributed power sources and loads. User preference is modelled as 'willingness to pay' parameter and logarithmic utility functions are used to model the behaviour of users. The energy management problem is cast as an optimization problem, where the objective is to maximize the utility services to the clients based on price-based demand response scheme. We have addressed the issue concerning the allocation of power among users from multiple sources/utilities within a distributed power network based on users' demands and willingness to pay. We envision a central entity providing a coordinated response to the huge number of scattered consumers, collecting power from all generators and assigning the power flow to the interested users. We propose a two layer price-based demand response architecture. The lower level energy management scheme deals with the power allocation from aggregator to the consumers, and the upper level deals with the distribution of power from utilities to aggregators to ensure the demand-supply balance.",human,0,sdg_abstracts_original,human,0.9998254179954528,"This paper proposes a distributed scheme for demand response and user adaptation in smart grid networks. Our system model considers scarce distributed power sources and loads. User preference is modelled as 'willingness to pay' parameter and logarithmic utility functions are used to model the behaviour of users. The energy management problem is cast as an optimization problem, where the objective is to maximize the utility services to the clients based on price-based demand response scheme. We have addressed the issue concerning the allocation of power among users from multiple sources/utilities within a distributed power network based on users' demands and willingness to pay. We envision a central entity providing a coordinated response to the huge number of scattered consumers, collecting power from all generators and assigning the power flow to the interested users. We propose a two layer price-based demand response architecture. The lower level energy management scheme deals with the power allocation from aggregator to the consumers, and the upper level deals with the distribution of power from utilities to aggregators to ensure the demand-supply balance.",,human,0.9998254179954529
211,,20036,"This paper evaluated the quality of Environmental Impact Assessment (EIA) reports submitted to EPA Punjab, Pakistan during 2005-2013 (2005 is the period when the practice became more common) prepared by different consultants and investigated whether the information provided in the report is adequate for ultimate decision making by using a comprehensive checklist. Data was collected through interviews, EPA library and Punjab public library. The results are presented in graphical form after the detailed review of randomly selected 100 EIA reports submitted to EPA Punjab during the study period. The selected EIA reports were prepared for new development projects, expansion/extension projects, rehabilitation projects and also for funding projects by donor agencies i.e. World Bank and ADB. The study revealed that the reports prepared for international funding agencies contained adequate data whereas the assessment report prepared by the consultants for local development project were lacking in substantial primary data and adequate evidences, showing lack of commitment towards sustainable development and environment protection.",human,0,sdg_abstracts_original,human,0.9998143315315248,"This paper evaluated the quality of Environmental Impact Assessment (EIA) reports submitted to EPA Punjab, Pakistan during 2005-2013 (2005 is the period when the practice became more common) prepared by different consultants and investigated whether the information provided in the report is adequate for ultimate decision making by using a comprehensive checklist. Data was collected through interviews, EPA library and Punjab public library. The results are presented in graphical form after the detailed review of randomly selected 100 EIA reports submitted to EPA Punjab during the study period. The selected EIA reports were prepared for new development projects, expansion/extension projects, rehabilitation projects and also for funding projects by donor agencies i.e. World Bank and ADB. The study revealed that the reports prepared for international funding agencies contained adequate data whereas the assessment report prepared by the consultants for local development project were lacking in substantial primary data and adequate evidences, showing lack of commitment towards sustainable development and environment protection.",,human,0.9998143315315247
212,,24707,"The method of author cocitation analysis (ACA) was first presented by White and Griffith in 1981 as a ""literature measure of intellectual structure"" and its applicability for the mapping of areas of science has since then been tested in various bibliometric science mapping studies. In this study, an experimental method of calculating the first or single author cocitation frequency is presented and compared with the standard method. Applying Ward's method of clustering, the analysis revealed that the two approaches did not produce similar results and a tentative interpretation of deviations was that the experimental method provided with a more detailed depiction of the specialty structure. It was also concluded that a number of additional research questions need to be resolved before a comprehensive understanding of the suggested method's merits and demerits is reached. ",human,0,sdg_abstracts_original,human,0.9365219473838806,"The method of author cocitation analysis (ACA) was first presented by White and Griffith in 1981 as a ""literature measure of intellectual structure"" and its applicability for the mapping of areas of science has since then been tested in various bibliometric science mapping studies. In this study, an experimental method of calculating the first or single author cocitation frequency is presented and compared with the standard method. Applying Ward's method of clustering, the analysis revealed that the two approaches did not produce similar results and a tentative interpretation of deviations was that the experimental method provided with a more detailed depiction of the specialty structure. It was also concluded that a number of additional research questions need to be resolved before a comprehensive understanding of the suggested method's merits and demerits is reached. ",,human,0.9365214705467224
213,,22683,"Amaranthus tricolor (Amaranthaceae) is a popular vegetable in China. Separation of lipophilic extracts of the aerial parts of A. tricolor by means of preparative 'High-speed Countercurrent Chromatography' (HSCCC) led to the isolation of two chlorophyll breakdown products which were characterized as 132-hydroxy-(132-S)-phaeophytin-a (1) and chlorophyll b methoxylactone (2). The structure of 1 was established using a combination of 1D- and 2D-NMR spectroscopic and mass spectrometric techniques. Component 2 was identified by means of ESI-MS/MS analysis. Industrial relevance: Chlorophylls and their degradation products are natural pigments of interesting bioactivities. For industry, recovery of these pigments from waste materials of fruit and vegetable processings could be a promising by-product line. Here we present the isolation of a pure chlorophyll breakdown product which is in principle suited as a nutraceutical ingredient. Large-scale isolation procedures using countercurrent chromatography techniques for chlorophyll related pigments are currently under development. ",human,0,sdg_abstracts_original,human,0.9997519850730896,"Amaranthus tricolor (Amaranthaceae) is a popular vegetable in China. Separation of lipophilic extracts of the aerial parts of A. tricolor by means of preparative 'High-speed Countercurrent Chromatography' (HSCCC) led to the isolation of two chlorophyll breakdown products which were characterized as 132-hydroxy-(132-S)-phaeophytin-a (1) and chlorophyll b methoxylactone (2). The structure of 1 was established using a combination of 1D- and 2D-NMR spectroscopic and mass spectrometric techniques. Component 2 was identified by means of ESI-MS/MS analysis. Industrial relevance: Chlorophylls and their degradation products are natural pigments of interesting bioactivities. For industry, recovery of these pigments from waste materials of fruit and vegetable processings could be a promising by-product line. Here we present the isolation of a pure chlorophyll breakdown product which is in principle suited as a nutraceutical ingredient. Large-scale isolation procedures using countercurrent chromatography techniques for chlorophyll related pigments are currently under development. ",,human,0.9997519850730896
214,,15001,"Capsule The uneven sex ratio in favour of males found in flocks of Purple Sandpipers is not evident in chicks; female chicks are lighter than males. Aims To test if there is an uneven sex ratio at hatching and to investigate whether sex ratio changes during chick growth and after fledging. Methods Sex of chicks was determined (by DNA) and each chick given a unique combination of colour-rings for later re-sightings. Biometric measurements were made for comparisons between the sexes. Results The sex ratio of 97 chicks did not deviate significantly from parity, 52.6% males (95% CI: 42.2-62.8%), and there was no evidence that the sex ratio changed as chicks grew older. However, sightings of colour-ringed birds in their first year indicated a change to 61% males. The same sex ratio (61% males) was observed among wintering birds in Iceland. Chicks already showed sexual size dimorphism in skeletal measurements as is also found in full-grown birds. Female chicks were relatively lighter than males, suggesting poorer condition. Conclusions The uneven sex ratios seen in non-breeding flocks of Purple Sandpipers are not determined at hatching but might arise owing to higher juvenile mortality among females soon after fledging. This may result from the relatively lighter mass of female chicks. ",human,0,sdg_abstracts_original,human,0.9997698664665222,"Capsule The uneven sex ratio in favour of males found in flocks of Purple Sandpipers is not evident in chicks; female chicks are lighter than males. Aims To test if there is an uneven sex ratio at hatching and to investigate whether sex ratio changes during chick growth and after fledging. Methods Sex of chicks was determined (by DNA) and each chick given a unique combination of colour-rings for later re-sightings. Biometric measurements were made for comparisons between the sexes. Results The sex ratio of 97 chicks did not deviate significantly from parity, 52.6% males (95% CI: 42.2-62.8%), and there was no evidence that the sex ratio changed as chicks grew older. However, sightings of colour-ringed birds in their first year indicated a change to 61% males. The same sex ratio (61% males) was observed among wintering birds in Iceland. Chicks already showed sexual size dimorphism in skeletal measurements as is also found in full-grown birds. Female chicks were relatively lighter than males, suggesting poorer condition. Conclusions The uneven sex ratios seen in non-breeding flocks of Purple Sandpipers are not determined at hatching but might arise owing to higher juvenile mortality among females soon after fledging. This may result from the relatively lighter mass of female chicks. ",,human,0.9997698664665222
215,,12896,"Background: Low- and middle-income countries bear the mortality burden of head injury compared with high-income countries. Not much has been studied about predictors of poor outcome after head injury in these countries. This study describes and quantifies the effect of demographics and injury causative factors on mortality in a cohort managed in a Nigerian tertiary hospital intensive care. Materials and Methods: A retrospective study was undertaken of all patients admitted into intensive care with severe head injury at the University of Port Harcourt Teaching Hospital, Nigeria between 1 <sup>st</sup> January, 1997 and 31 <sup>st</sup> December, 2006. Logistic regression analysis was performed to examine the effect of age, gender and injury etiology on risk of intensive care unit (ICU) mortality. Results: The number of ICU admission for severe head injury was 231 patients with a male to female ratio of 2.8:1. Patients' mean age and standard deviation was 31.2 ± 15.5 years. The mortality rate was 52.8%. Road traffic injury was the most common etiologic factor (84%). Logistic regression analysis indicated a 56% increase in the risk of ICU mortality between the ages of 21 and 40 years. The effect of age was found to be nonlinear (likelihood ratio P = 0.033). On multivariable analysis, patient's gender (odds ratio 1.07; 95% confidence interval: 0.56-1.97) and etiology of injury were not significantly associated with risk of mortality. Gender was not a modifier of the effect of age (P = 0.218). Conclusion: The study indicated a strong prognostic effect of age. Gender and etiology of injury had no effect on ICU mortality among study cohort.",human,0,sdg_abstracts_original,human,0.9997972846031188,"Background: Low- and middle-income countries bear the mortality burden of head injury compared with high-income countries. Not much has been studied about predictors of poor outcome after head injury in these countries. This study describes and quantifies the effect of demographics and injury causative factors on mortality in a cohort managed in a Nigerian tertiary hospital intensive care. Materials and Methods: A retrospective study was undertaken of all patients admitted into intensive care with severe head injury at the University of Port Harcourt Teaching Hospital, Nigeria between 1 <sup>st</sup> January, 1997 and 31 <sup>st</sup> December, 2006. Logistic regression analysis was performed to examine the effect of age, gender and injury etiology on risk of intensive care unit (ICU) mortality. Results: The number of ICU admission for severe head injury was 231 patients with a male to female ratio of 2.8:1. Patients' mean age and standard deviation was 31.2 ± 15.5 years. The mortality rate was 52.8%. Road traffic injury was the most common etiologic factor (84%). Logistic regression analysis indicated a 56% increase in the risk of ICU mortality between the ages of 21 and 40 years. The effect of age was found to be nonlinear (likelihood ratio P = 0.033). On multivariable analysis, patient's gender (odds ratio 1.07; 95% confidence interval: 0.56-1.97) and etiology of injury were not significantly associated with risk of mortality. Gender was not a modifier of the effect of age (P = 0.218). Conclusion: The study indicated a strong prognostic effect of age. Gender and etiology of injury had no effect on ICU mortality among study cohort.",,human,0.9997972846031189
216,,4750,"1. 1. Various types of distinctive quivering movements by worker honeybees inside the hive have been observed, and a detailed study of one of these (""shaking"") was undertaken. 2. 2. The workers were found to be shaken throughout the year, although with greatly increased frequency in the summer. 3. 3. A daily cycle of shaking, commencing at daybreak and finishing at dusk, has been demonstrated. Shaking was recorded prior to the first flights of the day. Resemblances to fluctuations in flight activity could be traced but these were not sufficiently consistent to indicate a direct relationship between the two activities, nor was it possible to show a relationship with meteorological factors. 4. 4. The ages of workers engaged in shaking ranged from 2 to 61 days, with a progressive increase in numbers up to about 3 weeks of age. Following this it is thought that there was little change in the proportion shaking relative to the total number of older marked bees in the hive. 5. 5. There were indications that the frequency of shaking increased slightly with increasing age, but the increase was not statistically significant. 6. 6. Workers with an age-range of 0-55 days were shaken, but the younger bees were shaken less frequently than the older. 7. 7. The behaviour of individual shakers and of bees shaken is discussed briefly. ",human,0,sdg_abstracts_original,human,0.9998297691345216,"1. 1. Various types of distinctive quivering movements by worker honeybees inside the hive have been observed, and a detailed study of one of these (""shaking"") was undertaken. 2. 2. The workers were found to be shaken throughout the year, although with greatly increased frequency in the summer. 3. 3. A daily cycle of shaking, commencing at daybreak and finishing at dusk, has been demonstrated. Shaking was recorded prior to the first flights of the day. Resemblances to fluctuations in flight activity could be traced but these were not sufficiently consistent to indicate a direct relationship between the two activities, nor was it possible to show a relationship with meteorological factors. 4. 4. The ages of workers engaged in shaking ranged from 2 to 61 days, with a progressive increase in numbers up to about 3 weeks of age. Following this it is thought that there was little change in the proportion shaking relative to the total number of older marked bees in the hive. 5. 5. There were indications that the frequency of shaking increased slightly with increasing age, but the increase was not statistically significant. 6. 6. Workers with an age-range of 0-55 days were shaken, but the younger bees were shaken less frequently than the older. 7. 7. The behaviour of individual shakers and of bees shaken is discussed briefly. ",,human,0.9998297691345215
217,,13178,"This study examined the prevalence, availability, and use of antimalarial medicines (AMLs) along the Thai-Cambodian border. The study was divided into two parts: the first looked at the quality of AMLs available in six Thai provinces and the second obtained information about the availability and use of AMLs. A randomized sampling methodology was used to select locations and collect samples, which were screened using Global Pharma Health Fund (GPHF) Mini- labs®. A subset of samples was sent to quality control laboratories for verification testing. For the second part of the study, face-to-face interviews were conducted with members of randomly selected households and the staff of health facilities in villages with the highest malaria incidence to find out where they acquired their AMLs and which were used most frequently. The results of quality testing showed an overall failure rate of 1% (7 of 709 samples) for active pharmaceutical ingredients (API); however, the API failure rate varied from 0.0% to 2.2% by location and the overall failure rates of samples by province varied from 0.0% to 3.4%. A total of 97.9% (n=272) of respondents had taken AMLS. The most commonly used medicines were primaquine (30% of respondents), chloroquine (15.8%), artesunate+mefloquine (12%), and quinine (10%). Most respondents (97.9%) had received medications from public hospitals or malaria clinics.",human,0,sdg_abstracts_original,human,0.999804675579071,"This study examined the prevalence, availability, and use of antimalarial medicines (AMLs) along the Thai-Cambodian border. The study was divided into two parts: the first looked at the quality of AMLs available in six Thai provinces and the second obtained information about the availability and use of AMLs. A randomized sampling methodology was used to select locations and collect samples, which were screened using Global Pharma Health Fund (GPHF) Mini- labs®. A subset of samples was sent to quality control laboratories for verification testing. For the second part of the study, face-to-face interviews were conducted with members of randomly selected households and the staff of health facilities in villages with the highest malaria incidence to find out where they acquired their AMLs and which were used most frequently. The results of quality testing showed an overall failure rate of 1% (7 of 709 samples) for active pharmaceutical ingredients (API); however, the API failure rate varied from 0.0% to 2.2% by location and the overall failure rates of samples by province varied from 0.0% to 3.4%. A total of 97.9% (n=272) of respondents had taken AMLS. The most commonly used medicines were primaquine (30% of respondents), chloroquine (15.8%), artesunate+mefloquine (12%), and quinine (10%). Most respondents (97.9%) had received medications from public hospitals or malaria clinics.",,human,0.999804675579071
218,,8305,"Aims: To evaluate the risk of having occult ductal carcinoma in situ or invasive carcinoma in the region of a focus of lobular (in situ) neoplasia (LN) diagnosed on needle core biopsy (NCB) of breast. Methods: All cases of LN diagnosed on NCB of breast over 10 years (2000-2009 inclusive) were reviewed. The clinical presentation, radiological appearances and final pathological diagnosis on open diagnostic biopsy (ODB) were correlated. Results: 125 cases of LN on NCB were identified from diagnostic codes. Of these, 72 (58%) had a coexistent, higher-grade lesion that mandated surgery. Fifty of the remaining 53 (94%) underwent ODB. The majority of patients were asymptomatic, with 68% presenting through the breast screening programme, and in 89% of patients, the target abnormality was microcalcification. Of the 50 patients, 13 (26%) had a final diagnosis of in situ or invasive carcinoma requiring therapeutic surgery. When the cases of pleomorphic LN were excluded, 21% (10/47) were upgraded. Two of these 10 cases had discordant radiology which could have been diagnosed on repeat NCB leaving an upgrade rate of 18% (8/45). In four of the eight cases of invasive malignancy, the disease was multifocal. Conclusions: LN is frequently asymptomatic, being identified by mammographic microcalcification alone. In 21% of classical LN cases, it is associated with an undiagnosed, higher-grade lesion requiring oncological management. In our view, patients with LN discovered on NCB should undergo open diagnostic biopsy.",human,0,sdg_abstracts_original,human,0.9998170733451844,"Aims: To evaluate the risk of having occult ductal carcinoma in situ or invasive carcinoma in the region of a focus of lobular (in situ) neoplasia (LN) diagnosed on needle core biopsy (NCB) of breast. Methods: All cases of LN diagnosed on NCB of breast over 10 years (2000-2009 inclusive) were reviewed. The clinical presentation, radiological appearances and final pathological diagnosis on open diagnostic biopsy (ODB) were correlated. Results: 125 cases of LN on NCB were identified from diagnostic codes. Of these, 72 (58%) had a coexistent, higher-grade lesion that mandated surgery. Fifty of the remaining 53 (94%) underwent ODB. The majority of patients were asymptomatic, with 68% presenting through the breast screening programme, and in 89% of patients, the target abnormality was microcalcification. Of the 50 patients, 13 (26%) had a final diagnosis of in situ or invasive carcinoma requiring therapeutic surgery. When the cases of pleomorphic LN were excluded, 21% (10/47) were upgraded. Two of these 10 cases had discordant radiology which could have been diagnosed on repeat NCB leaving an upgrade rate of 18% (8/45). In four of the eight cases of invasive malignancy, the disease was multifocal. Conclusions: LN is frequently asymptomatic, being identified by mammographic microcalcification alone. In 21% of classical LN cases, it is associated with an undiagnosed, higher-grade lesion requiring oncological management. In our view, patients with LN discovered on NCB should undergo open diagnostic biopsy.",,human,0.9998170733451843
219,,401,"Background: The finding of casual sex partners on the internet and methamphetamine use have been described as risk factors for HIV infection in men who have sex with men (MSM). However, the interplay between these factors has not been studied prospectively in one design. This study aims to determine the associations between finding casual sex partners on the internet and incident methamphetamine use and HIV infection. Methods: In this observational cohort study of Thai MSM, we recruited Bangkok residents aged 18 years or older with a history of penetrative male-to-male sex in the past 6 months. Baseline and follow-up visits were done at a dedicated study clinic in central Bangkok. Men were tested for HIV infection at every study visit and for sexually transmitted infections at baseline. Baseline demographics and HIV risk behaviour information were collected at every visit by audio computer-assisted self-interview. We used a descriptive model using bivariate odds ratios to elucidate the order of risk factors in the causal pathway to HIV incidence and methamphetamine use. We used Cox proportional hazard regression analysis to evaluate covariates for incident methamphetamine use and HIV infection. Findings: From April 6, 2006, to Dec 31, 2010, 1977 men were screened and 1764 were found eligible. 1744 men were enrolled, of whom 1372 tested negative for HIV and were followed up until March 20, 2012. Per 100 person-years of follow-up, incidence of methamphetamine use was 3·8 (128 events in 3371 person-years) and incidence of HIV infection was 6·0 (212 events in 3554 person-years). In our descriptive model, methamphetamine use, anal sex, and various other behaviours cluster together but their effect on HIV incidence was mediated by the occurrence of ulcerative sexually transmitted infections. Dual risk factors for both incident methamphetamine use and HIV infection were younger age and finding casual sex partners on the internet. Having ever received money for sex was predictive for incident methamphetamine use; living alone or with a housemate, recent anal sex, and ulcerative sexually transmitted infections at baseline were predictive for incident HIV infection. Interpretation: In MSM in Bangkok, casual sex partner recruitment on the internet, methamphetamine use, and sexually transmitted infections have important roles in sustaining the HIV epidemic. Virtual HIV prevention education, drug use harm reduction, and biomedical HIV prevention methods, such as pre-exposure prophylaxis, could help to reduce or revert the HIV epidemic among MSM in Bangkok. Funding: US Centers for Disease Control and Prevention and the Johns Hopkins Fogarty AIDS International Training and Research Program.",human,0,sdg_abstracts_original,human,0.9998088479042052,"Background: The finding of casual sex partners on the internet and methamphetamine use have been described as risk factors for HIV infection in men who have sex with men (MSM). However, the interplay between these factors has not been studied prospectively in one design. This study aims to determine the associations between finding casual sex partners on the internet and incident methamphetamine use and HIV infection. Methods: In this observational cohort study of Thai MSM, we recruited Bangkok residents aged 18 years or older with a history of penetrative male-to-male sex in the past 6 months. Baseline and follow-up visits were done at a dedicated study clinic in central Bangkok. Men were tested for HIV infection at every study visit and for sexually transmitted infections at baseline. Baseline demographics and HIV risk behaviour information were collected at every visit by audio computer-assisted self-interview. We used a descriptive model using bivariate odds ratios to elucidate the order of risk factors in the causal pathway to HIV incidence and methamphetamine use. We used Cox proportional hazard regression analysis to evaluate covariates for incident methamphetamine use and HIV infection. Findings: From April 6, 2006, to Dec 31, 2010, 1977 men were screened and 1764 were found eligible. 1744 men were enrolled, of whom 1372 tested negative for HIV and were followed up until March 20, 2012. Per 100 person-years of follow-up, incidence of methamphetamine use was 3·8 (128 events in 3371 person-years) and incidence of HIV infection was 6·0 (212 events in 3554 person-years). In our descriptive model, methamphetamine use, anal sex, and various other behaviours cluster together but their effect on HIV incidence was mediated by the occurrence of ulcerative sexually transmitted infections. Dual risk factors for both incident methamphetamine use and HIV infection were younger age and finding casual sex partners on the internet. Having ever received money for sex was predictive for incident methamphetamine use; living alone or with a housemate, recent anal sex, and ulcerative sexually transmitted infections at baseline were predictive for incident HIV infection. Interpretation: In MSM in Bangkok, casual sex partner recruitment on the internet, methamphetamine use, and sexually transmitted infections have important roles in sustaining the HIV epidemic. Virtual HIV prevention education, drug use harm reduction, and biomedical HIV prevention methods, such as pre-exposure prophylaxis, could help to reduce or revert the HIV epidemic among MSM in Bangkok. Funding: US Centers for Disease Control and Prevention and the Johns Hopkins Fogarty AIDS International Training and Research Program.",,human,0.9998088479042053
220,,13550,"The first book to expound the qualitative theory of systems defined by differential equations, Birkhoff's Dynamical Systems (DS) created a new branch of mathematics separate from its roots in celestial mechanics and making broad use of topology. In DS, Birkhoff summarized more than 15 years of his own research along three main axes: the general theory of dynamical systems; the special case with two degrees of freedom; and the three-body problem in celestial mechanics. In the first two chapters, Birkhoff's treatment was traditional: he gave proofs for existence, uniqueness, and continuity theorems, and then discussed Lagrange's equations, Hamiltonian mechanics, and changes of variables. In the third chapter, solutions were studied in their formal aspects, that is, as power series about which questions of convergence were systematically laid aside as irrelevant to the matter at hand. The next chapter followed Poincaré's idea of investigating the stability of formal solutions near equilibrium or periodic motion. Finally, the fifth chapter presented the methods by means of which the existence of periodic motions could be established Dynamical systems stands strangely isolated among the mathematical literature of its time as a fundamental intermediary between Poincaré's perceptive work and the modern theory. Birkhoff's main ambition was that the final aim of the theory of motion must be directed toward the qualitative determination of all possible types of motions and of the interrelation of these motions.. ",human,0,sdg_abstracts_original,human,0.9997772574424744,"The first book to expound the qualitative theory of systems defined by differential equations, Birkhoff's Dynamical Systems (DS) created a new branch of mathematics separate from its roots in celestial mechanics and making broad use of topology. In DS, Birkhoff summarized more than 15 years of his own research along three main axes: the general theory of dynamical systems; the special case with two degrees of freedom; and the three-body problem in celestial mechanics. In the first two chapters, Birkhoff's treatment was traditional: he gave proofs for existence, uniqueness, and continuity theorems, and then discussed Lagrange's equations, Hamiltonian mechanics, and changes of variables. In the third chapter, solutions were studied in their formal aspects, that is, as power series about which questions of convergence were systematically laid aside as irrelevant to the matter at hand. The next chapter followed Poincaré's idea of investigating the stability of formal solutions near equilibrium or periodic motion. Finally, the fifth chapter presented the methods by means of which the existence of periodic motions could be established Dynamical systems stands strangely isolated among the mathematical literature of its time as a fundamental intermediary between Poincaré's perceptive work and the modern theory. Birkhoff's main ambition was that the final aim of the theory of motion must be directed toward the qualitative determination of all possible types of motions and of the interrelation of these motions.. ",,human,0.9997772574424744
221,,12978,"This paper describes photovoltaic (PV) module architectures with parallel-connected submodule-integrated dc-dc converters (subMICs) that improve efficiency of energy capture in the presence of partial shading or other mismatch conditions. The subMICs are bidirectional isolated dc-dc converters capable of injecting or subtracting currents to balance the module substring voltages. When no mismatches are present, the subMICs are simply shut down, resulting in zero insertion losses. It is shown that the objective of minimum subMIC power processing can be solved as a linear programming problem.Asimple close-to-optimal distributed control approach is presented that allows autonomous subMIC control without the need for a central controller or any communication among the subMICs. Furthermore, the proposed control approach is well suited for an isolated-port architecture, which yields additional practical advantages including reduced subMIC power and voltage ratings. The architectures and the control approach are validated by simulations and experimental results using three bidirectional flyback subMICs attached to a standard 180-W, 72-cell PV module, yielding greater than 98% module-level power processing efficiency for a mismatch less than 25%. ",human,0,sdg_abstracts_original,human,0.9997525811195374,"This paper describes photovoltaic (PV) module architectures with parallel-connected submodule-integrated dc-dc converters (subMICs) that improve efficiency of energy capture in the presence of partial shading or other mismatch conditions. The subMICs are bidirectional isolated dc-dc converters capable of injecting or subtracting currents to balance the module substring voltages. When no mismatches are present, the subMICs are simply shut down, resulting in zero insertion losses. It is shown that the objective of minimum subMIC power processing can be solved as a linear programming problem.Asimple close-to-optimal distributed control approach is presented that allows autonomous subMIC control without the need for a central controller or any communication among the subMICs. Furthermore, the proposed control approach is well suited for an isolated-port architecture, which yields additional practical advantages including reduced subMIC power and voltage ratings. The architectures and the control approach are validated by simulations and experimental results using three bidirectional flyback subMICs attached to a standard 180-W, 72-cell PV module, yielding greater than 98% module-level power processing efficiency for a mismatch less than 25%. ",,human,0.9997525811195374
222,,18129,"The main purpose of this study, based on research conducted in two randomly selected maximum-security prisons in Israel, is to explore the effectiveness of Bibliodidactics, a unique teaching reading method. The research found Bibliodidactics significantly improves the levels of technical reading among nonnative illiterate and poor-reading prisoners of Hebrew. The method is also significantly more efficient for nonnative illiterate prisoners, both in reading comprehension and in technical reading. The authors conclude that literacy instruction for struggling readers benefits from a focus on emotional processing of text, providing meaningful contexts to learning materials that motivate and sustain the readers’ interest.",human,0,sdg_abstracts_original,human,0.994763195514679,"The main purpose of this study, based on research conducted in two randomly selected maximum-security prisons in Israel, is to explore the effectiveness of Bibliodidactics, a unique teaching reading method. The research found Bibliodidactics significantly improves the levels of technical reading among nonnative illiterate and poor-reading prisoners of Hebrew. The method is also significantly more efficient for nonnative illiterate prisoners, both in reading comprehension and in technical reading. The authors conclude that literacy instruction for struggling readers benefits from a focus on emotional processing of text, providing meaningful contexts to learning materials that motivate and sustain the readers’ interest.",,human,0.994763195514679
223,,4600,"Background: The fat mass and obesity associated (FTO) gene has been found to contribute to the risk of obesity in humans, but the function and regulation of FTO mRNA expression in adipose tissues remain to be clarified. Our aims were to assess the FTO gene expression in subcutaneous and visceral adipose tissues from morbidly obese women and its relation with obesity, insulin resistance indices, and most importantly, to obesity-related inflammatory markers. Methods: Paired subcutaneous and visceral fat were excised from 33 morbidly obese women and 12 control women who underwent bariatric surgery by laparoscopic gastric by-pass and elective surgery respectively. Adipose tissue mRNA expression was determined by real time RT-PCR. Results: FTO mRNA expression in visceral adipose tissue (VAT) was significantly higher than in subcutaneous adipose tissue (SAT) from obese but not control patients. SAT FTO expression was reduced in obese women compared to control subjects. It correlated negatively with BMI and insulin resistance indices. FTO expression in SAT was positively related to both circulating and mRNA levels of adiponectin, to adiponectin receptor and to PPAR-δexpression, but negatively with IL-6 gene expression and with circulating levels of leptin. FTO in VAT was also positively correlated with adiponectin, adiponectin receptor and PPAR-δ mRNA expression. Conclusion: FTO expression in subcutaneous adipose tissue negatively correlates with obesity and insulin resistance. On the other hand, FTO presents a positive association with the expression of adiponectin, an anti-inflammatory adipokine, and with PPAR-δ in both adipose tissues. Taken together, our results suggest that FTO is associated with an anti-inflammatory behaviour in morbid obesity. Copyright ",human,0,sdg_abstracts_original,human,0.999313473701477,"Background: The fat mass and obesity associated (FTO) gene has been found to contribute to the risk of obesity in humans, but the function and regulation of FTO mRNA expression in adipose tissues remain to be clarified. Our aims were to assess the FTO gene expression in subcutaneous and visceral adipose tissues from morbidly obese women and its relation with obesity, insulin resistance indices, and most importantly, to obesity-related inflammatory markers. Methods: Paired subcutaneous and visceral fat were excised from 33 morbidly obese women and 12 control women who underwent bariatric surgery by laparoscopic gastric by-pass and elective surgery respectively. Adipose tissue mRNA expression was determined by real time RT-PCR. Results: FTO mRNA expression in visceral adipose tissue (VAT) was significantly higher than in subcutaneous adipose tissue (SAT) from obese but not control patients. SAT FTO expression was reduced in obese women compared to control subjects. It correlated negatively with BMI and insulin resistance indices. FTO expression in SAT was positively related to both circulating and mRNA levels of adiponectin, to adiponectin receptor and to PPAR-δexpression, but negatively with IL-6 gene expression and with circulating levels of leptin. FTO in VAT was also positively correlated with adiponectin, adiponectin receptor and PPAR-δ mRNA expression. Conclusion: FTO expression in subcutaneous adipose tissue negatively correlates with obesity and insulin resistance. On the other hand, FTO presents a positive association with the expression of adiponectin, an anti-inflammatory adipokine, and with PPAR-δ in both adipose tissues. Taken together, our results suggest that FTO is associated with an anti-inflammatory behaviour in morbid obesity. Copyright ",,human,0.999313473701477
224,,16188,"A study on the private sector and disaster risk reduction (DRR) was conducted in six cities of the Americas: Bogotá (Colombia), Miami, Florida (United States), San José (Costa Rica), Santiago (Chile), Kingston (Jamaica), and Vancouver (Canada). The study was led by FIU and supported by USAID/OFDA and the UNISDR, with collaboration from researchers of INCAE Business School (Costa Rica), the University of Chile, Ohio University, and York University (Canada). Based on responses to nearly 1200 surveys, the key findings indicated that (1) 56% of respondents do not have a business continuity plan (BCP) in place; (2) 36.5% of businesses considered a BCP desirable, but stated that other priorities take precedence; (3) the lack of protection in the private sector is caused by not only financial constraints, but also the still not well-understood problems of avoidance, the competing priorities excuse, narrow decision making, and concerns about accountability; (4) small businesses show the least progress in establishing BCPs (14%) compared to larger businesses (44.9%); (5) there are insufficient incentives for DRR strategies to have practical impacts on business vulnerabilities and lack of resilience; (6) the implementation of regulations and enforcement mechanisms are weak to non-existent; and (7) little progress has been made in social responsibility and a sustained commitment to reducing the vulnerabilities of populations at risk. The current study offers recommendations to deepen the analysis, better understand the factors that intervene in the observed ""risk indifference,"" and identify possible interventions in order to move away from the status quo.",human,0,sdg_abstracts_original,human,0.9996167421340942,"A study on the private sector and disaster risk reduction (DRR) was conducted in six cities of the Americas: Bogotá (Colombia), Miami, Florida (United States), San José (Costa Rica), Santiago (Chile), Kingston (Jamaica), and Vancouver (Canada). The study was led by FIU and supported by USAID/OFDA and the UNISDR, with collaboration from researchers of INCAE Business School (Costa Rica), the University of Chile, Ohio University, and York University (Canada). Based on responses to nearly 1200 surveys, the key findings indicated that (1) 56% of respondents do not have a business continuity plan (BCP) in place; (2) 36.5% of businesses considered a BCP desirable, but stated that other priorities take precedence; (3) the lack of protection in the private sector is caused by not only financial constraints, but also the still not well-understood problems of avoidance, the competing priorities excuse, narrow decision making, and concerns about accountability; (4) small businesses show the least progress in establishing BCPs (14%) compared to larger businesses (44.9%); (5) there are insufficient incentives for DRR strategies to have practical impacts on business vulnerabilities and lack of resilience; (6) the implementation of regulations and enforcement mechanisms are weak to non-existent; and (7) little progress has been made in social responsibility and a sustained commitment to reducing the vulnerabilities of populations at risk. The current study offers recommendations to deepen the analysis, better understand the factors that intervene in the observed ""risk indifference,"" and identify possible interventions in order to move away from the status quo.",,human,0.9996167421340942
225,,12838,"Public universities occupy a unique place in the R&D system of the United States because of their state-controlled missions, sources of funding, and administrative structures. State governments preferentially support public university research that benefits local industry to stimulate regional innovation-based economic development. This article hence examines the geographic distribution of university patent citations during the years 1975 to 2000 to test if public university research spillovers are more likely to be localized at the state level as compared to those of private universities. The author finds little evidence in support of this hypothesis but a positive association between the quality of academic research and localization of resulting spillovers. Public universities should emphasize research quality as a means of fulfilling their regional innovation commitments. ",human,0,sdg_abstracts_original,human,0.9998146891593932,"Public universities occupy a unique place in the R&D system of the United States because of their state-controlled missions, sources of funding, and administrative structures. State governments preferentially support public university research that benefits local industry to stimulate regional innovation-based economic development. This article hence examines the geographic distribution of university patent citations during the years 1975 to 2000 to test if public university research spillovers are more likely to be localized at the state level as compared to those of private universities. The author finds little evidence in support of this hypothesis but a positive association between the quality of academic research and localization of resulting spillovers. Public universities should emphasize research quality as a means of fulfilling their regional innovation commitments. ",,human,0.9998146891593933
226,,18606,"Nowadays, lightweight materials and design is one of the means carmakers employ to reduce CO2 emissions according to EU's goals and sustainability principles. In this paper, a comparative analysis between alternative design solutions for Electric Vehicles (EVs) components was carried out using Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) methodologies to evaluate the trade-off between impacts from production and use stages. This work is part of the activities of the ENLIGHT European Green Vehicles project, where the objective was to advance highly innovative lightweight material technologies for application in EVs structural parts. The purposes of the study are: i) to evaluate the parameters influencing the trade-off from the environmental and economic perspective; ii) to combine LCA and LCC results interpretations. The results stressed that the trade-off becomes much more sensitive when innovative materials and technologies are applied to EVs lightweighting.",human,0,sdg_abstracts_original,human,0.99976247549057,"Nowadays, lightweight materials and design is one of the means carmakers employ to reduce CO2 emissions according to EU's goals and sustainability principles. In this paper, a comparative analysis between alternative design solutions for Electric Vehicles (EVs) components was carried out using Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) methodologies to evaluate the trade-off between impacts from production and use stages. This work is part of the activities of the ENLIGHT European Green Vehicles project, where the objective was to advance highly innovative lightweight material technologies for application in EVs structural parts. The purposes of the study are: i) to evaluate the parameters influencing the trade-off from the environmental and economic perspective; ii) to combine LCA and LCC results interpretations. The results stressed that the trade-off becomes much more sensitive when innovative materials and technologies are applied to EVs lightweighting.",,human,0.9997624754905701
227,,14039,"Purpose – This paper aims to summarise the methods and main findings of a study of the environmental impacts of providing higher education (HE) courses by campus-based and distance/open-learning methods. Design/methodology/approach – The approach takes the form of an environmental audit, with data from surveys of 20 UK courses – 13 campus-based, seven print-based and online distance learning courses – covering travel, paper and print consumption, computing, accommodation, and campus site impacts. Results were converted into energy and CO2 emissions per student per 100 hours of degree study. Findings – Distance learning HE courses involve 87 per cent less energy and 85 per cent lower CO2 emissions than the full-time campus-based courses. Part-time campus HE courses reduce energy and CO2 emissions by 65 and 61 per cent, respectively, compared with full-time campus courses. The lower impacts of part-time and distance compared with full-time campus courses is mainly due to a reduction in student travel and elimination of much energy consumption of students' housing, plus economies in campus site utilisation. E-learning appears to offer only relatively small energy and emissions reductions (20 and 12 per cent, respectively) compared with mainly print-based distance learning courses, mainly because online learning requires more energy for computing and paper for printing. Research limitations/implications – Assumptions were made in order to calculate the energy and emissions arising from the different HE systems. For example, it was decided to include all the energy consumed in term-time accommodation for full-time campus students while part-time campus and distance learning students live at home, only requiring additional heating and lighting for study. Future studies could include more distance and blended learning courses offered by institutions other than the UK Open University and impacts other than CO2 emissions. Practical implications – Existing HE sustainability programmes should be broadened beyond considering campus site impacts and “greening the curriculum”. Indeed, were HE expansion to take environmental impacts seriously, then part-time and distance education should be prioritised over increasing full-time provision. This appears compatible with the Leitch Review of Skills on continuing education and training for the UK workforce. Originality/value – The paper represents the only existing quantitative study of this issue. ",human,0,sdg_abstracts_original,human,0.9998024106025696,"Purpose – This paper aims to summarise the methods and main findings of a study of the environmental impacts of providing higher education (HE) courses by campus-based and distance/open-learning methods. Design/methodology/approach – The approach takes the form of an environmental audit, with data from surveys of 20 UK courses – 13 campus-based, seven print-based and online distance learning courses – covering travel, paper and print consumption, computing, accommodation, and campus site impacts. Results were converted into energy and CO2 emissions per student per 100 hours of degree study. Findings – Distance learning HE courses involve 87 per cent less energy and 85 per cent lower CO2 emissions than the full-time campus-based courses. Part-time campus HE courses reduce energy and CO2 emissions by 65 and 61 per cent, respectively, compared with full-time campus courses. The lower impacts of part-time and distance compared with full-time campus courses is mainly due to a reduction in student travel and elimination of much energy consumption of students' housing, plus economies in campus site utilisation. E-learning appears to offer only relatively small energy and emissions reductions (20 and 12 per cent, respectively) compared with mainly print-based distance learning courses, mainly because online learning requires more energy for computing and paper for printing. Research limitations/implications – Assumptions were made in order to calculate the energy and emissions arising from the different HE systems. For example, it was decided to include all the energy consumed in term-time accommodation for full-time campus students while part-time campus and distance learning students live at home, only requiring additional heating and lighting for study. Future studies could include more distance and blended learning courses offered by institutions other than the UK Open University and impacts other than CO2 emissions. Practical implications – Existing HE sustainability programmes should be broadened beyond considering campus site impacts and “greening the curriculum”. Indeed, were HE expansion to take environmental impacts seriously, then part-time and distance education should be prioritised over increasing full-time provision. This appears compatible with the Leitch Review of Skills on continuing education and training for the UK workforce. Originality/value – The paper represents the only existing quantitative study of this issue. ",,human,0.9998024106025696
228,,21133,"Background and Aim: Several in vitro studies have demonstrated the ability of pure trans-resveratrol (t-Res) to act as an anti-oxidant, but the scientific literature is lacking in in vivo studies dealing with dietary t-Res bioavailability in oxidative stress models. Our aim was to investigate the bioavailability of t-Res from dietary sources and its effect on an animal model of carbon tetrachloride (CCl4)-induced liver lipid peroxidation. Methods: Ten rats were intragastrically administered for 14 days with a grape-stalk extract determining a daily t-Res dosage of 3 mg/kg. The control group (10 rats) was daily injected with the vehicle solvent without the t-Res extract. After 1 week, the induction of liver lipid peroxidation by CCl 4 injection was carried out. Serum and liver samples, at different time intervals, were collected to evaluate t-Res content, by high-performance liquid chromatography (HPLC) and liquid chromatography-mass spectometry-mass spectometry (LC-MS-MS). Liver malondialdehyde (MDA) as marker of oxidative stress was measured. Results: t-Res accumulates in the liver reaching 49.8 ± 10.2 ng/g after 7 days and 191.8 ± 15.3 ng/g after 14 days. No t-Res was detected in serum. The increase of MDA liver concentration due to CCl4 injection after 24 h and 1 week was reduced by 38% and a 63%, respectively, by the treatment with the t-Res extract. Conclusions: A moderate consumption of t-Res from a dietary source resulted in a time-dose-dependent liver accumulation. It was able to counteract in vivo CCl4-induced liver lipid peroxidation thus demonstrating the hepatoprotective property of t-Res. ",human,0,sdg_abstracts_original,human,0.8612515330314636,"Background and Aim: Several in vitro studies have demonstrated the ability of pure trans-resveratrol (t-Res) to act as an anti-oxidant, but the scientific literature is lacking in in vivo studies dealing with dietary t-Res bioavailability in oxidative stress models. Our aim was to investigate the bioavailability of t-Res from dietary sources and its effect on an animal model of carbon tetrachloride (CCl4)-induced liver lipid peroxidation. Methods: Ten rats were intragastrically administered for 14 days with a grape-stalk extract determining a daily t-Res dosage of 3 mg/kg. The control group (10 rats) was daily injected with the vehicle solvent without the t-Res extract. After 1 week, the induction of liver lipid peroxidation by CCl 4 injection was carried out. Serum and liver samples, at different time intervals, were collected to evaluate t-Res content, by high-performance liquid chromatography (HPLC) and liquid chromatography-mass spectometry-mass spectometry (LC-MS-MS). Liver malondialdehyde (MDA) as marker of oxidative stress was measured. Results: t-Res accumulates in the liver reaching 49.8 ± 10.2 ng/g after 7 days and 191.8 ± 15.3 ng/g after 14 days. No t-Res was detected in serum. The increase of MDA liver concentration due to CCl4 injection after 24 h and 1 week was reduced by 38% and a 63%, respectively, by the treatment with the t-Res extract. Conclusions: A moderate consumption of t-Res from a dietary source resulted in a time-dose-dependent liver accumulation. It was able to counteract in vivo CCl4-induced liver lipid peroxidation thus demonstrating the hepatoprotective property of t-Res. ",,human,0.8612502217292786
229,,10492,"Among the many physical and chemical strategies used to make air pollution gas sensors for CO and NO2 monitoring, those employing multi-sensor arrays offer excellent selectivity, long lifetimes, low drift and low costs of manufacture. Targeting the two gases CO and NO2, new thick film sensor matrix was fabricated using the screen printing technology, With this purpose, undoped and noble metal-doped chick film SnO2 sensing layers deposited by screen printing technique upon micro-machined Si substrate have been characterised, The main objective in this study has been the challenging perspective to define a sensing layer type which allows a high quality monitoring and easy discrimination of low concentration of NO2 by an interfering species like CO, both present in the outdoor air. The experimental results reported in this work have shown that combining the sensor technology with a suitable catalytic element in the present case, gold, palladium and platinum, low concentration of NO2 can be revealed with a sufficient selectivity towards CO. ",human,0,sdg_abstracts_original,human,0.9980408549308776,"Among the many physical and chemical strategies used to make air pollution gas sensors for CO and NO2 monitoring, those employing multi-sensor arrays offer excellent selectivity, long lifetimes, low drift and low costs of manufacture. Targeting the two gases CO and NO2, new thick film sensor matrix was fabricated using the screen printing technology, With this purpose, undoped and noble metal-doped chick film SnO2 sensing layers deposited by screen printing technique upon micro-machined Si substrate have been characterised, The main objective in this study has been the challenging perspective to define a sensing layer type which allows a high quality monitoring and easy discrimination of low concentration of NO2 by an interfering species like CO, both present in the outdoor air. The experimental results reported in this work have shown that combining the sensor technology with a suitable catalytic element in the present case, gold, palladium and platinum, low concentration of NO2 can be revealed with a sufficient selectivity towards CO. ",,human,0.9980408549308777
230,,12620,"The main aim of this study was to provide new and reliable food composition data on carotenoids and retinol in South Asian Foods for the United Kingdom's national database. A total of 38 commonly consumed foods were analysed using HPLC and accredited methods of analyses. Palak paneer (spinach and soft Indian cheese) contained the highest levels of β-carotene (4066 μg/100. g) followed by gajjeralla (carrot based sweet, 2324 μg/100. g) and saag (mixed green leafy vegetables), which contained 1514 μg/100. g, whilst retinol was present in only a few foods, with ghee being the major source (968 μg/100. g). Meat dishes contained higher amounts of lycopene (up to 1140 μg/100. g in chicken balti) than vegetable or dhal dishes (highest in palak paneer 317 μg/100. g), because of larger quantities of tomatoes used in meat curries. A variety of ethnic vegetables (green leafy vegetables and other coloured vegetables), namely legumes/dhal, tomatoes and coriander, were identified to be the major ingredients containing carotenoids. These new data can be used in future diet and nutrition surveys, as well as to identify carotenoid-rich foods for dietary programs. ",human,0,sdg_abstracts_original,human,0.9998231530189514,"The main aim of this study was to provide new and reliable food composition data on carotenoids and retinol in South Asian Foods for the United Kingdom's national database. A total of 38 commonly consumed foods were analysed using HPLC and accredited methods of analyses. Palak paneer (spinach and soft Indian cheese) contained the highest levels of β-carotene (4066 μg/100. g) followed by gajjeralla (carrot based sweet, 2324 μg/100. g) and saag (mixed green leafy vegetables), which contained 1514 μg/100. g, whilst retinol was present in only a few foods, with ghee being the major source (968 μg/100. g). Meat dishes contained higher amounts of lycopene (up to 1140 μg/100. g in chicken balti) than vegetable or dhal dishes (highest in palak paneer 317 μg/100. g), because of larger quantities of tomatoes used in meat curries. A variety of ethnic vegetables (green leafy vegetables and other coloured vegetables), namely legumes/dhal, tomatoes and coriander, were identified to be the major ingredients containing carotenoids. These new data can be used in future diet and nutrition surveys, as well as to identify carotenoid-rich foods for dietary programs. ",,human,0.9998231530189514
231,,26054,"Competencies for school leadership in the initial training of teachers are necessary because the teaching action in the classrooms must be allied with leadership and the optimal organization of the educational institution and this includes competences for coordination and networking with the community. This article focuses on the views on these competencies of teaching staff and school leaders who collaborate on the practicum stage of students. To this end, a multiple case study is conducted using mixed methodology, with the participation of three pre-school and primary schools. The study sample (n=20) is made up of members of management teams and teachers who are working in pre-school or the two first years of primary school. The instrument used is an ad hoc questionnaire, distributed online during the 2016-2017 academic year. The results show that teachers and principals perceive that, from all the teaching functions, coordination is the less developed although it is the most important, above leadership and the relationship with the socio-educational environment. Teachers, they consider that the following competencies, related to leadership, can be improved: conflict resolution, complex situations analysis, classroom management and relationship with families and entities in the educational context.",human,0,sdg_abstracts_original,human,0.9995452761650084,"Competencies for school leadership in the initial training of teachers are necessary because the teaching action in the classrooms must be allied with leadership and the optimal organization of the educational institution and this includes competences for coordination and networking with the community. This article focuses on the views on these competencies of teaching staff and school leaders who collaborate on the practicum stage of students. To this end, a multiple case study is conducted using mixed methodology, with the participation of three pre-school and primary schools. The study sample (n=20) is made up of members of management teams and teachers who are working in pre-school or the two first years of primary school. The instrument used is an ad hoc questionnaire, distributed online during the 2016-2017 academic year. The results show that teachers and principals perceive that, from all the teaching functions, coordination is the less developed although it is the most important, above leadership and the relationship with the socio-educational environment. Teachers, they consider that the following competencies, related to leadership, can be improved: conflict resolution, complex situations analysis, classroom management and relationship with families and entities in the educational context.",,human,0.9995452761650085

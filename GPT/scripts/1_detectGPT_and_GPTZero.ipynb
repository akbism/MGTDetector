{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0m0lBYyFPxT",
        "outputId": "d744ee09-eb90-47f5-df10-a6622b964724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/GPT\n"
          ]
        }
      ],
      "source": [
        "### Set up the root folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XxeLw7N5So",
        "outputId": "869516b9-3f07-4281-de19-e30dc90413d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirements.txt -qq\n",
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJE46r7JW03n",
        "outputId": "10b44f12-7f66-4e72-a18e-947497c2a8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import transformers\n",
        "import pandas as pd\n",
        "from model import GPT2PPLV2 as GPT2PPL\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vi2N89S-YkHI"
      },
      "outputs": [],
      "source": [
        "datasetname = [ 'DagPap22','chatgpt_human_combined','IELTS_GPT35_Human_Comparison']\n",
        "\n",
        "detectgptmodelname = 'detectgpt'\n",
        "gptzeromodelname = 'gptzero'\n",
        "\n",
        "def getInputfileName(datasetname):\n",
        "    return f\"./output/{datasetname}_Corpus.csv\"\n",
        "\n",
        "def getOutputfileName(datasetname, modelname):\n",
        "    return f\"./output/{datasetname}_result_{modelname}.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_VNFqQ7JYkHI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report### DetectGPT\n",
        "#function to process the detectGPT and Chat GPT model on a given datasets file\n",
        "def processGPTModel(datasetFile):\n",
        "    print(f\"Processing for datasets : {datasetFile}\")\n",
        "    df_combined = pd.read_csv(getInputfileName(datasetFile))\n",
        "    # initialize the model\n",
        "    model = GPT2PPL()\n",
        "\n",
        "    sentence = \"\"\"Some people believe that a college or a university should be available to all students. Others think that higher education should be available only to good students. In my opinion, higher education should be available to all students. Why do I think so?\n",
        "    Firstly, to get higher education is a right of any person. If higher education is available only to good student, it will be infringement of human rights. We cannot say to man that she or he is not entitled to get higher education because it will be anti-constitutional. On the other hand, a college or university can have own rules for students who don't have a good GPA. For example, students can graduate from college if they have only \"A\" or \"B\". Another example, students can take several times the same exam while they won’t receive a good grade. It is very important to give a right to get higher education to all students, because it will be their chance to change something in their life, get better job in the future or built their career.\n",
        "    Secondly, who gives us right to share people in good and bad students. Maybe they have good abilities for studying, yet, these students chose the wrong way ignoring learning when they were high school students. We have a lot of examples when people didn't have a good grade when they were a high school students, however, they could graduate from college and became to work successfully. We know many examples about great people, Pushkin, Mendeleev, who weren't good students in all subjects, but they were very talented persons and whole world knows their names. To be a good or bad students are not the same to have abilities for learning. We should understand and remember it.\n",
        "    Some people can say that government and parents just waste money for education bad students. Nonetheless, the government also can change rules for those students. For example, not support financially if the student has a grade \"C\" or worse. However, our government sustains any students. I think that it is the right way, because we live in a free country and any man has the same right to get higher education. In summary, in spite of colleges and universities are mostly interested in good students, I strongly believe that any students, good or bad, should have the same right to earn high education.\"\"\"\n",
        "\n",
        "    temp = model(sentence, 200, \"v1.1\")\n",
        "\n",
        "    df_combined['token_count']= [len(tokenize.word_tokenize(x)) for x in df_combined.text]\n",
        "    result_detectgpt = pd.DataFrame(columns=['sr_no','prob', 'label', 'desc'])\n",
        "    result_gptzero = pd.DataFrame(columns=['sr_no','Perplexity', 'Perplexity per line', 'Burstiness', 'label', 'desc'])\n",
        "\n",
        "    #“perplexity” (aka randomness) and “burstiness” (aka variance)\n",
        "    # GPTZero:\n",
        "    # To determine whether an excerpt is written by a bot, GPTZero uses two indicators: \"perplexity\" and \"burstiness.\"\n",
        "\n",
        "    # Perplexity\n",
        "    # Perplexity measures the complexity of text; if GPTZero is perplexed by the text, then it has a high complexity and it's more likely to be human-written.\n",
        "    # However, if the text is more familiar to the bot — because it's been trained on such data — then it will have low complexity and therefore is more likely to be AI-generated.\n",
        "\n",
        "    # “burstiness”\n",
        "    # If a term is used once in a document, then it is likely to be used again. This phenomenon is called burstiness, and it implies that the second and later appearances of a word are less significant than the first appearance\n",
        "\n",
        "    for i in range(len(df_combined)):\n",
        "      print(i)\n",
        "      try:\n",
        "        token_count = min(df_combined['token_count'][i], 200)\n",
        "        pred, desc = model(df_combined.text[i], token_count, \"v1.1\")\n",
        "        pred['desc']= desc\n",
        "        pred['sr_no']= df_combined.sr_no[i]\n",
        "      except:\n",
        "        pred['prob']= None\n",
        "        pred['label']= None\n",
        "        pred['desc']= 'error'\n",
        "        pred['sr_no']= df_combined.sr_no[i]\n",
        "      result_detectgpt = result_detectgpt.append(pred, ignore_index=True)\n",
        "\n",
        "      try:\n",
        "        pred, desc = model(df_combined.text[i], token_count, \"v1\")\n",
        "        pred = dict(pred)\n",
        "        pred['desc']= desc\n",
        "        pred['sr_no']= df_combined.sr_no[i]\n",
        "      except:\n",
        "        pred['Perplexity']= None\n",
        "        pred['Perplexity per line']= None\n",
        "        pred['Burstiness']= None\n",
        "        pred['label']= None\n",
        "        pred['desc']= 'error'\n",
        "        pred['sr_no']= df_combined.sr_no[i]\n",
        "      result_gptzero = result_gptzero.append(pred, ignore_index=True)\n",
        "\n",
        "    result_detectgpt.to_csv(getOutputfileName(datasetFile, detectgptmodelname))\n",
        "    result_gptzero.to_csv(getOutputfileName(datasetFile, gptzeromodelname))\n",
        "\n",
        "    result_detectgpt['label']=result_gptzero.label.map({1:'human', 0:'chatgpt'})\n",
        "    result_gptzero['label']=result_gptzero.label.map({1:'human', 0:'chatgpt'})\n",
        "\n",
        "    gptzero_final=pd.concat([result_gptzero, df_combined], axis=1)\n",
        "    gptzero_final = gptzero_final[['sr_no', 'source', 'label']].dropna()\n",
        "    detectgpt_final=pd.concat([result_gptzero, df_combined], axis=1)\n",
        "    detectgpt_final = detectgpt_final[['sr_no', 'source', 'label']].dropna()\n",
        "\n",
        "    print(\"### DetectGPT\")\n",
        "    print(classification_report(detectgpt_final['source'], detectgpt_final['label']))\n",
        "    print(\"### GPTZero\")\n",
        "    print(classification_report(gptzero_final['source'], gptzero_final['label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_j79fZZYkHR",
        "outputId": "0c3ac099-cf5c-4632-d9ab-ef1a5a302317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing for datasets : DagPap22\n"
          ]
        }
      ],
      "source": [
        "#Process for  all datasets one by one\n",
        "for datasetfile in datasetname:\n",
        "    processGPTModel(datasetfile)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gx59GvJPnDFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}